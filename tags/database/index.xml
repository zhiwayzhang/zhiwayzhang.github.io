<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Database on Coding_Panda&#39;s Blog</title>
        <link>https://blog.ipandai.club/tags/database/</link>
        <description>Recent content in Database on Coding_Panda&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 26 Nov 2022 13:40:59 +0800</lastBuildDate><atom:link href="https://blog.ipandai.club/tags/database/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[ATC&#39;20] PinK: High-speed In-storage Key-value Store with Bounded Tails</title>
        <link>https://blog.ipandai.club/p/atc20-pink-high-speed-in-storage-key-value-store-with-bounded-tails/</link>
        <pubDate>Sat, 26 Nov 2022 13:40:59 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/atc20-pink-high-speed-in-storage-key-value-store-with-bounded-tails/</guid>
        <description>&lt;h1 id=&#34;0x00-everyday-english&#34;&gt;0x00 Everyday English&lt;/h1&gt;
&lt;p&gt;preferable 更可取的&lt;/p&gt;
&lt;p&gt;consequently 因此&lt;/p&gt;
&lt;p&gt;inevitably 必然的&lt;/p&gt;
&lt;p&gt;conventional 常规的、传统的&lt;/p&gt;
&lt;p&gt;exacerbates 加剧&lt;/p&gt;
&lt;p&gt;deteriorates 恶化&lt;/p&gt;
&lt;p&gt;deterministic 确定性&lt;/p&gt;
&lt;h1 id=&#34;0x01-intro&#34;&gt;0x01 Intro&lt;/h1&gt;
&lt;p&gt;本文主要提出了Pink，基于LSM tree的KV-SSD，主要通过避免使用布隆过滤器，而是使用少量DRAM来进行优化&lt;/p&gt;
&lt;p&gt;传统的KV-SSD主要通过在控制器的DRAM中维护一个hash table来寻址，受到DRAM的容量限制，超出容量的部分会暂时存放在闪存中，因此在寻址时带来了访问闪存的额外开销，同时当发生哈希碰撞时，可能会操作多个闪存，造成了大量不可预测的开销&lt;/p&gt;
&lt;p&gt;单纯使用LSM做为替换的话，可以减少DRAM的存储，但是存在一些性能问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尾延迟：很多LSM-tree使用布隆过滤器来快速检索，可靠性差，存在尾延迟问题&lt;/li&gt;
&lt;li&gt;写放大：由于LSM对KV索引的排序和合并操作，带来了很多对闪存额外的访问，还增大了FTL进行GC的负担&lt;/li&gt;
&lt;li&gt;布隆过滤器的重建和KV排序：消耗了大量的CPU资源，带来I/O性能的大幅下降&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文提出了一种基于LSM-tree的KV存储引擎-PinK，解决了上述三个问题。PinK使用了四种技术。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;PinK的核心是level pinning，将固定在LSM-tree最顶层的KV索引固定到DRAM中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由于部分level固定在DRAM中，因此可以直接进行compaction，无需占用闪存的I/O，被固定的索引通过电容进行保护，因此也不需要定期进行刷新，这里要注意作者使用DRAM容量很小，使用电容足以保证持久化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GC的主要I/O开销来自对LSM-tree索引的更新，Pink通过延迟LSM-tree中索引更新到compaction阶段来减少GC的I/O开销&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在SSD控制器和NAND芯片之间添加比较器，PinK在读取KV对象时执行KV排序，完全消除了compaction的CPU成本&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x02-bg&#34;&gt;0x02 Bg&lt;/h1&gt;
&lt;p&gt;了解一些有关NAND闪存SSD、KV-SSD、Hash-based KV-SSD、LSM-Tree-based KV-SSD的现状，并对Hash和LSM-tree进行比较&lt;/p&gt;
&lt;p&gt;作者对LSM-tree的设计，L0位于DRAM中，并作为write buffer，其余各层存储在闪存中&lt;/p&gt;
&lt;h1 id=&#34;0x03-design&#34;&gt;0x03 Design&lt;/h1&gt;
&lt;p&gt;Pink有四个主要的数据结构&lt;/p&gt;
&lt;p&gt;位于DRAM中的level lists和Skiplist&lt;/p&gt;
&lt;p&gt;位于闪存中的meta segments和data segments&lt;/p&gt;
&lt;p&gt;在LSM-tree KV 文件系统方面，作者主要参考了&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast16/technical-sessions/presentation/lu&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WiscKey FAST&#39;16&lt;/a&gt;，取代FTL进行GC、索引和磨损均衡&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221128145842322.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221128145842322&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;skiplist&#34;&gt;Skiplist&lt;/h2&gt;
&lt;p&gt;Skiplist作为LSM-tree中的L0，起到类似write buffer的作用，暂时保存KV对象，Skiplist中的对象为：&lt;code&gt;&amp;lt;key size, key, value size, value&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;当Skiplist满时，会将缓存的对象以meta segment和data segment的形式刷新到L1中，在L1和更低的level中，Key和Value将被分离存储到meta/data segment中，data segment中存储value、key和key size用于GC&lt;/p&gt;
&lt;h2 id=&#34;level-list&#34;&gt;Level list&lt;/h2&gt;
&lt;p&gt;用于跟踪闪存中每个level的meta segment&lt;/p&gt;
&lt;p&gt;每个level list是一个保存固定大小的指针对的数组，每个指针4B，第一个指针指向meta segment在闪存中的物理地址，第二个指针指向meta segment的start key&lt;/p&gt;
&lt;p&gt;meta segment的start key单独存储在DRAM中，可以支持动态大小的key，同时可以实现在level list查找meta segment时进行二分查找&lt;/p&gt;
&lt;p&gt;如果LSM-tree有5个Level，即h=5，除了L0之外，每个level都会有一个level list&lt;/p&gt;
&lt;p&gt;L0和Level list通过电容来保护，Pink不需要使用日志来维护原子性和数据的持久性&lt;/p&gt;
&lt;h2 id=&#34;level-pinning&#34;&gt;Level Pinning&lt;/h2&gt;
&lt;p&gt;消除尾延迟，将$top-k\ \  level(k&amp;lt;=h-1)$ 的meta segment保存在DRAM中，减少了read过程的尾部延迟&lt;/p&gt;
&lt;p&gt;当处理GET指令时，首先在DRAM中对Key进行查找，当DRAM中没有命中时，将对剩余的几个level在闪存中进行查找，在使用布隆过滤器时，最坏的查询时间复杂度为$O(h-1)$，该策略下可以优化为$O(h-k-1)$&lt;/p&gt;
&lt;p&gt;因此LSM-tree各level按照参数T进行增长，因此topk策略下空间占用不会过大&lt;/p&gt;
&lt;p&gt;同时可以直接在DRAM中进行compaction操作，无需占用闪存I/O，因为有电容的存在，无需定期刷新脏segment到闪存中&lt;/p&gt;
&lt;h2 id=&#34;optimizing-search-path&#34;&gt;Optimizing Search Path&lt;/h2&gt;
&lt;p&gt;在对Key进行查询时，LSM-tree将对各个Level进行二分查找&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221129190239823.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221129190239823&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;朴素的LSM-tree最坏的查询时间复杂度为$O(h^2\cdot log(T))$，使用布隆过滤器的情况下可以更低&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;PinK通过prefix来减少字符串比较的开销，并且先比较key的前4个字节，如果匹配成功才会比较后面的部分&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在level list上每个记录引入一个4字节的范围指针，实现多级cascading范围查找&lt;/p&gt;
&lt;p&gt;指针指向下一级有最大的start key同时key 小于等于上一级中的记录，在一级一级的传递中不断减少搜索的范围，如上图b所示，平均时间复杂度为$O(h\cdot log(T))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;speeding-up-compaction&#34;&gt;Speeding up Compaction&lt;/h2&gt;
&lt;p&gt;在compaction阶段，引入一个硬件加速器，位于闪存和数据主总线之间，可以方便的合并两个闪存中的level&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221129225719057.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221129225719057&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;工作流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;PinK向加速器发起compaction请求，请求参数为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两个level的meta segment的闪存地址$(L_i,L_{i+1})$&lt;/li&gt;
&lt;li&gt;回写的闪存地址$L_{i+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向多个闪存发起读取请求，同时还要对不同channel返回的数据进行重新排序，序列化meta segment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上图中灰色的比较器，会不断的获取两个level中的数据，输出key较小的那一个&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于无效的key，将通知PinK进行GC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输出的排序后结果将保存在write buffer中进行回写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;操作结束后，加速器通知PinK当前$L_{i+1}$使用的闪存页的数量，便于和此前提供的进行比较，用于GC&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PinK在DRAM的pinned level中也使用了类似的加速器，基于DMA&lt;/p&gt;
&lt;h2 id=&#34;optimizing-gc&#34;&gt;Optimizing GC&lt;/h2&gt;
&lt;p&gt;需要针对meta/data segment分别制定GC策略&lt;/p&gt;
&lt;h2 id=&#34;durability--scalability&#34;&gt;Durability &amp;amp;&amp;amp; Scalability&lt;/h2&gt;
&lt;p&gt;使用电容来保护整个DRAM，对于比较低端的SSD，没有足够的电容来维持，只能继续使用定期写回到闪存的机制（要保护level lists，pinned level，L0），同时还要记录日志，由于LSM-tree的特性，该操作比基于hash的KV-SSD开销略小&lt;/p&gt;
&lt;h1 id=&#34;conclusion--thinking&#34;&gt;Conclusion &amp;amp;&amp;amp; Thinking&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;与之前阅读的KV-SSD相比，PinK将部分Level固定到DRAM中，一定程度上降低了延迟&lt;/li&gt;
&lt;li&gt;PinK弃用了布隆过滤器，使用一种层级缩小查找范围的方式，优化查询效率&lt;/li&gt;
&lt;li&gt;引入了一个加速器来减少LSM-tree compaction操作的开销&lt;/li&gt;
&lt;li&gt;论文都提高了布隆过滤器使用时重建的开销略高，是否可以考虑使用布谷鸟过滤器，这部分主要着眼于LSM-tree的优化，可能并不是真正的性能瓶颈&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[OSDI&#39;21] Modernizing File System through In-Storage Indexing</title>
        <link>https://blog.ipandai.club/p/osdi21-modernizing-file-system-through-in-storage-indexing/</link>
        <pubDate>Sat, 29 Oct 2022 16:48:32 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi21-modernizing-file-system-through-in-storage-indexing/</guid>
        <description>&lt;h1 id=&#34;questions&#34;&gt;Questions&lt;/h1&gt;
&lt;p&gt;核心思想部分需要再思考和梳理下，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么采用 KV SSD 替代传统 block SSD 能够解决上述问题？&lt;/li&gt;
&lt;li&gt;其中，利用了 KV SSD 的什么特性？&lt;/li&gt;
&lt;li&gt;对文件系统哪些地方做了什么以优化？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;论文试图解决的问题：&lt;/p&gt;
&lt;p&gt;随着存储设备的不断发展，存储设备的性能越来越高，但当前操作系统内核的文件系统在一些操作上并不能够充分利用如今存储设备的性能。&lt;/p&gt;
&lt;p&gt;文件系统在执行数据写入时，需要执行大量操作维护元数据、进行硬盘的空间管理、维护文件系统的一致性，工作量大。&lt;/p&gt;
&lt;p&gt;核心思想：&lt;/p&gt;
&lt;p&gt;使用Key-Value存储接口取代传统的快设备接口。&lt;/p&gt;
&lt;p&gt;具体实现：&lt;/p&gt;
&lt;p&gt;提出Kevin，分为Kevin=KevinFS + KevinSSD&lt;/p&gt;
&lt;p&gt;KevinFS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维护文件和目录到KV对象的映射关系&lt;/li&gt;
&lt;li&gt;将POSIX系统调用转译成KV操作指令&lt;/li&gt;
&lt;li&gt;保证KV-SSD的一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KevinSSD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在存储设备中索引KV对象&lt;/li&gt;
&lt;li&gt;对多个KV对象提供事务操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;Kevin避免了大量元数据的维护带来的I/O放大&lt;/p&gt;
&lt;p&gt;不需要日志即可完成崩溃一致性的维护&lt;/p&gt;
&lt;p&gt;可以抵御文件分段后造成的性能下降&lt;/p&gt;
&lt;p&gt;存储的文件块通过LSM进行分部排序和索引&lt;/p&gt;
&lt;h1 id=&#34;0x01-bg--related-work&#34;&gt;0x01 BG &amp;amp;&amp;amp; Related Work&lt;/h1&gt;
&lt;h2 id=&#34;传统块设备&#34;&gt;传统块设备&lt;/h2&gt;
&lt;p&gt;提供块粒度（512B or 4KB）的访问&lt;/p&gt;
&lt;p&gt;HDD通过维护一个间接表来处理坏块&lt;/p&gt;
&lt;p&gt;基于闪存的SSD通过FTL来维护逻辑块到物理地址的映射索引表，以便在异地更新的NAND上模拟可重写介质并且排除坏块。&lt;/p&gt;
&lt;p&gt;现有研究缓解了I/O调度、文件碎片化、日志相关问题，没能消除元数据的修改带来的I/O流量&lt;/p&gt;
&lt;p&gt;DevFS实现了在存储设备内部的文件系统，直接将接口暴露给应用，调用时不用发生Trap，对于元数据的维护都在存储设备端执行，移除了I/O 栈减少了通信接口的开销。缺点是需要大量的DRAM和多核心的CPU、能提供的功能有限，还限制了快照、重复数据删除等文件系统高级功能的实现。&lt;/p&gt;
&lt;p&gt;KV存储可以高效处理元数据和小文件的写入&lt;/p&gt;
&lt;h2 id=&#34;lsm-tree&#34;&gt;LSM-Tree&lt;/h2&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221102145019013.png&#34; alt=&#34;image-20221102145019013&#34; style=&#34;zoom:70%;&#34; /&gt;
&lt;p&gt;LSM Tree有多级，包括$L_1,L_2,&amp;hellip;,L_{h-1},L_{h}$，h是树的高度，对于各级的大小有如下关系，$Size(L_{i+1})&amp;gt;=T*Size(L_i)$&lt;/p&gt;
&lt;p&gt;每层都有按Key排序的唯一KV对象。各层之间的Key范围可能会出现重叠。&lt;/p&gt;
&lt;p&gt;KV对象首先写入DRAM中的Memtable，当Memtable不为空时，缓存的KV对象将刷新到L1中进行持久化，当L1不为空时，KV对象刷新到L2中，依此类推。在刷新过程中会执行压缩操作来对两层之间的KV对象进行合并和排序，在排序后写回磁盘中。&lt;/p&gt;
&lt;p&gt;为了改善压缩操作的I/O开销，可以将所有的value存储在value log中，在LSM tree中只保存value指针，存储&lt;code&gt;&amp;lt;key, value pointer&amp;gt;&lt;/code&gt;形式。对于失效的value要考虑垃圾回收问题。&lt;/p&gt;
&lt;p&gt;在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。&lt;/p&gt;
&lt;h1 id=&#34;0x02-architecture&#34;&gt;0x02 Architecture&lt;/h1&gt;
&lt;p&gt;KEVIN分为两个部分&lt;/p&gt;
&lt;p&gt;KEVINFS：维护文件、目录到KV对象映射的文件系统&lt;/p&gt;
&lt;p&gt;KEVINSSD：索引KV对象到闪存的LSM-tree&lt;/p&gt;
&lt;h2 id=&#34;kv-command&#34;&gt;KV Command&lt;/h2&gt;
&lt;p&gt;支持多种KV指令，同时支持事务&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221102154423353.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102154423353&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者将Key限制为256B，value大小没有限制&lt;/p&gt;
&lt;h2 id=&#34;文件和目录的映射&#34;&gt;文件和目录的映射&lt;/h2&gt;
&lt;p&gt;KevinFS只使用三个类型的KV对象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$superblock$：保存文件系统的信息，大小128B&lt;/li&gt;
&lt;li&gt;$meta$：保存文件、目录的元数据，大小256B&lt;/li&gt;
&lt;li&gt;$data$：保存文件数据，unlimited，不超过文件大小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于目录的遍历通过ITERATE&lt;/p&gt;
&lt;p&gt;Key的命名遵循如下规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;meta对象的key组成：
&lt;ul&gt;
&lt;li&gt;前缀&lt;code&gt;m:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;父目录的inode number&lt;/li&gt;
&lt;li&gt;分隔符&lt;code&gt;:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;文件/目录名&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data对象的key组成：
&lt;ul&gt;
&lt;li&gt;前缀&lt;code&gt;d:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;文件的inode number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key命名规范参考了其他论文（Kai Ren and Garth Gibson. TABLEFS: Enhancing Metadata Efficiency in the Local File System. In Proceedings of the USENIX Annual Technical Conference, pages 145–156, 2013.），本篇文章将其扩展到了存储设备层面&lt;/p&gt;
&lt;p&gt;KevinFS没有实现dentry，如果需要访问一个目录内的所有项目，可以通过&lt;code&gt;ITERATE(m:50:, 2)&lt;/code&gt;，来获取两个父目录inode为50（前缀匹配给定的pattern）的子目录/文件的元数据。&lt;/p&gt;
&lt;p&gt;一个优化：为了防止ITERATE消耗太长时间，建议指定cnt&lt;/p&gt;
&lt;p&gt;一个优化：为了高效处理小文件，KevinFS将总体小于4KB的小文件的元数据和数据内容打包，I/O直接操作meta对象GET/SET来进行读写&lt;/p&gt;
&lt;p&gt;@TODO 一个优化：使用全路径索引来取代基于inode的索引，这提高了基于排序算法的KV存储的扫描性能，主要对seek time开销大的HDD优化比较明显&lt;/p&gt;
&lt;h2 id=&#34;kv对象索引&#34;&gt;KV对象索引&lt;/h2&gt;
&lt;p&gt;KevinSSD基本实现了传统SSD 中FTL的所有功能，将KV对象映射到闪存、分配和释放闪存的空间。&lt;/p&gt;
&lt;p&gt;FTL只做坏块管理和磨损均衡等简单工作。&lt;/p&gt;
&lt;p&gt;每个Level维护一个内存表，来记录Flash中的KV对象。表的每个记录都有&lt;code&gt;&amp;lt;start key, end key, pointer&amp;gt;&lt;/code&gt;，其中指针指向保存KV对象的闪存页面的位置，start key和end key是页面中key的范围。key的范围可以在多个Level上重叠。为了快速查找，所有记录都按开始键排序。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221102165350981.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102165350981&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;meta和data对象的Key和Value会被分离存储。通过&lt;code&gt;&amp;lt;key, value pointer&amp;gt;&lt;/code&gt;的方式进行存储，会有专门的flash page来分别存储这些信息（meta和data对象使用不同的flash page），key-index pages&lt;/p&gt;
&lt;h3 id=&#34;meta-object&#34;&gt;Meta object&lt;/h3&gt;
&lt;p&gt;在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。&lt;/p&gt;
&lt;p&gt;对目录项的更新只需要修改其元数据即可完成，而不需要修改4KB的块&lt;/p&gt;
&lt;h3 id=&#34;data-object&#34;&gt;Data object&lt;/h3&gt;
&lt;p&gt;为了避免对大数据的小范围更新带来的I/O高负载，KevinSSD将data object分割为4KB大小的子对象，使用单独的后缀来对他们进行表示，后缀从0开始，拼接在原始的data object Key后，因此data object的key最终构成为
$$
d:{inode\ \ \ number}:{subobject \ \ number,start\ with \ 0}
$$
不需要中间表进行索引，只需要通过偏移量即可定位要修改的对象&lt;/p&gt;
&lt;p&gt;Data object同样使用保存指针的方式将映射存储在key-index pages中，key-index pages会对键进行排序，因此属于同一个文件的subobject一般会在同一个flash page中。&lt;/p&gt;
&lt;p&gt;==优化==：在查找指针时，会将一整个闪存页内的数据全部取出，存储在控制器的DRAM中，减轻后续查找可能带来的负载&lt;/p&gt;
&lt;h2 id=&#34;缓解索引负载&#34;&gt;缓解索引负载&lt;/h2&gt;
&lt;p&gt;使用LSM-tree由于多级查找带来额外的I/O，传统FTL的映射都保存在DRAM，因此没有额外开销。作者介绍了三种主要原因和解决方案。&lt;/p&gt;
&lt;h3 id=&#34;压缩操作的开销&#34;&gt;压缩操作的开销&lt;/h3&gt;
&lt;p&gt;同上文介绍的方案，通过分离Value可以显著缓解I/O的数据传输负载，对data进行分片我个人感觉反而会加大压缩操作的合并过程，即使分片后键值都是有序的。&lt;/p&gt;
&lt;h3 id=&#34;层级查找的开销&#34;&gt;层级查找的开销&lt;/h3&gt;
&lt;p&gt;由于LSM-tree的特性，需要逐层进行查找，为了防止大量的顺序查找，作者使用布隆过滤器进行了优化。&lt;/p&gt;
&lt;p&gt;同时还缓存了热点K2V索引数据（与目标索引处在同一个flash page中的）&lt;/p&gt;
&lt;p&gt;为了利用大容量SSD中提供的超大DRAM，作者采用压缩存储K2V索引，并在其中插入没有压缩的K2V索引来作为二分查找的参照物。（快速在缓存中查找？不如继续用布隆过滤器）&lt;/p&gt;
&lt;h3 id=&#34;分散的对象带来的开销&#34;&gt;分散的对象带来的开销&lt;/h3&gt;
&lt;p&gt;LSM-tree允许各层之间的key范围重叠，因此有同一个前缀的目录或者文件可能被分配到不同的Level中，对于获取一个目录中所有目录、文件的操作，需要对多个闪存页进行访问。这个问题作者没有给出一个明确的方案（在压缩合并时隐式解决），但是提供了一个用户工具来主动触发合并，效率较高。&lt;/p&gt;
&lt;p&gt;作者对比了各种方法在随机读、局部读（？）、顺序读情况下对闪存页的读取次数，使用布隆过滤器的时候稳定会有一次读取&lt;/p&gt;
&lt;p&gt;使用了KevinSSD后对于SSD而言，I/O延迟有轻微的增加，整体而言效率提高了&lt;/p&gt;
&lt;h1 id=&#34;0x03-implement-vfs&#34;&gt;0x03 Implement VFS&lt;/h1&gt;
&lt;h2 id=&#34;write&#34;&gt;write&lt;/h2&gt;
&lt;p&gt;所有的写相关系统调用可以通过SET和DELETE来实现。&lt;/p&gt;
&lt;p&gt;例如unlink，只需要两次DELETE指令，移除meta和data object。&lt;/p&gt;
&lt;p&gt;SET时现在Memtable中保存一个KV对象，然后持久化到flash中，若Key已存在则丢弃旧的对象&lt;/p&gt;
&lt;p&gt;DELETE时在树上写入一个4B大小的墓碑&lt;/p&gt;
&lt;p&gt;失效的对象（被SET覆盖）和删除的对象在压缩期间被永久删除&lt;/p&gt;
&lt;h2 id=&#34;read&#34;&gt;read&lt;/h2&gt;
&lt;p&gt;通过GET和ITERATE实现&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;p&gt;执行open系统调用时，查询meta object获取文件的inode，通过GET命令实现&lt;/p&gt;
&lt;p&gt;执行lookup系统调用时，给定一个完整的路径&lt;code&gt;/home/alice/&lt;/code&gt;，从根目录开始获取多个meta object，最终获取目标文件的inode&lt;/p&gt;
&lt;p&gt;执行read系统调用时，将GET一个data object&lt;/p&gt;
&lt;p&gt;执行readdir系统调用时，使用ITERATE，批量获取一组meta objects来获取inode&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221102210344432.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102210344432&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;相较于Ext4，KevinFS可以更好的防止文件碎片化带来的影响。&lt;/p&gt;
&lt;h1 id=&#34;0x04-crash-consistency&#34;&gt;0x04 Crash Consistency&lt;/h1&gt;
&lt;h2 id=&#34;一致性维护&#34;&gt;一致性维护&lt;/h2&gt;
&lt;p&gt;通过事务即可实现原子化的操作&lt;/p&gt;
&lt;p&gt;在传统的日志结构中，由于事务的大小受日志大小的限制，一个系统调用可能会被划分到多个事务中。KVFS通过强制限制对一个文件的操作驻留在同一个事务中来确保原子性。&lt;/p&gt;
&lt;p&gt;事务的隔离？对==fsync==指令单独创建一个小型事务来提高效率。&lt;/p&gt;
&lt;p&gt;KevinFS基于KV事务的特性来维护一致性，不使用日志系统。&lt;/p&gt;
&lt;h2 id=&#34;kv事务的实现&#34;&gt;KV事务的实现&lt;/h2&gt;
&lt;p&gt;使用了三个数据结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务表$TxTable$：记录事务基本信息&lt;/li&gt;
&lt;li&gt;事务日志$TxLogs$：维护事务对象的K2V索引，存储在DRAM或者闪存中&lt;/li&gt;
&lt;li&gt;恢复日志$TxRecovery$：用于恢复和终止事务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是收到BeginTx指令时，KevinSSD在TxTable中创建一个对象，包含TID，当前事务的状态，与事务相关K2V索引的位置，初始状态为RUNNING（如①）。当后续指令到达后，KevinSSD将KV索引记录在TxLogs，并缓存在Memtable中。&lt;/p&gt;
&lt;p&gt;当收到EndTx指令后，完成事务的提交，标记事务状态为COMMITTED（如②）。将提交的KV索引同步到LSM-tree中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221103102304181.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221103102304181&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;为了减少在COMMITTED阶段，将TxLogs中的KV索引同步到LSM tree L1中时的负载，对每个事务在DRAM中维护了一个Skiplist，便于快速查找KV对象在TxLogs中的索引，在L1和L2需要进行压缩时再将skiplist中的数据同步到checkpoint，当同步完成后，事务的状态将变为CHECKPOINTED（如③），最终即将对DRAM中的TxTable和TxLogs进行回收。&lt;/p&gt;
&lt;h2 id=&#34;recovery&#34;&gt;Recovery&lt;/h2&gt;
&lt;p&gt;若发生掉电，KVSSD会利用电容将KV索引持久化到闪存中的TxLogs，TxTable更新内部的指针，在TxRecovery中进行记录。在系统重启时，KVSSD会扫描Tx Recovery，加载最新的TxTable，对于处于COMMITTED状态的事务，将其TxLogs索引加载到Skiplist中，对于处于RUNNING状态的事务，直接进行Abort并回收其资源。&lt;/p&gt;
&lt;h1 id=&#34;conclusion--thinking&#34;&gt;Conclusion &amp;amp;&amp;amp; Thinking&lt;/h1&gt;
&lt;p&gt;本文提出了一种基于K/V存储架构的文件系统，通过对Key的设计保证了在I/O操作中的高效性。通过本文了解了一些KV存储的trick，对于KV存储的各种场景，无论是在Redis中还是在KV文件系统中，对于Key的设计都至关重要，好的设计往往能够带来极大的效率提升，例如布隆过滤器的应用，有很多经验值得学习。同时了解了对事务的一些控制机制。&lt;/p&gt;
&lt;p&gt;此篇论文让我惊讶于文件系统还能通过KV存储来实现的通知，还让我回想起了TiDB，通过KV存储系统来实现了分布式关系型数据库，并完全兼容MySQL，说明在利用好KV存储的特性的情况下，可以有效的解决问题，要多去学习他们的设计思想。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Mysql学习笔记</title>
        <link>https://blog.ipandai.club/p/mysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Mon, 17 Jan 2022 23:16:08 +0000</pubDate>
        
        <guid>https://blog.ipandai.club/p/mysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h1 id=&#34;基础知识&#34;&gt;基础知识&lt;/h1&gt;
&lt;p&gt;单进程多线程，线程之间共享内存&lt;/p&gt;
&lt;p&gt;OLTP Online Transaction Processing 在线事务处理&lt;/p&gt;
&lt;h1 id=&#34;innodb&#34;&gt;InnoDB&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;overview&lt;/h2&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;行级锁&lt;/li&gt;
&lt;li&gt;支持外键&lt;/li&gt;
&lt;li&gt;支持事务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MVCC并发控制，插入缓冲，二次写，自适应哈希索引，预读&lt;/p&gt;
&lt;p&gt;四种隔离级别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;REPEATABLE 默认&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;聚集方式保存数据，按主键顺序存放，没有主键则生成6字节的ROWID&lt;/p&gt;
&lt;h2 id=&#34;后台线程&#34;&gt;后台线程&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Master Thread 缓冲区数据异步更新到磁盘&lt;/li&gt;
&lt;li&gt;IO Thread innodb使用异步IO，提高数据库性能，负责接受回调&lt;/li&gt;
&lt;li&gt;Purge Thread 事务提交后回收undolog页&lt;/li&gt;
&lt;li&gt;Page Cleaner Thread 脏页刷新&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;主要都是为了减轻Master的负担，提高性能，减少对用户查询线程的阻塞&lt;/p&gt;
&lt;h2 id=&#34;内存&#34;&gt;内存&lt;/h2&gt;
&lt;p&gt;用页管理记录&lt;/p&gt;
&lt;p&gt;缓冲池：通过内存来弥补低速硬盘的影响，数据库读取页，从磁盘获取页放入缓冲池&lt;code&gt;fix&lt;/code&gt;，读取时先判断缓冲区，命中后直接读取；修改时先修改缓冲池的页，然后通过checkpoint机制刷新到磁盘上。基本上和虚拟内存一样。&lt;/p&gt;
&lt;p&gt;配置参数&lt;code&gt;innodb_buffer_pool_size&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;数据页分类：索引页，数据页，undo页，插入缓冲，自适应哈希索引，innodb锁信息，数据字典信息等&lt;/p&gt;
&lt;p&gt;允许有多个缓冲池&lt;/p&gt;
&lt;h3 id=&#34;内存管理&#34;&gt;内存管理&lt;/h3&gt;
&lt;h4 id=&#34;lru-list&#34;&gt;LRU List&lt;/h4&gt;
&lt;p&gt;频繁使用的在列表前，不频繁的在列表后，先释放尾部的页&lt;/p&gt;
&lt;p&gt;页默认大小16KB&lt;/p&gt;
&lt;p&gt;innodb会把新读取的页放入midpoint位置，为列表长度的$$\frac{5}{8}$$，midpoint之后为old，之前为new，如果放在首部会导致某些sql导致缓冲池页被刷新，如全表扫描遍历全表。防止一条指令拖慢其他指令效率。&lt;/p&gt;
&lt;p&gt;Free列表保存空闲页&lt;/p&gt;
&lt;h1 id=&#34;myisam&#34;&gt;MyISAM&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;表锁&lt;/li&gt;
&lt;li&gt;全文索引&lt;/li&gt;
&lt;li&gt;不支持事务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缓冲区只缓存索引文件，不缓冲数据&lt;/p&gt;
&lt;p&gt;MYD保存数据，MYI保存索引文件&lt;/p&gt;
&lt;p&gt;myisampack使用Huffman编码压缩MYD，压缩后只读&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
