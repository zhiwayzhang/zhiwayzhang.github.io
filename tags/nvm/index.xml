<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>NVM - Tag - Coding Panda&#39;s Blog</title>
    <link>https://zhiwayzhang.github.io/tags/nvm/</link>
    <description>NVM - Tag - Coding Panda&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</managingEditor>
      <webMaster>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</webMaster><lastBuildDate>Mon, 19 Dec 2022 16:50:00 &#43;0800</lastBuildDate><atom:link href="https://zhiwayzhang.github.io/tags/nvm/" rel="self" type="application/rss+xml" /><item>
  <title>[FAST&#39;22] MT^2: Memory Bandwidth Regulation on Hybrid NVM/DRAM Platforms</title>
  <link>https://zhiwayzhang.github.io/posts/mt2/</link>
  <pubDate>Mon, 19 Dec 2022 16:50:00 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/mt2/</guid>
  <description><![CDATA[0x00 Intro NVM和DRAM共享内存总线，二者之间的负载相互干扰，给混合NVM/DRAM平台的带宽分配带来了挑战
本文提出了MT2，可以在混合NVM/DRAM平台中管理并发程序间的内存带宽。
MT2首先检查内存流量间的干扰并通过硬件监视器和软件汇报从混合流量监控不同类型的内存带宽
然后MT2利用一个动态的带宽流量调节算法基于多种方式来管理内存带宽
为了能够更好的管理不同程序，MT2被集成到了cgroup中，添加了一个用于带宽分配的cgroup subsystem
新兴的NVM存储器逐渐被用来做为可持久化内存来使用，基于NVM，提出了NVM文件系统，NVM编程库，NVM数据结构、NVM数据库，NVM作为大容量内存或者快速的字节寻址存储设备用于数据中心。
然而NVM/DRAM混合平台加剧了吵闹邻居问题。在云存储环境中，多个用户常常共享一个服务器，不同用户的不同应用程序共享主机的一条总线，某些应用可能会过度使用内存带宽。在NVM/DRAM中，NVM和DRAM共享同一个总线，因此不同的应用程序将竞争有限的内存带宽，影响整体的性能
在NVM/DRAM中调节带宽有如下几个问题：
内存带宽不对称，在NVM/DRAM，不同的内存访问（如DRAM read，DRAM write，NVM read和NVM write）会产生不同的最大内存带宽。内存的实际可用带宽主要取决于负载中不同类型访问的占比。同时设置静态的内存带宽而不考虑实际的I/O访问占比是不正确的做法。同时NVM最大带宽通常小于DRAM。而且不同类型的内存访问的干扰程度不同，因此不能单纯将所有内存==访问延迟==视为相等 NVM和DRAM共享内存总线，NVM流量和DRAM流量不可避免地会混合并且很难区分。对于混合的流量，监控不同种类的内存带宽。由于内存流量混合，几乎不可能在每个进程的基础上监控不同类型的内存带宽，这使为DRAM设计的现有硬件和软件调节方法无效。 内存调节的硬件和软件机制不足。由于NVM和DRAM都可以通过CPU负载/存储指令直接访问，因此为了性能，对每个内存访问进行计算和限流是不切实际的。CPU供应商，如英特尔，支持硬件机制来调节内存带宽。然而，带宽限制是粗粒度和定性迭代的，这不足以精确的内存带宽调节。其他一些方法，如频率缩放和CPU调度，可能会提供相对细粒度的带宽调整。然而，它们也是定性的，并减慢了计算和内存访问的速度，因此对整个平台性能缺乏影响。 MT2作为Linux Cgroup中的一个Subsystem，用于减轻吵闹邻居问题。在内存带宽分配和云SLO保证方面提高效率。本文的主要贡献：
发现了导致NVM/DRAM混合平台上内存密集型应用程序显著性能流失的内存带宽干扰问题 首次对在NVM/DRAM平台现存的硬件和软件带宽分配机制进行研究 高效有效地调节具有线程级粒度的NVM/DRAM混合平台上的内存带宽 在英特尔Optane SSD上进行了测试和分析 以下简称NVM/DRAM
0x01 BG MT^2 持久内存（PM）/ 非易失性内存（NVM）的出现正改变着存储系统的金字塔层次结构。本文发现，由于 NVM 和 DRAM 共享同一条内存总线，带宽干扰问题变得更为严重和复杂，甚至会显著降低系统的总带宽。本工作介绍了对内存带宽干扰的分析，对现有软硬件技术进行了深入调研，并提出了一种在 NVM/DRAM 混合平台上监控调节并发应用的内存带宽的设计（MT^2）。MT^2 以线程为粒度准确监测来自混合流量的不同类型的内存带宽，使用软硬件结合技术控制内存带宽。在多个不同的用例中，MT^2 能够有效限制“吵闹邻居”（noisy neighbors），消除带宽干扰，保证高优先级应用的性能。
Noisy Neighbors 在多租户的云环境中，内存带宽对应用程序的性能有很大影响。
两种可以减轻吵闹邻居问题的方法是：
Prevention：主动为应用程序设置带宽限制（固定的） Remedy：系统检测吵闹邻居情况的出现并识别出吵闹的“邻居”对其进行限制 这些方法需要去监控应用的带宽使用情况
NVM NVM得益于其存储容量大，访问速度快的特性，正在逐步作为内存使用于商业服务器中。得益于PMDK（Persistent Memory Development Kit）同时还涌现出了大量基于NVM Memory的应用程序，如PmemKV，Pmem-RocksDB。
NVM可以直接通过CPU的load/store指令进行访问
Memory Bandwidth Interference 由于NVM和DRAM共享内存总线，因此存在干扰问题。
用两个Flexible I/O Tester来竞争总带宽进行测试，包括NVM/DRAM的读写，分别比较在不同的访问方式下的带宽干扰情况
如下图所示，颜色越深表明干扰越明显，图中数据表示为在Task B运行的情况下Task A的吞吐量占Task Alone情况下的百分比（GB/s）
从图中得出的两个结论为：
内存的干扰和内存的访问情况有关，占据更小带宽的内存访问可能会对其他访问造成更大的影响 NVM的访问比DRAM访问对其他任务的影响更大，例如图中NVM Read列和DRAM Read列的对比，说明执行NVM write较多的应用更可能成为影响其他任务 作者还做了Task B进行NVM Write，Task A的延迟和吞吐量的变化，发现了随着带宽的逐渐降低，Task A的延迟逐渐增加。]]></description>
</item>
<item>
  <title>【存储技术基础】主存</title>
  <link>https://zhiwayzhang.github.io/posts/mainmemory/</link>
  <pubDate>Sat, 24 Sep 2022 18:26:22 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/mainmemory/</guid>
  <description><![CDATA[0x00 主存 当前面临的问题：
容量问题、带宽问题、QoS保证
多核处理器、数据密集应用、云计算、GPU
HDFS（GFS）基于外存储器
Spark 内存中数据处理
能耗功率问题
40%功耗在DRAM，Refresh操作耗能
DRAM发展遇到瓶颈
制成方式限制
改进：
3D-Stack DRAM 提供更高的带宽 低延迟DRAM 低功耗DRAM NVM（e.g. PCM）容量更大，延迟较高 0x01 DRAM的组成 Channel DIMM 内存条 Rank 二维阵列最小单元 Chip 芯片 Bank Row/Column 行为字线 word line
列为位线，交点是cell 内存单元
DRAM row是一个DRAM page
Sense Amplifiers 也被叫做Row buffer
每个地址通过&lt;row,colum&gt;编址
访问一个closed row的过程：
Activate：将row放到row buffer中 Read/Write：读写row buffer中的column Precharge：从row buffer中的数据写回到选中的row中 Bank Operation 给定Row Address通过Row decoder选中一行
将行加载到Row Buffer
若读取的Row在Row Buffer中，则为命中Hit状态，通过Column Mutex直接获取数据
若读取的Row不在Row Buffer中，则为冲突Conflict状态，将Row Buffer回写，然后再选中新的Row，读取数据
Chip 由多个(2-16个)Bank组成，Bank共享总线（指令/地址/数据总线）
每次只能读4或16 Bit
Rank and Module Module为DIMM内存条，连接在主板上，由一个或多个rank组成]]></description>
</item>
<item>
  <title>【存储技术基础】固态硬盘</title>
  <link>https://zhiwayzhang.github.io/posts/storagessd/</link>
  <pubDate>Thu, 22 Sep 2022 09:36:42 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/storagessd/</guid>
  <description><![CDATA[主要由Flash Memory 和FTL组成
Non-Volatile Memory 提供低延迟持久性的内存/存储，也可以用来做内存
根据延迟数量级，一般用PCM做内存，Flash Memory做外存
0x00 Flash Memory 闪存原理 类型 NOR闪存
存储密度低 可字节改写 NAND闪存（主流）
存储密度高 不可覆盖写 用于外存需要较高的存储量级，一般用NAND
闪存单元 读：电压代表不同数值
写：电子注入
相比晶体管添加了浮栅门，保存电子
原理其实比较简单，非电子系就不做太详细的研究了
闪存页(4KB,8KB,16KB，读写单元)，阵列中的每一行
闪存块(擦除单元)，由多个页组成的单元
选中行和列，然后将数据加载到Sense Amplifiers
存储单元有两个阈值的电压，可以根据两个电压的中点作为读电压，2.5V读电压时左边通电，数据为1，右边则不通电，数据为0
Pass Through
选取一个较大的电压，使得所有的单元都接通，数据为1，不影响其他行的状态
如图所示的存储结构，在第二行施加2.5V电压，其他行施加5V，最终读取数据为0011
上述为SLC，Single Level Cell，单存储单元
多比特闪存 多比特闪存单元MLC，包含2Bits 4个Level的数据
TLC 3 Bits 8个Level
QLC 4 Bits 16个Level
多比特使用格雷码来编码
使用格雷码使得相邻单元只有一位差异，方便纠错
多比特提高了存储密度，但是提高了错误率，因为施加的电压差距很小。可靠性会降低。
对于多比特的写，MLC分为高比特和低比特，对于低比特的状态加偏移电压确定高比特，在低比特时需要加的电压较大，操作难度低，运行速度快，在高比特时需要加的电压小，波形的间距小，操作难度高，运行的速度较慢。
对于多比特的读，先看lower bit，加一次电压，即可筛选出低位的0，1，再加两次电压确定upper bit。因为upper bit为0的在中间部分，为1的在两侧，因此需要在两个分界线分别加一次电压来确定upper bit为多少。
闪存 Block的大小的一种配置：
一行有两个Page，Upper Page和Lower Page，每个单元中，低位构成Lower Page，高位构成Upper Page，有128个单元，128K/8=16KB
有64列bitlines，一个block的大小即为16KB*64*2=2MB，一般按照此比例配置Block
写入时按照固定顺序，写入高低页面相互独立，不能同时写，在写入加压时容易使相邻单元发生数据偏移，要降低错误率
写入是需要先擦除再写入，擦除整个块
特性 读写粒度 闪存页读写粒度：]]></description>
</item>
</channel>
</rss>
