<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Paper on Coding_Panda&#39;s Blog</title>
        <link>https://blog.ipandai.club/tags/paper/</link>
        <description>Recent content in Paper on Coding_Panda&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 17 Mar 2023 23:30:48 +0800</lastBuildDate><atom:link href="https://blog.ipandai.club/tags/paper/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[FAST&#39;23] HadaFS: A File System Bridging the Local and Shared Burst Buffer for Exascale Supercomputers</title>
        <link>https://blog.ipandai.club/p/fast23-hadafs-a-file-system-bridging-the-local-and-shared-burst-buffer-for-exascale-supercomputers/</link>
        <pubDate>Fri, 17 Mar 2023 23:30:48 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/fast23-hadafs-a-file-system-bridging-the-local-and-shared-burst-buffer-for-exascale-supercomputers/</guid>
        <description>&lt;p&gt;HadaFS，一个为超算提供本地和共享Burst Buffer的文件系统&lt;/p&gt;
&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;现代的超算通过SSD来实现Burst Buffer（BB）layer。&lt;/p&gt;
&lt;p&gt;根据部署位置，BB可以分为两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local BB，作为本地硬盘部署在每个计算节点上，有高伸缩性和性能&lt;/li&gt;
&lt;li&gt;Shared BB，部署在专用节点上，可以被多个计算节点访问，可以共享数据，部署成本低&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HadaFS已经部署在了神威新一代超算（Sunway New- generation Supercomputer，SNS）中。支持最大600000用户，最大I/O带宽3.1TB/s。&lt;/p&gt;
&lt;p&gt;Burst Buffer作为数据加速层，一般使用SSD。自2016年起，越来越多的超算开始使用Burst Buffer。&lt;/p&gt;
&lt;p&gt;对于local BB，有一些局限性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由于难以进行数据共享，Local BB不适用于所有场景，例如N-1 I/O mode，所有进程共享一个文件、workflow&lt;/li&gt;
&lt;li&gt;由于超算程序之间的I/O负载差异较大，而数据密集型应用的比例较低，造成了大量的资源浪费&lt;/li&gt;
&lt;li&gt;随着超算规模的扩大，local BB的部署成本未来会急剧升高&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相比于Local BB，Share BB便于数据共享，部署成本低，但是在超算上部署也面临许多问题。LPCC将SSD集成到Lustre FS Client，提高read/write性能的一种缓存技术，LPCC存储在Lustre client SSD中的数据必须先刷新到Lustre server才能进行共享。BeeOND类似于LPCC，继承了BeeGFS的可伸缩性和缓存共享限制。&lt;/p&gt;
&lt;p&gt;超算的发展使得并行I/O需求增加，BB架构相较于传统GFS有着同样的高性能，但是容量较小，因此BB要与GFS进行整合。目前的BB架构数据迁移的效率很低，浪费了很多计算资源，因此高伸缩性的BB数据管理和迁移也是目前要解决的问题。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，作者提出了BB File System&amp;ndash;HadaFS。&lt;/p&gt;
&lt;p&gt;基于share BB，结合了local BB的伸缩性、性能优势和share BB的数据共享、部署成本低的优点。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HadaFS提供Localized Triage Architecture（LTA）局部分类架构，解决了shared BB伸缩性不足的问题，实现了超大规模的扩展和数据共享，LTA将所有HadaFS server构建为一个共享存储池，可以在client和server之间灵活的控制并发问题，来保证数据共享。&lt;/li&gt;
&lt;li&gt;HadaFS提出了一个运行时的user-level接口，来保证来自client的I/O请求可以被最近的server处理，类似local BB。&lt;/li&gt;
&lt;li&gt;为了解决由POSIX接口强一致性导致的性能问题，HadaFS提出了一种包含三种元数据同步机制的全路径索引方法，来解决传统文件系统在复杂元数据的管理上的问题，以及文件系统和应用I/O行为不匹配的问题。使用KV的方法来代替传统的目录树结构。&lt;/li&gt;
&lt;li&gt;HadaFS集成了数据管理工具，帮助用户管理BB中的数据，完成BB和GFS之间的高效数据迁移。&lt;/li&gt;
&lt;li&gt;提出Hadash，在BB中提供高效的数据查询，加速BB和传统超算存储的数据迁移。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x01-motivation--bg&#34;&gt;0x01 Motivation &amp;amp;&amp;amp; Bg&lt;/h1&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;h3 id=&#34;bb可伸缩性和应用行为的矛盾&#34;&gt;BB可伸缩性和应用行为的矛盾&lt;/h3&gt;
&lt;p&gt;随着超算百亿亿次计算记录的打破，尖端超算中的I/O并行可以达到数十万，加大了BB在伸缩性上的压力。&lt;/p&gt;
&lt;p&gt;目前的一些顶尖超算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frontier使用独立的硬件来分别构造local BB和shared BB，需要使用大量的SSD和高昂的建设维护成本。&lt;/li&gt;
&lt;li&gt;Fugaku使用shared BB，使用软件来提供存储服务，local BB和shared BB使用不同的name space。这种方式是静态的，很难控制在高并发情况下的I/O竞态问题。&lt;/li&gt;
&lt;li&gt;Summit部署local BB，支持数据在应用之间的共享，基于GFS，效率较低。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于shared BB既可以用于计算节点，也可以用于数据转发节点，作者相信shared BB更适合超级计算机。&lt;/p&gt;
&lt;h3 id=&#34;复杂元数据管理与应用行为的不匹配&#34;&gt;复杂元数据管理与应用行为的不匹配&lt;/h3&gt;
&lt;p&gt;传统的文件系统需要考虑兼容性，因此严格遵循POSIX协议。超算中，计算节点普遍使用read/write，执行目录树访问的次数较少。&lt;/p&gt;
&lt;p&gt;因此减轻对POSIX的实现成为很多文件系统的优化方向。由于应用程序的种类很多，如何减少POSIX接口面临巨大挑战。&lt;/p&gt;
&lt;p&gt;Wang等人分析了一些HPC应用的行为，整理了几种一致性语义，如下表Table 1&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322160638470.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322160638470&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong consistency&lt;/li&gt;
&lt;li&gt;Commit consistency&lt;/li&gt;
&lt;li&gt;Session consistency&lt;/li&gt;
&lt;li&gt;Eventual consistency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;系统支持的一致性等级越高，系统的适应性越强。也就是说，当一个系统能够保证数据在不同节点之间保持一致，那么它就能更好地适应不同的应用场景和需求。&lt;/p&gt;
&lt;p&gt;需要灵活地选择合适的一致性语义来平衡需求和BB系统的性能。&lt;/p&gt;
&lt;h3 id=&#34;低效的数据管理&#34;&gt;低效的数据管理&lt;/h3&gt;
&lt;p&gt;虽然使用BB加速了I/O，但是BB的利用率很低。&lt;/p&gt;
&lt;p&gt;BB仅用于I/O的加速，并不永久的存储数据，同时BB的容量相较于GFS很小，因此要考虑BB和GFS之间高效的数据传输。&lt;/p&gt;
&lt;p&gt;BB和GFS之间的数据迁移/传输有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;透明迁移，软件自动以blocks或者file的形式将BB中的数据迁移到GFS，造成了大量不必要的数据传输&lt;/li&gt;
&lt;li&gt;非透明迁移，计算节点来实现数据迁移，导致计算资源在数据迁移时产生空闲。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两种数据传输方式都会异步地提前将数据从GFS加载到BB，以达到预取的目的。&lt;/p&gt;
&lt;p&gt;然而他们都不能支持用户在程序运行时动态的管理BB数据迁移，不利于提高BB的利用率。&lt;/p&gt;
&lt;h2 id=&#34;bg-神威超算架构&#34;&gt;Bg-神威超算架构&lt;/h2&gt;
&lt;p&gt;SNS神威超算的架构图如下&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322164156234.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322164156234&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每个计算节点包含一个神威多核处理器SW26010P（自研），采用多种架构，有6个Core Groups，390个计算核心。通过环网相连接。&lt;/p&gt;
&lt;p&gt;整个系统有超过100000个SW26010P处理器，通过一种Fat-tree 网络 SWnet连接。&lt;/p&gt;
&lt;p&gt;计算节点与I/O转发节点连接，I/O转发节点提供I/O请求的转发或存储介质。&lt;/p&gt;
&lt;p&gt;SNS使用类似太湖之光的软件架构（LWFS+Lustre），通过独立的存储网络来链接存储节点，以提供I/O请求转发。&lt;/p&gt;
&lt;p&gt;当提供存储服务时，SNS使用HadaFS，I/O转发节点作为HadaFS的server，使用NVMe SSD来处理用户的I/O。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我个人感觉，Burst Buffer就类似于I/O栈中的Page Cache，HadaFS处理的就是Buffer I/O。&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x02-design&#34;&gt;0x02 Design&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;下图为HadaFS的架构，包括HadaFS client，HadaFS server和数据管理工具。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322170237986.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322170237986&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;HadaFS作为shared BB文件系统，提供所有client的global view。&lt;/p&gt;
&lt;p&gt;HadaFS Client运行于计算节点，并提供静态/动态链接库，用于拦截并转发应用程序的POSIX I/O请求到HadaFS Server，HadaFS client的生命周期取决于应用程序。&lt;/p&gt;
&lt;p&gt;HadaFS不支持move，rename，link操作，这些操作很少在并行系统中使用。&lt;/p&gt;
&lt;p&gt;HadaFS Server运行于Burst Buffer节点，包含NVMe SSD，提供全局的数据和元数据存储服务。&lt;/p&gt;
&lt;p&gt;在HadaFS中，每个文件与两种服务器有关。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据存储服务器：通过NVMe SSD上的基本文件系统存储HadaFS文件数据。&lt;/li&gt;
&lt;li&gt;元数据存储服务器：通过高性能的数据库存储HadaFS文件的元数据（使用RocksDB）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hadash是数据管理工具，运行于用户登录节点，用于管理HadaFS和GFS之间的数据迁移。&lt;/p&gt;
&lt;h2 id=&#34;lta-localized-triage-architecture&#34;&gt;LTA (Localized Triage Architecture)&lt;/h2&gt;
&lt;p&gt;kernel在实现完整POSIX接口和处理I/O时有较大的开销，kernel bypass是一种常用手段。&lt;/p&gt;
&lt;p&gt;在高并发的情况下，kernel bypass可能会导致服务不稳定。&lt;/p&gt;
&lt;p&gt;例如，计算节点有24CPU核心，所有的进程只需要进行一次挂载就可以访问运行于kernel mode的文件系统client。相反，每个进程需要在用户层挂载一个文件系统client。两种方法各有优点和局限性，HadaFS通过LTA将两种优点进行整合。&lt;/p&gt;
&lt;p&gt;HadaFS直接挂载Client到应用，实现kernel bypass。&lt;/p&gt;
&lt;p&gt;为了防止单个server处理的FS client过多，HadaFS采用每个client只连接一个server的方法。&lt;/p&gt;
&lt;p&gt;对于一个HadaFS client，与之相连的HadaFS server被称为bridge server，bridge server负责处理client的所有I/O请求，并根据client I/O request的offset和size写入数据到文件。每个文件对应在bridge server ext4 文件系统中的一个独立文件。当client需要访问其他服务器的数据时，文件必须通过bridge server转发。&lt;/p&gt;
&lt;p&gt;如果一个bridge server的空间已满，所有的client将自动切换到其他server。&lt;/p&gt;
&lt;p&gt;为了保证大部分I/O请求在bridge server中处理并减少转发的次数，HadaFS提出了&lt;code&gt;mount(mount_point, Seq)&lt;/code&gt;接口来允许应用在必要时（&lt;strong&gt;自己选择bridge server有优势时&lt;/strong&gt;）控制bridge server的选择。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mount_point表示挂载点，在HadaFS中这是一个文件路径的前缀&lt;/li&gt;
&lt;li&gt;Seq可以根据&lt;strong&gt;应用数据共享模式&lt;/strong&gt;、&lt;strong&gt;网络拓扑&lt;/strong&gt;、&lt;strong&gt;适应应用程序数据和系统架构并行性的其他因素&lt;/strong&gt;来灵活设置。Seq有三种类型：
&lt;ul&gt;
&lt;li&gt;应用程序&lt;code&gt;MPI_RANK&lt;/code&gt;，client将会按照轮询的方式连接server。适用于应用被多次提交到不同的计算节点的情况，来保证每个应用可以准确连接到原先的bridge server，减少数据转发。&lt;/li&gt;
&lt;li&gt;计算节点ID，用于匹配计算节点和BB节点的拓扑结构，保证计算节点可以将数据存储到网络中最近的BB节点。&lt;/li&gt;
&lt;li&gt;根据应用数据分发和共享需要设置，每个client可以显示指定要连接的server，应用可以通过灵活的控制client到server的映射改善数据访问的效率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LTA通过bridge server作为local BB，还实现了shared BB的功能（数据共享）。&lt;/p&gt;
&lt;p&gt;mount_point，Seq可以通过环境变量设置，因此，HadaFS可以通过在应用程序开始之前加载HadaFS lib以提前读取环境变量来支持用户的透明挂载。&lt;/p&gt;
&lt;p&gt;为了充分利用HadaFS的高性能，尤其是read性能，建议用户执行实现client-to-server的映射关系，减少数据转发。在挂载HadaFS之后，应用可以直接调用POSIX文件接口来实现I/O。&lt;/p&gt;
&lt;h2 id=&#34;namespace--metadata-handing&#34;&gt;Namespace &amp;amp;&amp;amp; metadata handing&lt;/h2&gt;
&lt;p&gt;为了改善伸缩性和性能，HadaFS不使用目录树的索引机制，使用类似CHFS和Vesta的full-path索引。&lt;/p&gt;
&lt;p&gt;对一个HadaFS文件，其数据保存在client中的bridge server，元数据存储位置由路径的hash决定。&lt;/p&gt;
&lt;p&gt;文件的元数据以KV的方式存储，每个文件路径都有全局唯一的ID作为key。&lt;/p&gt;
&lt;p&gt;HadaFS client直接通过文件绝对路径的前缀来检查文件是否符合要求。&lt;/p&gt;
&lt;p&gt;当多个文件需要访问，负载可以分发到多个服务器，改善元数据访问的性能。&lt;/p&gt;
&lt;p&gt;HadaFS的元数据兼容Linux的权限结构，包括name，ino，owner，mode，timestamp等。HadaFS将这些信息分成四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;维护文件的创建信息，name，owner，mode等&lt;/li&gt;
&lt;li&gt;维护文件的访问信息，file size，modification time，access time等&lt;/li&gt;
&lt;li&gt;❌因为使用全局唯一ID，HadaFS不需要维护，ino，stdev等信息&lt;/li&gt;
&lt;li&gt;文件分段的位置信息，是一个有序链表，链表通过offset排序，每个元素包括server name，fragment offset，size，writing time等信息&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HadaFS server维护两种元数据数据库，数据结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322200851411.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322200851411&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;一个数据库为Local Metadata Database LMDB，保存第一和第四种元数据类别，文件的local id（LIB）是文件的本地路径。&lt;/p&gt;
&lt;p&gt;另一种为Global Metadata Database，保存第一、二、四种元数据（第三种不需要维护），HadaFS文件的元数据存储在唯一的GMDB，通过文件的全路径hash索引。&lt;/p&gt;
&lt;p&gt;两种数据库基于RocksDB，该数据库不支持多线程写，但这并不是性能瓶颈。&lt;/p&gt;
&lt;p&gt;两种元数据库的Key构造包含：用户的UID、GID、PATH，GID和UID用于前缀字符串检索。&lt;/p&gt;
&lt;p&gt;对于多对多N-N I/O模式，每个client写入到独立的file，并且存储在LMDB的元数据和存储在GMDB的第一、四类元数据匹配。&lt;/p&gt;
&lt;p&gt;对于多对一N-1I/O模式，多个client需要共享一个文件，可能使用不同的HadaFS bridge server，这时，GMDB负责从多个LMDB合并文件的元数据。&lt;/p&gt;
&lt;p&gt;在文件read/write时，LMDB记录文件元数据的改变，维护一个包含本地数据段位置信息的有序链表，并将数据发送到相应的GMDB。GMDB负责维护全局的文件数据段链表，来保证全局的数据共享。&lt;/p&gt;
&lt;h2 id=&#34;hadafs-io-control--data-flow&#34;&gt;HadaFS I/O Control &amp;amp;&amp;amp; Data Flow&lt;/h2&gt;
&lt;p&gt;介绍HadaFS种的控制流和数据流。下图展示了3个HadaFS Client和3个server的关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322204336470.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322204336470&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Client A发起I/O请求，创建文件F1并写入100MB数据。数据将会直接写入Client A的Bridge server：X&lt;/li&gt;
&lt;li&gt;Server X写入F1文件的元数据信息和位置信息到LMDB&lt;/li&gt;
&lt;li&gt;基于F1的文件路径，处理好F1的元数据信息。Server X写入F1文件的元数据信息和位置信息到Server Y上的GMDB。&lt;/li&gt;
&lt;li&gt;Client C发起读取文件F1的I/O请求&lt;/li&gt;
&lt;li&gt;Client C的Bridge Server Z接受I/O请求，从Server Y根据F1的路径获取F1的元数据和位置信息&lt;/li&gt;
&lt;li&gt;Server Z从Server X读取数据，并转发到Client C&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于局部写和全局读是有利的，尤其是对于需要频繁输出检查点的应用。&lt;/p&gt;
&lt;p&gt;对于读敏感的应用可以自行维护client到bridge server的映射表，减少数据转发，通过mount接口实现高性能。&lt;/p&gt;
&lt;p&gt;HadaFS除了限制了每个server连接的client数量，减少了性能抖动，还为存储系统充分支持应用程序的并行性奠定了基础。&lt;/p&gt;
&lt;h2 id=&#34;data-management-tool&#34;&gt;Data Management Tool&lt;/h2&gt;
&lt;p&gt;现有的BB方案，例如LPCC，可能会导致迁移大量的临时数据。Datawarp要求应用程序在其源代码或脚本中指定BB和GFS之间的迁移，这通常是一种静态迁移方法，并要求计算节点参与迁移。&lt;/p&gt;
&lt;p&gt;HadaFS中，用户使用Hadash以目录树的视图来获取和管理文件，视图根据功能分为两类，元数据信息查询和数据迁移。&lt;/p&gt;
&lt;p&gt;元数据信息查询提供如下指令：ls，du，find，grep等。ls和find可以在目录树视图查询文件信息。Hadash从metadata-数据库获取信息，并以Linux shell中常用的命令的形式展示这些信息。&lt;/p&gt;
&lt;p&gt;其他触发数据迁移的指令，例如rm、get、put等，Hadash通过Redis pipeline发送指令到HadaFS server中的数据管理模块，然后每个HadaFS server的数据管理模块使用LMDB处理本地数据请求，并行地执行这些指令。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Redis Pipeline就是一组Redis命令的组装，避免频繁的执行命令的Request/Response，类似事务的概念，还可以使用Lua脚本实现类似的功能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是从HadaFS到GFS进行数据迁移的控制流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323103315198.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323103315198&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户通过UI发起数据管理指令到Hadash server&lt;/li&gt;
&lt;li&gt;Hadash Server接收并转发指令到BB节点的所有Hadash代理&lt;/li&gt;
&lt;li&gt;Hadash代理解析指令，通过LMDB获取文件相应的链表信息，在本地SSD读取文件&lt;/li&gt;
&lt;li&gt;最终，Hadash代理写入这些文件到GFS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当所有的文件写入完成时，Hadash代理通过另一个Redis Pipeline返回success，Hadash将通知用户迁移已经完成。&lt;/p&gt;
&lt;p&gt;如果将要被迁移文件正持续追加写入，Hadash将持续拷贝新写入的数据，类似于Linux默认的数据拷贝机制cp。&lt;/p&gt;
&lt;p&gt;Hadash使用前缀匹配方法来展示目录树，可以直接通过LMDB本地执行，减少了对GMDB的影响。&lt;/p&gt;
&lt;p&gt;Hadash使用分布式管理方法来实现数据的本地化管理。该方式主要的瓶颈在GFS，性能根据BB节点的数量增长。&lt;/p&gt;
&lt;h2 id=&#34;hadafs优化&#34;&gt;HadaFS优化&lt;/h2&gt;
&lt;h3 id=&#34;持久化语义和元数据优化&#34;&gt;持久化语义和元数据优化&lt;/h3&gt;
&lt;p&gt;HadaFS采用宽松的持久化语义，不支持在client和server中缓存数据，而是基于ext4文件系统缓存机制，持久化语义取决于元数据的同步。&lt;/p&gt;
&lt;p&gt;HadaFS为不同的应用场景提出了三种元数据的同步机制，避免出现传统文件系统复杂的设计。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;mode 1:异步地更新所有元数据（eventual consistency），在文件的打开、删除、读写期间，所有的操作都先在bridge server本地执行，然后元数据将被异步从LMDB更新到GMDB。该模式的性能最高，相当于为计算节点提供本地存储，适用于没有数据依赖性的场景&lt;/li&gt;
&lt;li&gt;mode 2:同步地更新部分元数据，异步地更新另一部分元数据（session/commit consistency semantics）。第一类元数据（name，owner）将在文件被创建时同步地更新。第二类元数据（file size，modify time）将在文件读写时被异步地进行更新，或通过flush操作同步地更新。该模式为默认方式，可以平衡读写文件读写的性能&lt;/li&gt;
&lt;li&gt;mode 3:在所有的open、read、write操作时同步元数据（strong consistency semantics，HadaFS不支持覆盖写）。所有的server要先获取文件位置来保证第一类元数据的同步，诸如open、write、read、flush的操作需要同步第二类元数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HadaFS使用全路径索引和bridge server，不需要分布式锁，对于使用N-N I/O的应用，不会产生数据冲突，然而对于N-1写模式，HadaFS需要先将数据写入到每个Bridge Server，因此不支持覆盖写，原子写只在mode 3中支持。&lt;/p&gt;
&lt;h3 id=&#34;优化共享文件&#34;&gt;优化共享文件&lt;/h3&gt;
&lt;p&gt;LTA架构适合N-N I/O模型，对于N-1 I/O模型，HadaFS使用类似ADIOS BP的文件布局。先将数据写入到独立的bridge server，再由GMDB维护文件的元数据（BP group index）。一个共享的文件可以存储在多个服务器中，read/write可以被转换成并发的read/write。&lt;/p&gt;
&lt;p&gt;HadaFS以文件的形式存储数据，因此可能会产生碎片。例如，100000个进程并发地写同一个文件，每个进程写入6次，在完全随机的情况下可能产生600000个文件碎片。HadaFS使用一个根据offset排序的链表，合并相同bridge server相邻的段位置信息。读写时的段插入和获取操作时间复杂度为LogN，N为文件段的数量。&lt;/p&gt;
&lt;p&gt;所有的元数据同步机制支持N-1模式，元数据只存储在GMDB中。&lt;/p&gt;
&lt;h3 id=&#34;避免干扰&#34;&gt;避免干扰&lt;/h3&gt;
&lt;p&gt;超算中不同的任务之间存在资源竞争，造成I/O干扰。HadaFS中主要是由于不同的client共享一个server，可以通过用户自行决定连接的server，改善该问题。&lt;/p&gt;
&lt;p&gt;HadaFS还提供了自动调节工具，通过监控系统来优化I/O，自动指定连接的BB资源，设置环境变量，选择合适的元数据同步机制，达到隔离BB资源的目的。&lt;/p&gt;
&lt;h2 id=&#34;超算中的hadafs&#34;&gt;超算中的HadaFS&lt;/h2&gt;
&lt;p&gt;HadaFS已经在SNS中部署了一年多，下图为HadaFS的部署架构图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323151800866.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323151800866&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每个I/O转发节点有两个HadaFS server，每个HadaFS server使用一个NVMe SSD来支持文件数据的存储和元数据（LMDB、GMDB）。&lt;/p&gt;
&lt;p&gt;容错的代价是很高的，应用会周期性写入检查点来减少数据恢复的开销，如果一个BB节点失效，只要SSD没有损坏，HadaFS就可以恢复，为了减少数据恢复的成本，HadaFS周期性地备份关键数据到GFS，在HadaFS部署的一年多时间里，总共产生了15次BB节点的失效，其中没有SSD的故障。&lt;/p&gt;
&lt;h1 id=&#34;0x03-evaluation&#34;&gt;0x03 Evaluation&lt;/h1&gt;
&lt;h2 id=&#34;数据转发性能&#34;&gt;数据转发性能&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323223311503.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323223311503&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于大气科学研究模型NEMO，HadaFS性能如下，说明了对于需要共享文件的程序，HadaFS可以提升其吞吐量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323223349489.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323223349489&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;元数据访问性能&#34;&gt;元数据访问性能&lt;/h2&gt;
&lt;p&gt;MDTest&lt;/p&gt;
&lt;p&gt;Client-Server比例为256:1&lt;/p&gt;
&lt;p&gt;竞品为BeeGFS，配置与Hada相同，GFS使用132 OSS和4 MDS&lt;/p&gt;
&lt;p&gt;测试了Create/Stat/Remove操作的OPS&lt;/p&gt;
&lt;p&gt;BeeGFS由于集群管理服务的限制不能达到65536进程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323224828257.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323224828257&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据访问性能&#34;&gt;数据访问性能&lt;/h2&gt;
&lt;p&gt;IOR测试I/O带宽&lt;/p&gt;
&lt;p&gt;竞品为BeeGFS和GFS，配置同metadata测试&lt;/p&gt;
&lt;p&gt;BeeGFS和mode1、mode2性能相当，但是不能达到65536进程数&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323225137305.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323225137305&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用VPIC-I/O和BD-CATS-I/O测试共享文件的读写访问性能&lt;/p&gt;
&lt;p&gt;HadaFS和BeeGFS使用16个服务器，可以看出低进程情况下二者性能相当，但是BeeGFS的伸缩性较差，HadaFS通过LTA来避免I/O冲突。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323225545079.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323225545079&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据迁移性能&#34;&gt;数据迁移性能&lt;/h2&gt;
&lt;p&gt;HadaFS使用256台服务器&lt;/p&gt;
&lt;p&gt;Datawarp使用4096进程&lt;/p&gt;
&lt;p&gt;随着文件大小和数量的增加，HadaFS的性能逐渐优于Datawarp&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323230150281.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323230150281&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;io干扰&#34;&gt;I/O干扰&lt;/h2&gt;
&lt;p&gt;使用五种程序进行测试：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DNDC：农业生态系统的生物地球化学程序&lt;/li&gt;
&lt;li&gt;CAM：气候模拟、全球大气模型&lt;/li&gt;
&lt;li&gt;Shentu：高伸缩性图引擎&lt;/li&gt;
&lt;li&gt;WRF：天气预测系统&lt;/li&gt;
&lt;li&gt;APT：动态粒子仿真程序&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图深色色块代表程序速度降低的指数，每一行代表左侧的应用受到下放程序的影响&lt;/p&gt;
&lt;p&gt;由于可以灵活的选择与client连接的server，可以避免一定的I/O干扰&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323230453622.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323230453622&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x04-conclusion&#34;&gt;0x04 Conclusion&lt;/h1&gt;
&lt;p&gt;HadaFS已经用于新一代的神威超算中，基于Shared Burst Buffer，使应用既可以单点访问BB，也可以通过HadaFS实现对文件同时进行读写，并实现了高性能。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>[FAST&#39;23] More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba</title>
        <link>https://blog.ipandai.club/p/fast23-more-than-capacity-performance-oriented-evolution-of-pangu-in-alibaba/</link>
        <pubDate>Thu, 02 Mar 2023 15:11:40 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/fast23-more-than-capacity-performance-oriented-evolution-of-pangu-in-alibaba/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast23/presentation/li-qiang-deployed&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[FAST&#39;23] More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FAST&#39;23 会议论文翻译，《不仅仅是容量:盘古面向性能的演变》&lt;/p&gt;
&lt;p&gt;本论文讲述了Pangu存储系统是如何随着硬件及商业需求，去演变提供更高的性能的，存储服务的I/O延迟达到了100-us。盘古的演变主要有两个部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Phase 1: 盘古通过优化文件系统并设计了用户端的存储操作系统，积极引入高速SSD和Remote Direct Memory Access(RDMA)网络技术。因此，盘古在有效降低了I/O延迟的同时，还提高了吞吐量和IOPS。&lt;/li&gt;
&lt;li&gt;Phase 2: 盘古从面向卷的存储供应商转变为面向性能。为了适应这一商业模式的改变，盘古使用足够多的SSD和25Gbps-100Gbps的RDMA带宽更新了基础设施。这引入了一些列的关键设计，包括减少流量放大，远程直接缓存访问，和CPU计算卸载，来保证盘古完全获得基于硬件升级所带来的性能提升。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;除了技术上的创新，作者还分享了盘古发展过程中的运营经验，并讨论了其中的重要教训。&lt;/p&gt;
&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;盘古的开发始于2009年，目前已经是阿里巴巴集团和阿里云统一存储平台。盘古为阿里的核心业务提供了可伸缩性、高性能和可靠性。&lt;/p&gt;
&lt;p&gt;Elastic Block Storage(EBS), Object Storage Service(OSS), Network-Attached Storage(NAS), PolarDB, MaxCompute这些云服务基于盘古建立。经过十几年的发展，盘古已经成为了一个拥有ExaBytes并管理万亿文件的全局存储系统。&lt;/p&gt;
&lt;h2 id=&#34;盘古-10-提供存储容量&#34;&gt;盘古 1.0: 提供存储容量&lt;/h2&gt;
&lt;p&gt;Pangu 1.0设计于2009-2015年，通过高性能的CPU和HDD组成，可以提供ms毫秒级别的I/O延迟和Gbps级别的&lt;strong&gt;数据中心带宽&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Pangu 1.0基于Linux Ext4设计了一个分布式的内核文件系统和内核TCP，并给予不同种类的存储服务提供多种文件类型支持（Tempfile，LogFile，Random Access file）。&lt;/p&gt;
&lt;p&gt;此时正处于云计算的初始阶段，性能受限于HDD性能和网络带宽，相较于更快的访问速度，用户更关注存储容量。&lt;/p&gt;
&lt;h2 id=&#34;新的硬件新的设计&#34;&gt;新的硬件，新的设计&lt;/h2&gt;
&lt;p&gt;自2015年起，为了引入新兴的SSD和RDMA技术，盘古2.0开始设计和开发。盘古2.0的目标是提供100us级别I/O延迟的高性能的存储服务。尽管SSD和RDMA在存储和网络中实现低延迟、高性能的I/O，团队发现：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;盘古1.0中使用的多种文件类型，特别是允许随机访问的文件类型，在固态硬盘上的表现很差，而固态硬盘在顺序操作上可以实现高吞吐量和IOPS。&lt;/li&gt;
&lt;li&gt;由于数据复制和频繁的中断，内核空间的软件栈无法跟上SSD和RDMA的高IOPS和低I/O延迟。&lt;/li&gt;
&lt;li&gt;从以服务器为中心的数据中心架构向资源分散的数据中心架构的范式转变，对实现低I/O延迟提出了额外的挑战。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;盘古20-phase-1-通过重构文件系统与用户空间的存储操作系统来拥抱ssd和rdma&#34;&gt;盘古2.0 Phase 1: 通过重构文件系统与用户空间的存储操作系统来拥抱SSD和RDMA&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;为了实现高性能和低I/O延迟，盘古2.0首先在其文件系统中的关键组件提出了新的设计。为了简化整个系统的开发和管理，盘古设计了一个统一、追加写入的持久化层。它还引入了一个独立的分块布局，以减少文件写入操作的I/O延迟。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;盘古2.0设计了一个用户空间的存储操作系统（USSOS），USSOSS使用一个RTC(Run to completion)线程模型来实现用户空间存储栈和用户空间网络栈的高效协作。它还为高效的CPU和内存资源分配提出了一个用户空间的调度机制。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;盘古2.0部署了在动态环境下提供SLA保证的机制。通过这些创新，盘古2.0实现了毫秒级别的P999 I/O延迟。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;盘古20-phase-2-通过基础设施更新和突破网络内存cpu瓶颈适应以性能为导向的业务模式&#34;&gt;盘古2.0 Phase 2: 通过基础设施更新和突破网络/内存/CPU瓶颈，适应以性能为导向的业务模式&lt;/h2&gt;
&lt;p&gt;2018年起，盘古逐渐从容量导向的商业模式转变为性能导向。这是因为越来越多的企业用户将他们的业务转移到了阿里云并且他们对存储性能的延迟和性能有很严格的要求。这在COVID-19疫情爆发之后变得越来越快，为了适应这一商业模式转变和日益增长的用户，盘古2.0需要继续升级基础设施。&lt;/p&gt;
&lt;p&gt;用原有的服务器和交换机沿着基于CLOS架构的拓扑结构来对基础设施进行扩容是不经济的，包括高昂的总成本（机架空间、电力、散热、人力成本）和更高的碳排放/环境问题。因此，盘古开发来室内高容量存储服务器（每个服务器96TB SSD）并且升级到了25Gbps-100Gbps的网络带宽。&lt;/p&gt;
&lt;p&gt;为了完全获得这些升级带来的性能提升，盘古2.0提出了一系列的技术来处理在{网络/内存/CPU}的性能瓶颈并充分利用其强大的硬件资源。具体来说，盘古2.0通过减少网络流量放大率和动态调整不同流量的优先级来优化网络带宽；通过提出Remote Direct Cache Access(RDCA)来处理内存瓶颈；通过消除数据序列化/反序列化的开销并引入CPU等待指令来同步超线程，以此来解决CPU瓶颈问题。&lt;/p&gt;
&lt;h2 id=&#34;生产中的高性能&#34;&gt;生产中的高性能&lt;/h2&gt;
&lt;p&gt;盘古2.0成功支持了elastic SSD block存储服务，并可达到100us级别的I/O延迟和1M的IOPS。在2018年双十一活动，盘古2.0加持下的阿里数据库实现了280us的延迟。&lt;/p&gt;
&lt;p&gt;对于OTS存储服务，同样的硬件条件下。盘古2.0的I/O延迟比盘古1.0降低了一个数量级。&lt;/p&gt;
&lt;p&gt;对于写敏感的服务（EBS云盘），P999 I/O延迟低于1ms。&lt;/p&gt;
&lt;p&gt;对于读敏感的服务（在线搜索），P999 I/O延迟低于11ms。&lt;/p&gt;
&lt;p&gt;在第二阶段，通过将2x25Gbps带宽升级到2x100Gbps，并解决了网络、内存、CPU瓶颈，每台泰山存储服务器的有效吞吐量增加了6.1倍。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elastic SSD Block/EBS云盘：是为阿里云为云服务器ECS提供的低时延、持久性、高可靠的块级随机存储。块存储支持在可用区内自动复制用户的数据，防止意外硬件故障导致的数据不可用，保护业务免于硬件故障的威胁。&lt;/p&gt;
&lt;p&gt;OTS：Open Table Service，已更名Table Store，是构建在阿里云飞天分布式系统之上的NoSQL数据库服务，提供海量结构化数据的存储和实时访问。Table Store以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;0x01-bg&#34;&gt;0x01 Bg&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;盘古是大规模分布式存储系统，由：盘古核心，盘古服务层，盘古监控系统组成（Figure 1）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230303140052844.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230303140052844&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;盘古core&#34;&gt;盘古Core&lt;/h3&gt;
&lt;p&gt;盘古核心由：clients，masters，chunk severs组成，提供追加写入的存储语义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Client&lt;/strong&gt;提供访问盘古云存储服务（EBS，OSS）的SDK，并负责接收从服务端发送的文件请求，与masters和chunk servers通信来实现这些请求。类似于其他分布式文件系统（Tectonic，Colossus），盘古中的Clients负责较重的工作并在盘古的复制管理、SLA保障、数据一致性管理中扮演关键角色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Master&lt;/strong&gt;管理盘古中的所有元数据并使用基于Raft的协议来维护元数据的分布式一致性。为了更好的水平扩展性和延伸性（大量的文件数），盘古master分解元数据服务为两个部分：namespace服务和stream meta服务，stream是一组chunk的抽象。这两个服务首先根据目录树分隔元数据来实现局部原数据，然后通过哈希将这些stream进一步分隔以达到负载均衡。namespace服务提供文件的信息（目录树和命名空间），stream元数据服务提供从文件到chunk的映射（chunk的位置）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ChunkServers&lt;/strong&gt;以chunk存储数据并配备有自定义的用户空间存储文件系统（USSFS），USSFS为不同硬件（SMRSTORE for HM-SMR drives）提供高性能，追加写入的存储引擎。在盘古2.0的第一阶段，每个存储在chunkservers的文件都有三个冗余，由GCWorker进行垃圾回收，并使用EC（Erasure Coding）编码来存储文件。在盘古2.0的第二阶段，在商业模式中，使用EC替换3个冗余的存储方式来减少流量放大。&lt;/p&gt;
&lt;h3 id=&#34;盘古service&#34;&gt;盘古Service&lt;/h3&gt;
&lt;p&gt;盘古服务层提供传统的云存储服务（EBS、OSS、NAS），通过面相云原生的文件系统（Fisc）提供云原生存储服务。&lt;/p&gt;
&lt;h3 id=&#34;盘古monitoring-system&#34;&gt;盘古Monitoring System&lt;/h3&gt;
&lt;p&gt;Perseus为盘古核心和盘古服务提供实时监控和人工智能辅助的根本原因分析服务。盘古Core、盘古Service、盘古Monitoring System通过高速网络相连。&lt;/p&gt;
&lt;h2 id=&#34;盘古20的设计目标&#34;&gt;盘古2.0的设计目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;低延迟：盘古2.0要利用SSD和RDMA低延迟的特性，在计算-存储分离架构中实现平均100us级别I/O延迟的性能目标，即使在网络流量抖动和服务器故障等动态环境下，也能提供毫秒级别 P999 SLA。&lt;/li&gt;
&lt;li&gt;高吞吐量：使存储服务器的有效吞吐量接近其容量。&lt;/li&gt;
&lt;li&gt;为所有服务提供统一的高性能支持：为运行在其上的所有业务提供统一的高性能支持，例如在线搜索、数据流分析、EBS、OSS和数据库。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;目前有很多分布式存储系统被提出和使用&lt;/p&gt;
&lt;p&gt;开源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://hadoop.apache.org/docs/r1.2.1/hdfs_design.htmll&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;HDFS（Hadoop FS）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ceph.com/en/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ceph（Redhat）[OSDI&#39;06] Ceph: A Scalable, High-Performance Distributed File System&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;私有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://pdos.csail.mit.edu/6.824/papers/gfs.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GFS（Google）[SOSP&#39;03] The Google File System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast21/presentation/pan&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Tectonic (FaceBook)[FAST&#39;21] Facebook’s Tectonic Filesystem: Efficiency from Exascale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://aws.amazon.com/products/storage/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;AWS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;阿里巴巴的盘古团队分享过很多盘古的设计理念，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDMA大型部署,&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/system/files/nsdi21-gao.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[NSDI&#39;21] When Cloud Storage Meets RDMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;横向扩展云存储服务的Key-Value键值存储引擎&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/abs/10.1145/3448016.3457553&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[SIGMOD&#39;21] A Key-Value Engine for Scalable Cloud Storage Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EBS存储服务的网络和存储软件栈协同设计&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/abs/10.1145/3544216.3544238&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[SIGCOMM&#39;22] From Luna to Solar: the Evolu- tions of the Compute-to-Storage Networks in Alibaba Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;namespace元数据服务的关键设计&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast22/presentation/lv&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[FAST&#39;22] InfiniFS: An Efficient Metadata Service for Large-Scale Distributed Filesystems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x02-phase-one-embracing-ssd-and-rdma&#34;&gt;0x02 Phase One: Embracing SSD and RDMA&lt;/h1&gt;
&lt;p&gt;相较于HDD和TCP，SSD和RDMA技术显著降低了I/O延迟和网络问题。&lt;/p&gt;
&lt;p&gt;盘古通过开发用户空间存储操作系统并提出一些文件系统的设计来实现高吞吐量、100us级别I/O延迟的高IOPS性能。同时提供了新的机制来保障SLA。&lt;/p&gt;
&lt;h2 id=&#34;append-only-file-system&#34;&gt;Append-Only File System&lt;/h2&gt;
&lt;p&gt;盘古提出了统一、追加写入的持久层，通过名为FlatLogFile的追加写入接口来简化架构。FlatLogFile具有高吞吐量和低延迟。基于FlatLogFile，盘古采用追加写入的chunk，并使用独立chunk布局来管理chunkserver中的chunk&lt;/p&gt;
&lt;h3 id=&#34;unified-append-only-persistence-layer&#34;&gt;Unified, Append-Only Persistence Layer&lt;/h3&gt;
&lt;p&gt;盘古的持久化层为存储服务提供接口。在早期开发阶段，不同的存储服务会使用不同的接口，例如LogFile接口服务于低延迟的NAS服务，TempFile接口服务于高吞吐量的大型计算数据分析服务。然而，这种设计使开发和管理非常复杂。每个接口都需要有人开发和维护，人力成本高切容易出错。&lt;/p&gt;
&lt;p&gt;因此，需要简化开发管理过程，还要引入低延迟的SSD，受到计算机网络分层架构的启发，盘古提出了统一的文件类型：FlatLogFile（Figure 2）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230303201714075.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230303201714075&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;FlatLogFile仅支持追加写入，上层服务（OSS）可以可以使用类似键值的映射来更新数据，并使用垃圾收集机制来压缩历史数据。FlatLogFile为存储服务提供简单、统一的接口操作数据。盘古的开发者必须保证数据操作都是通过FlatLogFile，尤其是写入操作，可以高效并可靠的在存储介质上执行。因此，存储服务的任何升级和改变对于开发者而言都是透明的，简化了开发和管理。&lt;/p&gt;
&lt;p&gt;在底层，团队观察到SSD由于其自身的存储单元和闪存事务层的特性，可以在顺序操作上获得较高的吞吐量和IOPS。为了保证通过FlatLogFile进行的数据操作能够在SSD上高效地执行，我们将FlatLogFile上的顺序操作对齐以实现高性能。&lt;/p&gt;
&lt;h3 id=&#34;heavy-weight-client&#34;&gt;Heavy weight Client&lt;/h3&gt;
&lt;p&gt;脏活累活都是Client来干。Client负责与chunkservers一起进行数据操作，与master一起进行元数据信息的检索和更新。在从masters获取chunk信息后，一个盘古Client将负责相应的复制协议和EC协议。Client有重试机制（备份读取¥3.3）来处理意外的性能抖动（丢包）来保障I/O SLA。&lt;/p&gt;
&lt;p&gt;Client还有探测机制，定期从masters获取最新的 chunkserver 状态，并评估 chunkserver 的服务质量。类似于Facebook Tectonic FS的client，盘古的Client可以选择合适的读写参数来处理具体的存储服务指令（EBS/OSS）。&lt;/p&gt;
&lt;h3 id=&#34;append-only-chunk-management&#34;&gt;Append-Only Chunk Management&lt;/h3&gt;
&lt;p&gt;传统的文件系统在写入文件时，同时分离写入文件的元数据，产生两次SSD的写操作。&lt;/p&gt;
&lt;p&gt;为了降低延迟和延长SSD寿命，盘古基于FlatLogFile的append-only语义，选择以chunk为单位存储文件，而非block，chunk存储在chunksever中，并有独立的布局，每个chunk都保存了自己的数据和元数据信息。&lt;/p&gt;
&lt;p&gt;chunk只需要一次操作就可以被写入到存储介质中，可以有效减少写入延迟和存储介质的寿命。&lt;/p&gt;
&lt;p&gt;Figure 3是chunk的布局。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230304141646186.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230304141646186&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;一个chunk包含多个部分，每个部分包含三个元素：data、padding、footer。&lt;/p&gt;
&lt;p&gt;footer保存了chunk的元数据，例如chunk ID、chunk 长度、CRC校验和。&lt;/p&gt;
&lt;p&gt;独立的chunk布局也使得chunkserver可以执行进行纠错恢复。例如，当一个client连续写入chunk到存储设备时，chunkserver在内存中存储这些chunk的元数据，并且周期性地将这些信息的检查点传递给存储设备。当发生错误导致一些不能完成的写入操作时，chunkserver会加载检查点中的元数据并且和chunk中的元数据进行比较，如果二者不同，chunkserver会检查Chunk的CRC来进行恢复。&lt;/p&gt;
&lt;h3 id=&#34;metadata-operation-optimization-元数据操作优化&#34;&gt;Metadata Operation Optimization 元数据操作优化&lt;/h3&gt;
&lt;p&gt;盘古的Master提供两种元数据服务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;namespace：负责目录树和文件管理&lt;/li&gt;
&lt;li&gt;Stream：负责chunk信息的维护&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stream内包含一组chunk，同一个Stream中的chunk保存在一个文件中。&lt;/p&gt;
&lt;p&gt;这两个服务都是用了分布式架构来保证更好的伸缩性。&lt;/p&gt;
&lt;p&gt;他们根据元数据局部性和负载均衡对元数据进行划分（先根据目录树划分，再进行哈希）。&lt;/p&gt;
&lt;p&gt;同时还用多种机制来优化元数据操作的高效性。&lt;/p&gt;
&lt;h4 id=&#34;并行的元数据处理&#34;&gt;并行的元数据处理&lt;/h4&gt;
&lt;p&gt;namespace和stream都使用并行处理（InfiniFS）来实现元数据的低延迟访问。&lt;/p&gt;
&lt;p&gt;盘古使用哈希算法来映射关联性强的元数据到不同的元数据服务器中。使用一种新的数据结构，支持可预测的目录ID，并允许Client高效地平行执行路径解析。同时还引入了几种加速Client从Stream服务中检索chunk信息的技术。&lt;/p&gt;
&lt;h4 id=&#34;可变长度的chunk&#34;&gt;可变长度的chunk&lt;/h4&gt;
&lt;p&gt;盘古2.0采用大chunk，有三个好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少了元数据的数量&lt;/li&gt;
&lt;li&gt;避免了Client频繁请求chunk产生的I/O延迟&lt;/li&gt;
&lt;li&gt;提高了SSD的寿命&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果仅仅提高chunk size会有碎片的风险，因此采用了可变长度的chunk（从1MB到2GB）。&lt;/p&gt;
&lt;p&gt;例如EBS服务的chunk size的95%分位数为64MB，99%分位数为286.4MB。&lt;/p&gt;
&lt;h4 id=&#34;在client中缓存chunk信息&#34;&gt;在Client中缓存chunk信息&lt;/h4&gt;
&lt;p&gt;每个client维护了一个本地的元数据缓存池，来减少元数据的请求次数。缓存池通过LRU进行维护。&lt;/p&gt;
&lt;p&gt;当一个程序想访问数据时，client首先访问元数据缓存，当缓存没有命中时将发起对master的请求，当缓存命中时，响应请求时，对应的chunkserver会通知client其元数据已经过期（由于副本迁移）。&lt;/p&gt;
&lt;h4 id=&#34;批处理chunk信息请求&#34;&gt;批处理chunk信息请求&lt;/h4&gt;
&lt;p&gt;每个client在短时间内汇总多个chunk请求，并将其批量发送给master，来提高查询效率。主站并行处理成批的请求，汇总结果并将其发回给client。client对结果进行分解，并将其分配给相应的应用程序。&lt;/p&gt;
&lt;h4 id=&#34;推测chunk信息进行预取&#34;&gt;推测chunk信息进行预取&lt;/h4&gt;
&lt;p&gt;设计了一个基于贪心和统计学的预取机制来减少chunk信息的请求。&lt;/p&gt;
&lt;p&gt;当master节点收到了来自client的读请求时，master将返回有关的chunk元数据和其他chunk的元数据。&lt;/p&gt;
&lt;p&gt;当master收到写入请求时，master将返回多个chunk，超出client的请求数量。&lt;/p&gt;
&lt;p&gt;client因此可以在不请求块的情况下切换块。&lt;/p&gt;
&lt;h4 id=&#34;数据捎带减少往返时延&#34;&gt;数据捎带减少往返时延&lt;/h4&gt;
&lt;p&gt;受到了QUIC和HTTP3的启发，使用数据捎带来改善写入延迟，在client从master检索到chunk地址后，他将chunk创建请求和数据写入请求合并为一个请求，然后发送给chunkserver。&lt;/p&gt;
&lt;p&gt;因此可以减少一个RTT。（但是当个RTT长了，数据包大小问题？）&lt;/p&gt;
&lt;h2 id=&#34;chunkserver-ussos&#34;&gt;ChunkServer USSOS&lt;/h2&gt;
&lt;p&gt;chunkserver负责执行所有的数据操作。因此，精心设计运行时操作系统以确保数据操作能以&lt;strong&gt;低延迟&lt;/strong&gt;和&lt;strong&gt;高吞吐量&lt;/strong&gt;完成是非常重要的。在新兴的高速网络技术和存储领域，坚持通过内核空间进行数据操作的传统设计是低效的。这不仅会导致频繁的系统中断，从而消耗CPU资源，而且还会导致用户空间和内核空间之间不必要的数据重复。
为了解决这些问题，盘古采用了kernel-bypass绕过内核的设计，为chunkserver开发了一个高性能的用户空间存储操作系统，它提供了一个统一的用户空间存储软件平台。除了在USSOS中实现设备管理和RTC运行到完成的线程模型，盘古还实现了用户级的内存管理，轻量级的用户空间调度策略，还为SSD定制了高性能append-only的用户空间存储文件系统（USSFS）。&lt;/p&gt;
&lt;h3 id=&#34;用户级别内存管理&#34;&gt;用户级别内存管理&lt;/h3&gt;
&lt;p&gt;chunkserver的USSOS基于现有的用户空间技术，如网络栈中的RDMA，存储栈中的DPDK和SPDK。盘古对网络栈和存储栈进行了整合，以减少延迟并实现高性能的数据操作。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用RTC线程模型，在传统的流水线pipeline线程模型中，一个请求被分解成多个阶段，每个阶段运行在一个线程中。相反，USSOS中一个请求自始至终都处于一个线程里，减少了上下文切换和线程间通信的开销。&lt;/li&gt;
&lt;li&gt;线程请求一个大页内存空间用作网络栈和存储栈的共享内存。从网络中接受的数据可以通过RDMA存储在大页内存空间中。发送大页内存的元数据之后（地址和大小），数据可以直接通过SPDK从大页内存写入到存储设备。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;用户空间的调度机制&#34;&gt;用户空间的调度机制&lt;/h3&gt;
&lt;h4 id=&#34;通过阻塞后续请求阻止任务&#34;&gt;通过阻塞后续请求阻止任务&lt;/h4&gt;
&lt;p&gt;每个chunkserver都有固定数量的线程。&lt;/p&gt;
&lt;p&gt;一个新的请求通过哈希映射被下发给一个working线程。被发送到同一个线程的请求基于先到先执行FIFS的机制执行。&lt;/p&gt;
&lt;p&gt;如果一个请求占用了太多的时间片（表查询搜索，内存申请），将会阻塞其他任务。&lt;/p&gt;
&lt;p&gt;针对不同场景使用不同的调度策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于负载较高的任务，使用心跳机制来监控任务的执行时间，并设置告警。如果任务超出了规定时间片，会将其下发到后台线程进行执行。&lt;/li&gt;
&lt;li&gt;对于系统产生的负载，盘古使用TCMalloc（Thread Cached Malloc）缓存来允许在缓存中执行高频率的操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;优先级调度保障高qos&#34;&gt;优先级调度保障高QoS&lt;/h4&gt;
&lt;p&gt;盘古对不同请求标注不同的QoS标签（用户请求高优先级，GC请求低优先级）。&lt;/p&gt;
&lt;p&gt;USSOS创建了一个&lt;strong&gt;优先队列&lt;/strong&gt;，队列中的任务根据QoS目标来进行调度。以此来防止低QoS任务过多导致高QoS不能被按时完成。&lt;/p&gt;
&lt;h4 id=&#34;轮询和事件驱动的切换napi&#34;&gt;轮询和事件驱动的切换（NAPI）&lt;/h4&gt;
&lt;p&gt;为了防止频繁发起中断导致高CPU利用率，USSOS使用切换机制。&lt;/p&gt;
&lt;p&gt;NIC（Network Interface Controller，网卡）提供了&lt;strong&gt;文件描述符fd&lt;/strong&gt;的监控，基于监听数据达到后的fd事件。&lt;/p&gt;
&lt;p&gt;程序默认使用&lt;strong&gt;事件驱动&lt;/strong&gt;，当程序收到了NIC的通知，将切换为轮询模式。如果程序一段时间内没有收到任何I/O请求，将会切换会事件驱动模式并告知NIC。&lt;/p&gt;
&lt;h3 id=&#34;append-only-ussos&#34;&gt;Append Only USSOS&lt;/h3&gt;
&lt;p&gt;USSFS使用一系列基于chunk的指令（open，close，seal，format等），支持append-only write。&lt;/p&gt;
&lt;p&gt;支持append-only的顺序写入，充分利用了SSD的顺序写和随机读特性。&lt;/p&gt;
&lt;p&gt;通过多种机制最大化SSD性能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;充分利用独立chunk布局，极大的减少了数据操作的次数，而不需要使用page cache和日志等机制。&lt;/li&gt;
&lt;li&gt;不需要建立分级的索引机制（例如ext4的inode和文件目录dentries）。所有的操作会通过日志进行记录，该日志可以用来在挂载文件系统时重建元数据信息。&lt;/li&gt;
&lt;li&gt;使用轮询机制减少中断。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;单个SSD的容量一般很大，并且一个chunk的大小通常是64MB，设定USSFS的最小空间分配粒度为1MB，主要考虑使用的内存大小和SSD的空间利用率。&lt;/p&gt;
&lt;h2 id=&#34;高性能sla保障&#34;&gt;高性能SLA保障&lt;/h2&gt;
&lt;h3 id=&#34;chasing&#34;&gt;Chasing&lt;/h3&gt;
&lt;p&gt;减少系统抖动在写延迟上的影响。&lt;/p&gt;
&lt;p&gt;当$MaxCopy$个副本中的$MinCopy$个在chunkserver中成功写入时，允许client向应用返回success。&lt;/p&gt;
&lt;p&gt;规定 $2\times MinCopy\textgreater\ MaxCopy$。&lt;/p&gt;
&lt;p&gt;下图Figure 4展示了Chasing在 $MaxCopy=3\and MinCopy=2$情况下的工作情况：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230308133528640.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230308133528640&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;应用程序请求Client写入数据$[1,2,3]$到3个副本chunkserver中，在时间$T$时，chunkserver1和2已经向client返回了写操作success，但是chunkserver3还没有回复。此时Client将向应用程序回复success。&lt;/p&gt;
&lt;p&gt;在Client中还需要在内存中保留写入的数据，并等待一个时间段$t$（ms级别），如果chunkserver 3在$T+t$时间以前回复了success，Client就会在内存中删除释放要写入的数据。&lt;/p&gt;
&lt;p&gt;如果chunkserver 3没有完成写入操作，且没有完成的部分小于阈值$k$，Client将在chunkserver 3中发起重试操作；若没有完成的部分大于$k$，Client将会在chunkserver 3中封存这个chunk，保证不会收到后续追加写入的影响，Client通知master，master将从chunkserver 1或者2复制一个不同的chunk来保证总共存在3个副本，&lt;strong&gt;Client的内存中还有一个副本？&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;那么chunkserver 3中封存过的chunk怎么处理？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这种Early- Write- Acknowledgment机制，减少了写入操作的尾延迟，在盘古上部署了十多年，没发生过数据丢失。&lt;/p&gt;
&lt;h3 id=&#34;non-stop-write-不停写&#34;&gt;Non-stop write 不停写&lt;/h3&gt;
&lt;p&gt;减少当一个chunk写入失败导致的写入延迟。&lt;/p&gt;
&lt;p&gt;当写入失败时，Client🐝封存chunk并且汇报master当前已经成功写入的数据长度，master使用一个新的chunk来继续写入数据。如果写入到封存的chunk中的数据损坏，会在后台使用其他已经成功写入的副本复制到一个新的chunk中。如果没有可用副本，Client将再次写入数据到一个新的chunk。&lt;/p&gt;
&lt;h3 id=&#34;backup-read&#34;&gt;Backup read&lt;/h3&gt;
&lt;p&gt;减少动态环境下读延迟。在收到读请求的response前，Client向其他Chunkserver发送额外的读请求作为备份。&lt;/p&gt;
&lt;p&gt;该机制涉及两个参数，发送请求的数量和等待时间。&lt;/p&gt;
&lt;p&gt;为此，盘古计算了不同硬盘类型和I/O大小的延迟，并使用这些信息动态调整发送备份读取请求的时间。它还限制了备份读取请求的数量，以控制系统的负载。&lt;/p&gt;
&lt;h3 id=&#34;blacklisting&#34;&gt;Blacklisting&lt;/h3&gt;
&lt;p&gt;为了避免向服务质量差的chunkserver发送I/O请求，盘古使用两个黑名单，&lt;strong&gt;确定黑名单&lt;/strong&gt;和&lt;strong&gt;非确定黑名单&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当盘古确定一个chunkserver已经无法提供服务（chunkserver的SSD损坏），这个服务器将被加入确定黑名单。&lt;/p&gt;
&lt;p&gt;如果一个chunkserver可以提供服务，但是延迟超过上限，将会被加入非确定黑名单，并设置其服务延迟增加的可能性。&lt;/p&gt;
&lt;p&gt;如果chunkserver的延迟超过了所有server延迟的中位数，会将它直接加入非确定性黑名单中，可能性为1.&lt;/p&gt;
&lt;p&gt;Client阶段性的向chunkserver发送I/O探测，以便维护非确定性黑名单。&lt;/p&gt;
&lt;p&gt;如果在确定性黑名单中的服务器成功返回了response，会将其在黑名单中移除。&lt;/p&gt;
&lt;p&gt;对于在非确定性黑名单中的服务器，将会基于response的响应时间来判断。&lt;/p&gt;
&lt;p&gt;盘古同时限制黑名单中的服务数量来保证系统的可用性。对于每个服务器而言，这引入了宽限期来维护黑名单保证系统的稳定性。&lt;/p&gt;
&lt;p&gt;盘古为RDMA和TCP两种链接分别维护了黑名单。&lt;/p&gt;
&lt;h2 id=&#34;评估&#34;&gt;评估&lt;/h2&gt;
&lt;p&gt;Figure 5为2018年双十一期间的峰值延迟，在需要进行大量事物处理的情况下，盘古2.0延迟低于280us。&lt;/p&gt;
&lt;p&gt;Figure 6为OTS服务的延迟，在相同的请求负载下，盘古2.0的查询延迟接近降低了一个数量级。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230309135723278.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230309135723278&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Figure 7、8分别为EBS、在线搜索服务的平均的尾延迟。&lt;/p&gt;
&lt;p&gt;EBS服务写入较多，P999尾延迟低于1ms。&lt;/p&gt;
&lt;p&gt;在线搜索服务读取较多，P999尾延迟低于5ms。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230309135656800.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230309135656800&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x03-phase-two-adapting-to-performance-oriented-business-model-适配面相性能的商业模型&#34;&gt;0x03 Phase Two: Adapting to Performance-Oriented Business Model 适配面相性能的商业模型&lt;/h1&gt;
&lt;h2 id=&#34;网络瓶颈&#34;&gt;网络瓶颈&lt;/h2&gt;
&lt;h3 id=&#34;扩大带宽&#34;&gt;扩大带宽&lt;/h3&gt;
&lt;p&gt;为了匹配SSD的吞吐量，将网络带宽从25Gbps升级到100Gbps。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;硬件优化&lt;/strong&gt;：采用高性能的NIC/RNCI网卡、光模块（QSFP28 DAC，QSFP28 AOC，QSFP28）、单模/多模光纤、高性能交换机。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;网络软件层&lt;/strong&gt;：采用有损RDMA，不使用暂停帧。&lt;/p&gt;
&lt;p&gt;如果使用无损RDMA，当RDMA网络中暂停帧过多时，要在一段时间内关闭NIC端口或临时切换RDMA为TCP，这种基于流量的控制不能解决由暂停帧引发的其他问题（死锁、头阻塞）。因此盘古使用有损RDMA。&lt;/p&gt;
&lt;h3 id=&#34;流量优化&#34;&gt;流量优化&lt;/h3&gt;
&lt;p&gt;减少流量放大率。&lt;/p&gt;
&lt;p&gt;流量放大率为系统中所有的流量传输量除以真实的文件大小，如Figure 9：&lt;/p&gt;
&lt;p&gt;以EBS服务为例&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;EBS Client发送文件（1x）到盘古Client&lt;/li&gt;
&lt;li&gt;盘古Client将文件传输到3个存储节点，并写入三个副本（3x）&lt;/li&gt;
&lt;li&gt;GC Worker读取文件（1x）并进行GC&lt;/li&gt;
&lt;li&gt;最终文件以EC(8,3)(1.375x)的形式写回存储节点，保证至少与3个副本同等级别的容错，并节省了空间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230309151419443.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230309151419443&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;过程中的流量放大率为6.375x(1x+3x+1x+1.375x)。&lt;/p&gt;
&lt;p&gt;因此提出了三个优化：&lt;/p&gt;
&lt;h4 id=&#34;使用ec替换3个副本&#34;&gt;使用EC替换3个副本&lt;/h4&gt;
&lt;p&gt;在保证良好容错性的情况下，有效降低流量。使用b1中的EC(4,2)，可以将流量降低为4.875x。该方法存在两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于小文件的EC存储开销较大，需要填充大量的0达到规定长度。使用多种机制减少空间浪费。可以对小的写请求进行聚合，并动态切换存储策略（EC、3个副本）&lt;/li&gt;
&lt;li&gt;计算EC引入了额外延迟，盘古采用&lt;a class=&#34;link&#34; href=&#34;https://github.com/intel/isa-l&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Intel ISA-L&lt;/a&gt;，将EC计算的延迟相较于Jerasure降低了2.5-3倍&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;压缩flatlogfile&#34;&gt;压缩FlatLogFile&lt;/h4&gt;
&lt;p&gt;FlatLogFile存在高度冗余。盘古Client和GC Worker在进行下一步写入时压缩FlatLogFile，来减少流量。&lt;/p&gt;
&lt;p&gt;盘古使用LZ4算法来实现高效的压缩/解压缩。&lt;/p&gt;
&lt;h4 id=&#34;前后台流量的动态带宽调节&#34;&gt;前后台流量的动态带宽调节&lt;/h4&gt;
&lt;p&gt;动态调整后台流量（GC等）的网络带宽。&lt;/p&gt;
&lt;p&gt;当存储集群中流量空闲时，减少后台流量的限额，使前台应用使用更多的带宽。&lt;/p&gt;
&lt;h2 id=&#34;内存瓶颈&#34;&gt;内存瓶颈&lt;/h2&gt;
&lt;p&gt;内存瓶颈来自内存带宽和网卡（NIC 执行DMA）的竞争，以及接受端的应用处理（数据拷贝，复制，gc）。&lt;/p&gt;
&lt;p&gt;NIC无法获得足够的内存带宽，使得PCIe的带宽超出NIC的处理能力，NIC buffer中都是在途数据包，导致丢包。&lt;/p&gt;
&lt;p&gt;因拥塞控制导致的性能损失会降低30%的网络吞吐量，10%的延迟。&lt;/p&gt;
&lt;h3 id=&#34;加内存&#34;&gt;加内存&lt;/h3&gt;
&lt;p&gt;增加了很多小容量的DRAM（16GB），最大化Memory Channel的利用率，提高每个服务器可用的内存带宽。&lt;/p&gt;
&lt;p&gt;使用NUMA来避免跨Sockets内存访问。&lt;/p&gt;
&lt;h3 id=&#34;tcp-to-rdma&#34;&gt;TCP to RDMA&lt;/h3&gt;
&lt;p&gt;TCP使用更多的内存拷贝，使用RDMA后，后台流量的内存带宽减少了75%。&lt;/p&gt;
&lt;p&gt;为了保证前端流量的QoS，盘古设计了类似Linux tc频率控制机制，来控制后台流量发送到网络中的速率。&lt;/p&gt;
&lt;h3 id=&#34;rdca&#34;&gt;RDCA&lt;/h3&gt;
&lt;p&gt;使得发送端绕过接收端的内存并直接访问接收端的cache。&lt;/p&gt;
&lt;p&gt;该池使用共享接收器队列（SRQ）来接收小消息，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存驻留缓冲池：使用共享接受队列来接受小信息，并使用基于窗口（类似TCP滑动窗口？）的速率控制机制的READ缓冲区来接收大消息，因此RDMA操作所需的内存缓冲区可以放入缓存中&lt;/li&gt;
&lt;li&gt;快速缓存回收：为了在少量LLC二级Cache下达到100Gbps NIC线性速率操作，设计两种机制减少数据通过NIC的时间跨度：
&lt;ol&gt;
&lt;li&gt;在一个流水线下平行处理数据&lt;/li&gt;
&lt;li&gt;通过硬件卸载和轻量化的序列化/反序列化 优化数据处理&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;缓存压力感知的逃逸机制：解决性能抖动，逃逸机制监控LLC的使用并执行响应的策略
&lt;ol&gt;
&lt;li&gt;通过向缓存驻留缓冲池添加一个新的缓冲区来替换掉队数据的缓存缓冲区，这样RDCA中用于适应新到达的请求的可用缓存的大小保持不变&lt;/li&gt;
&lt;li&gt;如果发生太多替换，则主动将运行缓慢的应用程序的数据复制到内存中，这样其他应用程序可以使用RDCA缓冲池，并且该池不会占用太多缓存&lt;/li&gt;
&lt;li&gt;让NIC在&lt;strong&gt;拥塞通知包&lt;/strong&gt;中标记显式&lt;strong&gt;拥塞通知&lt;/strong&gt;（ECN），如果复制到内存失败或不足以释放缓存压力，则指示拥塞&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cpu瓶颈&#34;&gt;CPU瓶颈&lt;/h2&gt;
&lt;p&gt;数据的序列化和反序列化、数据压缩、数据CRC校验产生了大量的CPU开销&lt;/p&gt;
&lt;h3 id=&#34;混合rpc&#34;&gt;混合RPC&lt;/h3&gt;
&lt;p&gt;盘古通过Protobuf（Google gRPC）发送RPC请求，该框架需要对数据进行序列化/反序列化，产生了30%的CPU开销。&lt;/p&gt;
&lt;p&gt;盘古内部的数据通路只有少量为RPC，因此，为了不需要序列化即可发送数据，盘古使用一种类似FlatBuffer的原生结构取代数据通路操作。&lt;/p&gt;
&lt;p&gt;得益于Protobuf灵活性，盘古继续使用Protobuf来进行控制操作。&lt;/p&gt;
&lt;p&gt;每个CPU核心的网络吞吐量增加了59%。&lt;/p&gt;
&lt;h3 id=&#34;使用cpu-wait支持超线程&#34;&gt;使用CPU wait支持超线程&lt;/h3&gt;
&lt;p&gt;hyper-threading（HT）超线程技术有两个主要问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;两个HT在一个无力核心上执行需要上下文切换&lt;/li&gt;
&lt;li&gt;一个HT影响其他HT的执行，会使整体的延迟升高&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;盘古提出了CPU wait指令，包括&lt;strong&gt;monitor&lt;/strong&gt;和&lt;strong&gt;mwait&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;例如：网络空闲轮询线程将监控的内存地址，执行mwait，直到内存地址被其他线程修改时才被唤醒。&lt;/p&gt;
&lt;p&gt;在mwait过程中，它运行的HT进入&lt;strong&gt;空闲睡眠状态&lt;/strong&gt;（C-States中的除C0之外的一个状态），因此不会干扰其他HT。&lt;/p&gt;
&lt;p&gt;执行CPU wait的时间开销小于5ms，唤醒一个HT的开销也是ms级别。&lt;/p&gt;
&lt;p&gt;对比没有CPU wait，网络吞吐量提高了31.6%。&lt;/p&gt;
&lt;h3 id=&#34;软硬件co-desgin&#34;&gt;软硬件Co-Desgin&lt;/h3&gt;
&lt;p&gt;将一些任务卸载到可编程硬件。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数据压缩&lt;/strong&gt;：卸载到基于FPGA的可计算存储，达到3GB/s的吞吐量，可以节省10个物理核心。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CRC计算&lt;/strong&gt;：卸载到RDMA-总线 NIC网卡，为每个数据块计算CRC，CPU会汇总这些CRC并执行轻量化的检查，保证了CPU的低负载和上层应用的数据集成。节省了30%的CPU开销。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;评估-1&#34;&gt;评估&lt;/h2&gt;
&lt;p&gt;标准化的有效吞吐量：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230311205320296.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230311205320296&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;从左到右：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;盘古使用2*25GBps网络带宽和4TB-SDD&lt;/li&gt;
&lt;li&gt;SSD增加到更高性能的8TB，使用100Gbps网络（PCIe gen3）&lt;/li&gt;
&lt;li&gt;优化了内存带宽&lt;/li&gt;
&lt;li&gt;使用PCIe gen4，网络带宽增加到2*100Gbps，存在流量放大问题。&lt;/li&gt;
&lt;li&gt;Client采取EC(4,2)，数据压缩等优化缓解了流量放大问题&lt;/li&gt;
&lt;li&gt;卸载数据压缩和CRC到外置硬件&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x04-operation-experiences&#34;&gt;0x04 Operation Experiences&lt;/h1&gt;
&lt;p&gt;介绍了在开发运营过程中遇到的一些案例研究&lt;/p&gt;
&lt;h1 id=&#34;0x05-lessons&#34;&gt;0x05 Lessons&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;user-space system：
&lt;ul&gt;
&lt;li&gt;在user space开发节省人力并且容易监控和调整参数&lt;/li&gt;
&lt;li&gt;user space开发需要学习kernel space开发&lt;/li&gt;
&lt;li&gt;独立chunk布局还优化了HDD的写入效率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tradeoff：成本和性能的平衡，提高资源利用率&lt;/li&gt;
&lt;li&gt;PMem：Intel放弃了PMem的业务&lt;/li&gt;
&lt;li&gt;hardware offloading：付出了大量成本解决这一问题，收益明显&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x06-conclusion&#34;&gt;0x06 Conclusion&lt;/h1&gt;
&lt;p&gt;文章介绍了盘古从1.0版本到2.0版本的整体演进过程，有非常大的实战经验，给出了对于分布式文件系统、面向性能的服务模式、CPU瓶颈、内存带宽的实现方案。文章个别地方没有很详细的描述，阿里云团队也发表过多篇学术论文，本文还是总结为主。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>[OSDI&#39;22] XRP: In-Kernel Storage Functions with eBPF</title>
        <link>https://blog.ipandai.club/p/osdi22-xrp-in-kernel-storage-functions-with-ebpf/</link>
        <pubDate>Thu, 09 Feb 2023 14:51:48 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi22-xrp-in-kernel-storage-functions-with-ebpf/</guid>
        <description>&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;随着超低延迟SSD的发展，I/O过程中来自内核的延迟比重不断升高。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230224131538122.png&#34; alt=&#34;image-20230224131538122&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;观察一个请求的从发起到完成的整个过程，内核部分占据了48.6%的延迟。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230224132654967.png&#34; alt=&#34;image-20230224132654967&#34; style=&#34;zoom: 67%;&#34; /&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225105147344.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230225105147344&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;因此可以考虑跳过内核中一系列的数据传递，即Kernel Bypass。目前现有的研究大都对内核进行了激进的修改策略，或引入了新的硬件，Kernel Bypass主要使用SPDK，直接对硬件设备进行访问，而SPDK会强制开发者实现自己的文件系统，需要自行维护隔离性和安全性。&lt;/p&gt;
&lt;p&gt;在等待I/O完成时，往往会进行轮询，给CPU带来了一些性能损失。有研究表明当可调度的线程数量超过了CPU核心数量时，使用SPDK会提高平均延迟和尾延迟，严重降低了吞吐量。&lt;/p&gt;
&lt;p&gt;因此作者的核心思想为既可以完成Kernel Bypass，也不需要引入额外硬件并不对内核和文件系统进行很大的修改，作者借助BPF(Berkeley Packet Filter)来实现，BPF允许应用程序下放部分工作到内核中，同时还能保证隔离性，允许多线程共享一个CPU核心，提高利用率。&lt;/p&gt;
&lt;p&gt;许多I/O负载需要多次调用函数访问硬盘上的大型数据结构（B+ tree），作者称之为resubmission。&lt;/p&gt;
&lt;p&gt;作者提出了eXpress Resubmission Path，XRP在NVMe驱动中的中断处理器上添加了一个hook，使得XRP可以在I/O完成时直接从NVMe驱动层来发起BPF函数调用，快速启动I/O的重新提交。&lt;/p&gt;
&lt;p&gt;XRP的主要贡献为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首次使用BPF来卸载I/O任务到内核&lt;/li&gt;
&lt;li&gt;将B-tree的查找吞吐量提高了2.5倍&lt;/li&gt;
&lt;li&gt;XRP提供了接近Kernel Bypass的延迟，而且允许线程和进程高效地共享CPU核心&lt;/li&gt;
&lt;li&gt;XRP适用于多种不同用例，支持不同类型数据结构以及存储操作&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x01-bg--motivation&#34;&gt;0x01 Bg &amp;amp;&amp;amp; Motivation&lt;/h1&gt;
&lt;h2 id=&#34;io是目前的瓶颈&#34;&gt;I/O是目前的瓶颈&lt;/h2&gt;
&lt;h3 id=&#34;时间都去哪了&#34;&gt;时间都去哪了&lt;/h3&gt;
&lt;p&gt;作者通过实验发现延迟的部分来源是软件层面，即系统内核部分中block I/O的传递&lt;/p&gt;
&lt;h3 id=&#34;为什么不单纯的kernel-bypass&#34;&gt;为什么不单纯的kernel bypass&lt;/h3&gt;
&lt;p&gt;大部分bypass的方法都是直接对NVMe driver发起请求，此类方案不能实现细粒度的隔离或者再不同应用之间共享数据，同时不能有效的去接收I/O完成产生的中断，需要应用不断轮询，因此CPU就被某个应用独占，不能得到共享。而且当多个轮询线程共享一个CPU处理器时，他们之间对CPU的竞争同时缺少同步会导致尾延迟升高和吞吐量的降低。&lt;/p&gt;
&lt;h2 id=&#34;bpf&#34;&gt;BPF&lt;/h2&gt;
&lt;p&gt;Berkeley Packet Filter允许用户将一些简单函数下放到内核层来执行，起初是用于TCP的数据包过滤、负载均衡、数据包转发，目前推出了eBPF扩展。&lt;/p&gt;
&lt;p&gt;BPF可以验证函数的安全性，主要检查是否超出内存地址空间、是否有死循环、指令是否太多。&lt;/p&gt;
&lt;p&gt;BPF可以直接发起一系列I/O请求，来获取那些不被程序直接利用的中间数据，例如指针寻址过程，B-tree索引便利。&lt;/p&gt;
&lt;p&gt;一些在硬盘维护并且使用指针来查找的数据结构，例如LSM tree，可以用于加速查找的过程。&lt;/p&gt;
&lt;p&gt;有研究对比了分别从User Space、Syscall、NVMe Driver层发起I/O之间吞吐量和延迟的差异。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225134629890.png&#34; alt=&#34;image-20230225134629890&#34; style=&#34;zoom: 33%;&#34; /&gt;
&lt;p&gt;在NVMe Driver中发起I/O有效提高了吞吐量降低了延迟，原因是越靠近存储设备的一层，总体的延迟和性能越高。因此XRP选择在NVMe Driver层来实现。&lt;/p&gt;
&lt;h3 id=&#34;io_uring&#34;&gt;io_uring&lt;/h3&gt;
&lt;p&gt;io_uring是Linux的一个系统调用，可以批量提交异步I/O，并且相较于aio减少了系统调用的次数，作者通过实验验证了在NVMe Driver层提交I/O也可以提高io_uring的性能。&lt;/p&gt;
&lt;h1 id=&#34;0x02-challenges--principles&#34;&gt;0x02 Challenges &amp;amp;&amp;amp; Principles&lt;/h1&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;地址转换和安全问题：NVMe Driver不能访问文件系统的元数据，因此不能完成逻辑地址到物理地址的转换；BPF可以访问其他文件和用户的任何block。&lt;/li&gt;
&lt;li&gt;并发和缓存问题：对于从文件系统发起的并发读写很难维护。从文件系统发出的写请求将写入到page cache中，对于XRP不可见；同时对于硬盘上数据结构布局的修改，例如改变某一个指针，将会导致XRP获取到错误的数据，虽然可以进行加锁，但是从NVMe中断访问锁的开销太大。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;observation&#34;&gt;Observation&lt;/h2&gt;
&lt;p&gt;作者研究发现，大部分的存储引擎的硬盘数据结构都是稳定的，或不会进行就地更新。&lt;/p&gt;
&lt;p&gt;例如在LSM-tree中，进行索引的写入操作，写入到文件&lt;code&gt;SSTables&lt;/code&gt;中，这些文件为不可变，直至其被删除，不可变文件还降低了文件并发访问的成本。B-tree的索引虽然可以就地更新，但是在实际测试中（24小时YCSB读写改实验）发现，索引的修改次数很少，因此也不需要在NVMe Driver中频繁的更新文件系统的元数据。&lt;/p&gt;
&lt;p&gt;同时，索引一般存储在少量的大文件中，并且每个索引不会跨越多个文件。&lt;/p&gt;
&lt;h2 id=&#34;design-principles&#34;&gt;Design Principles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;一次只访问一个文件：可以简化地址转换和访问控制，还最小化了需要传递给NVMe driver的元数据&lt;/li&gt;
&lt;li&gt;面向于稳定的数据结构：XRP以不会频繁更新的数据结构为目标RocksDB、LevelDB、TokuDB、WiredTiger，不支持需要锁来访问的数据结构&lt;/li&gt;
&lt;li&gt;用户管理缓存：XRP无法直接访问Page Cache，因此如果block位于Page Cache中，XRP函数不能安全的并发执行，而大多数存储引擎都是在用户空间自行管理Cache。&lt;/li&gt;
&lt;li&gt;错误处理：如果访问失败（映射信息过期），需要程序在用户空间重试或回滚。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x03-design--implementation&#34;&gt;0x03 Design &amp;amp;&amp;amp; Implementation&lt;/h1&gt;
&lt;p&gt;XRP的架构图：&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225161425201.png&#34; alt=&#34;image-20230225161425201&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;resubmission-logic&#34;&gt;Resubmission Logic&lt;/h2&gt;
&lt;p&gt;XRP的Resubmission主要包含3个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BPF hook&lt;/li&gt;
&lt;li&gt;文件系统地址翻译&lt;/li&gt;
&lt;li&gt;NVMe请求的构建和提交&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当NVMe指令请求完成后，设备发起中断使内核发生上下文切换执行中断处理，对于在&lt;code&gt;中断上下文&lt;/code&gt;(即ISR中断处理程序保存的请求发起程序的上下文)每个NVMe请求，XRP调用相关的BPF函数，即上图的&lt;code&gt;bpf_func_0&lt;/code&gt;，该函数指针存储在内核I/O请求的结构体中（bio）。&lt;/p&gt;
&lt;p&gt;在调用BPF函数后，XRP调用元数据信息摘要，此文件系统摘要用于做地址转换。&lt;/p&gt;
&lt;p&gt;最终，XRP发起下一次NVMe请求，将请求append到相应CPU核心NVMe的SQ中完成resubmission（如上图所示）。&lt;/p&gt;
&lt;h3 id=&#34;bpf-hook&#34;&gt;BPF Hook&lt;/h3&gt;
&lt;p&gt;XRP引入&lt;code&gt;BPF_PROG_TYPE_XRP&lt;/code&gt;作为BPF函数的签名（函数签名一般包括函数的参数类型、个数等），符合该签名的程序可以被BPF Hook调用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225182708553.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230225182708553&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;参数解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;：存储从硬盘中读取的信息（例如B-tree Page），可以被BPF函数解析&lt;/li&gt;
&lt;li&gt;&lt;code&gt;done&lt;/code&gt;：在resubmission阶段通过此字段判断是否将数据上报给用户程序，或是进行后续I/O&lt;/li&gt;
&lt;li&gt;&lt;code&gt;next_addr&lt;/code&gt;：表示下一次resubmission要请求的逻辑地址，最大输入量fanout默认限制在16&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size&lt;/code&gt;：表示&lt;code&gt;next_addr&lt;/code&gt;逻辑地址的大小，某些情况下可以将其设置为0表示不发起I/O&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scratch&lt;/code&gt;：对于用户和BPF函数私有，用于向BPF函数传递用户的参数，BPF函数也可以在此存储两次I/O之间的中间变量，亦或是保存传递给用户的数据。相当于BPF可以利用的一个4KB的buffer。如果BPF函数需要更大的空间，还可以使用BPF maps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BPF上下文对于一个NVMe请求是私有的，不需要考虑加锁。为用户提供scratch buffer避免了调用&lt;code&gt;bpf_map_lookup_elem&lt;/code&gt;来访问buffer产生的开销。&lt;/p&gt;
&lt;h3 id=&#34;bpf-verifier&#34;&gt;BPF Verifier&lt;/h3&gt;
&lt;p&gt;BPF校验器通过追踪&lt;code&gt;寄存器中value的语义&lt;/code&gt;来保证内存安全。&lt;/p&gt;
&lt;p&gt;一个有效的value既可以是一个标量也可以是一个指针。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SCALAR_TYPE&lt;/code&gt;表示一个不能被指针取值(cannot be dereference)的value。&lt;/p&gt;
&lt;p&gt;verifier定义了多种指针类型。&lt;/p&gt;
&lt;p&gt;每个BPF函数都定义了一个回调函数&lt;code&gt;is_valid_access()&lt;/code&gt;来检查上下文访问是否正确并返回上下文中value type字段。&lt;/p&gt;
&lt;p&gt;PTR_TO_MEM表示一个指向固定内存大小的指针，支持直接通过offset取值（不超过边界的情况下），&lt;code&gt;BPF_PROG_TYPE_XRP&lt;/code&gt;的data和scratch字段通过&lt;code&gt;PTR_TO_MEM&lt;/code&gt;访问，其余部分则为SCALAR_TYPE。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;is_valid_access()&lt;/code&gt;还可以用来传递data/scratch buffer的大小，用于做边界检查。&lt;/p&gt;
&lt;h3 id=&#34;metadata-digest&#34;&gt;Metadata Digest&lt;/h3&gt;
&lt;p&gt;为了完成在NVMe Driver中的地址转换，XRP在文件系统和中断处理程序之间添加了一个接口。文件系统共享逻辑地址到物理地址的映射。&lt;/p&gt;
&lt;p&gt;元数据摘要共有两个函数，如下图所示。&lt;/p&gt;
&lt;p&gt;当地址映射被更新时，&lt;strong&gt;文件系统&lt;/strong&gt;将调用update函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中断处理程序&lt;/strong&gt;则负责调用lookup函数，通过接收到的inode地址，给出offset和长度获取地址映射；lookup中也会有越界检查来保证安全性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225205022040.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230225205022040&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;当检测到不合法的逻辑地址，XRP将立即返回错误代码到用户空间。&lt;/p&gt;
&lt;p&gt;作者在ext4文件系统的实现中，元数据摘要来自缓存中的extent status tree，使用RCU机制进行并发控制。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;太长不看：&lt;/p&gt;
&lt;p&gt;RCU是一种类似于COW写时复制的机制，因为读取操作不需要加锁，在保证数据一致性的同时可以保证高性能，RCU主要用于Linux内核中对高并发访问数据结构的优化，而写时复制主要用于内存管理、文件系统等领域。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linux RCU：&lt;/p&gt;
&lt;p&gt;Linux RCU（Read-Copy-Update）是一种用于管理并发访问共享数据结构的技术。它是一种无锁并发机制，允许多个线程同时读取共享数据，而不需要互斥锁或读写锁等传统的同步机制，从而提高了并发性能和可伸缩性。&lt;/p&gt;
&lt;p&gt;RCU的基本思想是，当一个线程需要更新共享数据时，它将数据复制一份，并在副本上进行修改，而不是在原始数据上进行修改。其他线程仍然可以访问原始数据，直到更新完成。在更新完成后，新的数据副本将替换原始数据，并且RCU机制将确保在该更新之前任何已经读取的数据都不会丢失。&lt;/p&gt;
&lt;p&gt;RCU适用于高并发访问数据结构的场景，例如哈希表、链表和树等数据结构。它已经被广泛应用于Linux内核中，使得Linux内核在多处理器系统上的性能得到显著提升。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;ext4中，extents扩展块是文件系统中一段连续的块，一般用于存储大文件的数据信息。&lt;/p&gt;
&lt;p&gt;为了保证缓存数据与ext4中的extents一致，update函数会在extents发生修改或者删除时调用。同时为了防止竞态，为每个扩展块维护了一个版本号。&lt;/p&gt;
&lt;p&gt;在数据读取后，但还没有传递给BPF函数之前，会先调用元数据摘要的查找。如何相应的extents已经被删除或者当前版本号已经落后，XRP将中断此次操作。&lt;/p&gt;
&lt;p&gt;一般而言，为保障安全和正确性，在程序之间进行同步时就会对文件中一个部分的加锁，因此版本号的改变大多来自于恶意修改或程序出错。&lt;/p&gt;
&lt;p&gt;一种更简单的实现利用ext4文件系统现有的extent tree更新和访问函数来获取元数据摘要，这会保证ext4中的extent tree时刻是最新状态，但是在查找ext4中extent查找函数会获取自旋锁，对于中断处理程序带来了极大的开销，因此不太合适。&lt;/p&gt;
&lt;p&gt;目前XRP仅支持了ext4文件系统。对于F2FS可以直接利用存储了逻辑物理地址块映射的Node Address Table（NAT），通过保存一个NAT的副本来得到元数据摘要。每次更新NAT都调用&lt;code&gt;update_mapping&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;resubmitting-nvme-request&#34;&gt;Resubmitting NVMe Request&lt;/h3&gt;
&lt;p&gt;在查询到数据块的物理地址后，XRP将准备提交NVMe请求。&lt;/p&gt;
&lt;p&gt;XRP重复利用刚刚完成的NVMe请求结构，修改NVMe请求中的物理扇区和块地址为通过lookup函数查询到的偏移量，然后继续进行请求。&lt;/p&gt;
&lt;p&gt;由于bpf_xrp支持的最大地址个数为16，目前的resubmit只能获取到和初始NVMe请求一样多的物理段。也就是第一个NVMe请求只申请了一个物理段，那么后续的NVMe请求也只能去获取一个物理段。&lt;/p&gt;
&lt;p&gt;XRP会中断包含无效地址的BPF调用，可以在第一次I/O中next_addr中设置16个伪NVMe指令，以保证后续的请求可以有足够的访问空间。&lt;/p&gt;
&lt;h2 id=&#34;synchronization-limitations&#34;&gt;Synchronization Limitations&lt;/h2&gt;
&lt;p&gt;BPF目前只支持自旋锁进行同步。&lt;/p&gt;
&lt;p&gt;Verifier只允许BPF程序一次只获取一个锁，并且在函数退出前要将其释放。&lt;/p&gt;
&lt;p&gt;用户程序不需要直接去访问BPF的自旋锁，可以直接进行BPF的系统调用，该系统调用可以在持有锁时对保护的数据结构进行读写。&lt;/p&gt;
&lt;p&gt;需要在多个读写操作之间进行同步的复杂操作在用户空间无法完成。&lt;/p&gt;
&lt;p&gt;用户可以通过BPF的原子操作实现自定义的自旋锁，这允许BPF函数和用户程序直接获取任何自旋锁。然而BPF函数的行为会被限制，防止无限地等待某一个自旋锁。&lt;/p&gt;
&lt;p&gt;另一种同步方法就是RCU。XRP BPF都运行在NVMe中断处理程序上，而中断处理程序不能被抢占，可以认为他们已经进入了RCU读端临界区（a RCU read-side critical section）的状态，因为在中断处理程序执行时，其他任务无法访问共享数据结构，从而避免了并发冲突。因此，在这种情况下，BPF程序不需要获得锁来读取共享数据结构，从而避免了锁的开销，并提高了程序的执行效率。&lt;/p&gt;
&lt;h2 id=&#34;linux-schedulers&#34;&gt;Linux Schedulers&lt;/h2&gt;
&lt;h3 id=&#34;进程调度&#34;&gt;进程调度&lt;/h3&gt;
&lt;p&gt;作者研究发现根据Linux的CFS完全公平调度算法，当计算密集型的进程和I/O密集型的进程运行在一个核心时，由于超低延迟SSD的中断过于频繁，使得I/O密集型应用产生的I/O中断占用大量时间片，导致计算密集型进程的饥饿。而在普通SSD上，由于中断产生的频率较低，不会发生此现象。&lt;/p&gt;
&lt;p&gt;连续产生中断较多的XRP、网络中断较多的程序会加剧（exarcerbates）此问题，作者计划在之后考虑此问题。&lt;/p&gt;
&lt;h3 id=&#34;io调度&#34;&gt;I/O调度&lt;/h3&gt;
&lt;p&gt;NVMe默认使用noop调度器，如果需要公平调度，可以开启硬件队列总裁。&lt;/p&gt;
&lt;h1 id=&#34;0x04-case-studies&#34;&gt;0x04 Case Studies&lt;/h1&gt;
&lt;h2 id=&#34;use-xrp&#34;&gt;Use XRP&lt;/h2&gt;
&lt;p&gt;用户调用下图两个接口来加载BPF函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bpf_prog_load&lt;/code&gt;：加载&lt;code&gt;BPF_PROG_TYPE_XRP&lt;/code&gt;类型的BPF函数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;read_xrp&lt;/code&gt;：将调用BPF函数应用到一个具体的请求上&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230226143117688.png&#34; alt=&#34;image-20230226143117688&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;p&gt;具体的使用实例，作者给出了两个示例。&lt;/p&gt;
&lt;h2 id=&#34;bpf-kv&#34;&gt;BPF-KV&lt;/h2&gt;
&lt;p&gt;作者使用B+树在磁盘组织数据，同时自行维护DRAM Cache来对块和对象进行索引。&lt;/p&gt;
&lt;p&gt;最大请求数量限制为31，保证可以遍历一个节点的所有31个孩子。&lt;/p&gt;
&lt;p&gt;BPF函数用于查找KV对，首先获取在scratch buffer中的查询目标key，并线性地在node中查找。&lt;/p&gt;
&lt;p&gt;使用&lt;code&gt;read_xrp&lt;/code&gt;来实现read系统调用，BPF-KV首先为scratch buffer分配一块空间，并计算出offset来启动read。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230226152454999.png&#34; alt=&#34;image-20230226152454999&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;BPF-KV支持范围查询，查询过程中的数据保存在scratch buffer中。&lt;/p&gt;
&lt;p&gt;BPF-KV支持SUM、MAX、MIN此类操作&lt;/p&gt;
&lt;h2 id=&#34;wiredtiger&#34;&gt;WiredTiger&lt;/h2&gt;
&lt;p&gt;WiredTiger是MongoDB的默认存储引擎，可以使用LSM-tree作为数据结构，数据保存于LSM的不同level之间，每个level保存在一个文件中，没个文件使用B-tree索引。所有的文件为只读，更新和插入操作先写入如buffer中，buffer空间不足时保存到一个新的文件中。&lt;/p&gt;
&lt;p&gt;XRP只针对read操作做优化，update和insert在buffer中维护。&lt;/p&gt;
&lt;p&gt;WT的BPF函数和BPF-KV差不多，WT需要对B-tree的Page做解析操作。&lt;/p&gt;
&lt;h1 id=&#34;0x05-thinking&#34;&gt;0x05 Thinking&lt;/h1&gt;
&lt;p&gt;XRP是OSDI&#39;22的best paper，文章整体的工作量非常大，修改了ext4文件系统、BPF模块，并设计了优化后的KV存储引擎。文章从很新颖的角度优化了数据库引擎等I/O密集型应用，不需要频繁传递数据于用户空间和NVMe Driver时间，也减少了I/O过程中在轮询时的开销，提高了CPU利用率，通过BPF这一内核特性可以实现自定义I/O函数，同时保证一定的安全性和隔离。&lt;/p&gt;
&lt;h1 id=&#34;0x06-reference--more-information&#34;&gt;0x06 Reference &amp;amp;&amp;amp; More information&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LSM 不可修改文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kernel.org/doc/html/latest/RCU/whatisRCU.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org】 RCU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://draveness.me/mongodb-wiredtiger/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【draveness】MongoDB和Wiretiger存储引擎&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>[FAST&#39;22] MT^2: Memory Bandwidth Regulation on Hybrid NVM/DRAM Platforms</title>
        <link>https://blog.ipandai.club/p/fast22-mt2-memory-bandwidth-regulation-on-hybrid-nvm/dram-platforms/</link>
        <pubDate>Mon, 19 Dec 2022 16:50:00 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/fast22-mt2-memory-bandwidth-regulation-on-hybrid-nvm/dram-platforms/</guid>
        <description>&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;NVM和DRAM共享内存总线，二者之间的负载相互干扰，给混合NVM/DRAM平台的带宽分配带来了挑战&lt;/p&gt;
&lt;p&gt;本文提出了MT2，可以在混合NVM/DRAM平台中管理并发程序间的内存带宽。&lt;/p&gt;
&lt;p&gt;MT2首先检查内存流量间的干扰并通过硬件监视器和软件汇报从混合流量监控不同类型的内存带宽&lt;/p&gt;
&lt;p&gt;然后MT2利用一个动态的带宽流量调节算法基于多种方式来管理内存带宽&lt;/p&gt;
&lt;p&gt;为了能够更好的管理不同程序，MT2被集成到了cgroup中，添加了一个用于带宽分配的cgroup subsystem&lt;/p&gt;
&lt;p&gt;新兴的NVM存储器逐渐被用来做为可持久化内存来使用，基于NVM，提出了NVM文件系统，NVM编程库，NVM数据结构、NVM数据库，NVM作为大容量内存或者快速的字节寻址存储设备用于数据中心。&lt;/p&gt;
&lt;p&gt;然而NVM/DRAM混合平台加剧了吵闹邻居问题。在云存储环境中，多个用户常常共享一个服务器，不同用户的不同应用程序共享主机的一条总线，某些应用可能会过度使用内存带宽。在NVM/DRAM中，NVM和DRAM共享同一个总线，因此不同的应用程序将竞争有限的内存带宽，影响整体的性能&lt;/p&gt;
&lt;p&gt;在NVM/DRAM中调节带宽有如下几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;内存带宽不对称，在NVM/DRAM，不同的内存访问（如DRAM read，DRAM write，NVM read和NVM write）会产生不同的最大内存带宽。内存的实际可用带宽主要取决于负载中不同类型访问的占比。同时设置静态的内存带宽而不考虑实际的I/O访问占比是不正确的做法。同时NVM最大带宽通常小于DRAM。而且不同类型的内存访问的干扰程度不同，因此不能单纯将所有内存==访问延迟==视为相等&lt;/li&gt;
&lt;li&gt;NVM和DRAM共享内存总线，NVM流量和DRAM流量不可避免地会混合并且很难区分。对于混合的流量，监控不同种类的内存带宽。由于内存流量混合，几乎不可能在每个进程的基础上监控不同类型的内存带宽，这使为DRAM设计的现有硬件和软件调节方法无效。&lt;/li&gt;
&lt;li&gt;内存调节的硬件和软件机制不足。由于NVM和DRAM都可以通过CPU负载/存储指令直接访问，因此为了性能，对每个内存访问进行计算和限流是不切实际的。CPU供应商，如英特尔，支持硬件机制来调节内存带宽。然而，带宽限制是粗粒度和定性迭代的，这不足以精确的内存带宽调节。其他一些方法，如频率缩放和CPU调度，可能会提供相对细粒度的带宽调整。然而，它们也是定性的，并减慢了计算和内存访问的速度，因此对整个平台性能缺乏影响。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MT2作为Linux Cgroup中的一个Subsystem，用于减轻吵闹邻居问题。在内存带宽分配和云SLO保证方面提高效率。本文的主要贡献：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;发现了导致NVM/DRAM混合平台上内存密集型应用程序显著性能流失的内存带宽干扰问题&lt;/li&gt;
&lt;li&gt;首次对在NVM/DRAM平台现存的硬件和软件带宽分配机制进行研究&lt;/li&gt;
&lt;li&gt;高效有效地调节具有线程级粒度的NVM/DRAM混合平台上的内存带宽&lt;/li&gt;
&lt;li&gt;在英特尔Optane SSD上进行了测试和分析&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以下简称NVM/DRAM&lt;/p&gt;
&lt;h1 id=&#34;0x01-bg&#34;&gt;0x01 BG&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;MT^2&lt;/strong&gt; 持久内存（PM）/ 非易失性内存（NVM）的出现正改变着存储系统的金字塔层次结构。本文发现，由于 NVM 和 DRAM 共享同一条内存总线，带宽干扰问题变得更为严重和复杂，甚至会显著降低系统的总带宽。本工作介绍了对内存带宽干扰的分析，对现有软硬件技术进行了深入调研，并提出了一种在 NVM/DRAM 混合平台上监控调节并发应用的内存带宽的设计（MT^2）。MT^2 以线程为粒度准确监测来自混合流量的不同类型的内存带宽，使用软硬件结合技术控制内存带宽。在多个不同的用例中，MT^2 能够有效限制“吵闹邻居”（noisy neighbors），消除带宽干扰，保证高优先级应用的性能。&lt;/p&gt;
&lt;h2 id=&#34;noisy-neighbors&#34;&gt;Noisy Neighbors&lt;/h2&gt;
&lt;p&gt;在多租户的云环境中，内存带宽对应用程序的性能有很大影响。&lt;/p&gt;
&lt;p&gt;两种可以减轻吵闹邻居问题的方法是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prevention：主动为应用程序设置带宽限制（固定的）&lt;/li&gt;
&lt;li&gt;Remedy：系统检测吵闹邻居情况的出现并识别出吵闹的“邻居”对其进行限制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法需要去监控应用的带宽使用情况&lt;/p&gt;
&lt;h2 id=&#34;nvm&#34;&gt;NVM&lt;/h2&gt;
&lt;p&gt;NVM得益于其存储容量大，访问速度快的特性，正在逐步作为内存使用于商业服务器中。得益于PMDK（Persistent Memory Development Kit）同时还涌现出了大量基于NVM Memory的应用程序，如PmemKV，Pmem-RocksDB。&lt;/p&gt;
&lt;p&gt;NVM可以直接通过CPU的load/store指令进行访问&lt;/p&gt;
&lt;h2 id=&#34;memory-bandwidth-interference&#34;&gt;Memory Bandwidth Interference&lt;/h2&gt;
&lt;p&gt;由于NVM和DRAM共享内存总线，因此存在干扰问题。&lt;/p&gt;
&lt;p&gt;用两个Flexible I/O Tester来竞争总带宽进行测试，包括NVM/DRAM的读写，分别比较在不同的访问方式下的带宽干扰情况&lt;/p&gt;
&lt;p&gt;如下图所示，颜色越深表明干扰越明显，图中数据表示为在Task B运行的情况下Task A的吞吐量占Task Alone情况下的百分比（GB/s）&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223132031882.png&#34; alt=&#34;image-20230223132031882&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;从图中得出的两个结论为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;内存的干扰和内存的访问情况有关，占据更小带宽的内存访问可能会对其他访问造成更大的影响&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NVM的访问比DRAM访问对其他任务的影响更大，例如图中NVM Read列和DRAM Read列的对比，说明执行NVM write较多的应用更可能成为影响其他任务&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者还做了Task B进行NVM Write，Task A的延迟和吞吐量的变化，发现了&lt;strong&gt;随着带宽的逐渐降低，Task A的延迟逐渐增加。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;memory-bandwidth-monitoring&#34;&gt;Memory Bandwidth Monitoring&lt;/h2&gt;
&lt;p&gt;Intel的MBM技术可以用来监控从L3 Cache到下一级内存系统的带宽使用情况，即NVM或者DRAM，为每个逻辑核心都提供了硬件级别的内存带宽测量。&lt;/p&gt;
&lt;p&gt;每个逻辑核心分配了一个Resource Monitoring ID（RMID），一组逻辑核心可以分配相同的RMID，底层的硬件通过追踪具有相同RMID的内存带宽并将它们进行汇总。&lt;/p&gt;
&lt;h2 id=&#34;memory-bandwidth-allocation&#34;&gt;Memory Bandwidth Allocation&lt;/h2&gt;
&lt;p&gt;Intel MBA技术提供间接和粗略内存带宽控制，基本无性能损失。&lt;/p&gt;
&lt;p&gt;MBA在每个物理核心和共享L3 Cache之间引入了一个可编程的请求速率控制器，控制器通过在内存请求之间插入延迟来对内存带宽的使用进行限流。&lt;/p&gt;
&lt;p&gt;Intel要求使用Classes of Service（CLOS）为线程进行分组，然后为每个CLOS设置一个限流值。&lt;/p&gt;
&lt;h1 id=&#34;0x02-design&#34;&gt;0x02 Design&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221223002925874.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221223002925874&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，MT2工作在Kernel space，可以更方便高效地访问硬件信息。还能与内核其他组件高效的通信。&lt;/p&gt;
&lt;p&gt;MT2对user space暴露了相关管理接口，系统管理员可以将线程分配到不同的group并且制定各个group所使用的带宽，称为TGroup，Throttling Group&lt;/p&gt;
&lt;p&gt;MT2由两个部分组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor，负责从VFS，Performance Monitoring Unit（PMU），MBM收集性能数据，并将其分为四种类型DRAM/NVM R/W，将信息转发到Regulator&lt;/li&gt;
&lt;li&gt;Regulator，根据监控的数据和策略，执行两种机制：
&lt;ul&gt;
&lt;li&gt;调整MBA限流值&lt;/li&gt;
&lt;li&gt;调整CPU配额&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MT2使用动态带宽限流算法，根据实时带宽和干扰级别不断监控和调整限制。&lt;/p&gt;
&lt;p&gt;MT2提供两种缓解吵闹邻居的方案，来适应不同场景，如上文所示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prevention：系统管理员为每个TGroup设置带宽上限，MT2监控实时带宽并令所有group不会使用超过设定值的带宽。&lt;/li&gt;
&lt;li&gt;Remedy：对于没有超过带宽限制，但是仍然有很大影响的TGroup，可以由系统自动进行限制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;划分TGroup之后，管理员限制每个TGroup的最大带宽，监控器会保证每个TGroup的带宽不会超过阈值&lt;/p&gt;
&lt;p&gt;还要保证在大部分线程没有超过阈值时，产生的I/O干扰问题（空闲状态）&lt;/p&gt;
&lt;h2 id=&#34;monitor&#34;&gt;Monitor&lt;/h2&gt;
&lt;h3 id=&#34;带宽估计&#34;&gt;带宽估计&lt;/h3&gt;
&lt;p&gt;monitor需要获得不同种类I/O准确的带宽，例如DRAM的read/write带宽，NVM的read/write带宽。Regulator使用这些信息来决定是否要限制每个TGroup的内存带宽。&lt;/p&gt;
&lt;p&gt;对于NVM/DRAM的read带宽，可以直接通过PMU进行获取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVM Read Bandwidth：&lt;code&gt;ocr.all_data_rd.pmm_hit_local_pmm.any_snoop&lt;/code&gt; 需要通过PMU事件计数器来检索本地NVM read的数量并乘上cache line size 即64B来得出总带宽&lt;/li&gt;
&lt;li&gt;DRAM Read Bandwidth：&lt;code&gt;ocr.all_data_rd.l3_miss_local_dram.any_snoop&lt;/code&gt; DRAM PMU counter进行统计&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于Write的统计无法通过PMU，需要利用MBM来监控每个TGroup的总内存访问带宽，为DRAM read/write Bandwidth 和 NVM read/write Bandwidth的总和。二者的Read带宽可以很容易计算出来。&lt;/p&gt;
&lt;p&gt;只需通过周期性地收集NVM write的数量，就可以计算出一个TGroup总NVM write Bandwidth&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User-Space对于NVM的write只能通过文件系统API和通过MMap后的CPU store指令操作内存来实现，对于文件系统API可以hook内核VFS并监控每个TGroup NVM write 的数量。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于MMap的访问，作者基于app是否受信提出了两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于受信任的app（来自受信用户或者公司企业），使用他们来辅助收集使用mmap方式对NVM进行的write数量。作者提出了修改后的PMDK，调用PMDK的API刷新Cache lines 到NVM或执行永久地内存写入（movnt），通过计算并统计每个线程NVM write的数量到计数器中。为了要将计数器报告给MT2，每个进程都设置一个与内核共享的页面，进程中的每个线程将其每个线程计数器的值写入页面中的不同位置。内核中的MT2定期检查计数器，并计算每个TGroup的带宽。基于PMDK的应用无序进行修改，其他应用需要对源码进行少量修改。最终根据收集的NVM write信息，继续计算出DRAM的write 带宽&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
BW_{DW}=BW_{Total}-BW_{DR}-BW_{NR}-BW_{NW}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于不受信的app（可能不会按照规定上传NVM write带宽信息），只能进行粗略估计。利用PEBS（Processor Event Based Sampling 处理器事件采样），用于对每个TGroup带有目的地址的内存write操作进行采样，通过比较NVM的地址范围，可以计算出NVM和DRAM中采样的write操作比例，对NVM和DRAM write进行粗略估计。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;干扰检测&#34;&gt;干扰检测&lt;/h3&gt;
&lt;p&gt;MT2通过测量不同类型内存访问的延迟来检测干扰级别，因为内存访问延迟与带宽负相关&lt;/p&gt;
&lt;p&gt;MT2通过四个性能参数得到read延迟， &lt;code&gt;unc_m_pmm_rpq_occupancy.all &lt;/code&gt;($RPQ_O$), &lt;code&gt;unc_m_pmm_rpq_inserts&lt;/code&gt; ($RPQ_1$), &lt;code&gt;unc_m_rpq_occupancy&lt;/code&gt;, and &lt;code&gt;unc_m_rpq_inserts&lt;/code&gt;，通过$\frac{RPQ_O}{RPQ_1}$来计算NVM read延迟，同理得到DRAM read延迟&lt;/p&gt;
&lt;p&gt;MT2周期性地发起NVM和DRAM write请求，以此来测量完成时间，进而获得延迟&lt;/p&gt;
&lt;p&gt;通过设置阈值来判断带宽是否发生干扰&lt;/p&gt;
&lt;p&gt;干扰检测伪代码&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;detect_interference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bt&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bandwidth_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;latency&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;THRESHOLD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;作者使用吞吐量降低10%的延迟作为阈值&lt;/p&gt;
&lt;h2 id=&#34;regulator&#34;&gt;Regulator&lt;/h2&gt;
&lt;h3 id=&#34;内存调节机制&#34;&gt;内存调节机制&lt;/h3&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223164328178.png&#34; alt=&#34;image-20230223164328178&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;MBA&lt;/strong&gt;，通过设置限流值来对程序进行限制。MBA只支持throttling values，不能进行太精细的控制，MBA对DRAM敏感的负载的限制效果更好，对NVM-write的限制几乎没有效果，如上图b。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CPU调度&lt;/strong&gt;，可以进行更细粒度的内存带宽调整，利用Linux Cgroup CPU子系统来对应用使用的CPU核心进行限额。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有效性和比较&lt;/strong&gt;，作者通过对比测试后发现，CPU限流会影响应用程序的其他任务，限制其整体的效率，而MBA进对内存的访问进行限制，对程序整体的运行影响较小。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223170600388.png&#34; alt=&#34;image-20230223170600388&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;动态带宽限流&#34;&gt;动态带宽限流&lt;/h3&gt;
&lt;h4 id=&#34;识别吵闹邻居&#34;&gt;识别吵闹邻居&lt;/h4&gt;
&lt;p&gt;Prevention策略，所有超过带宽上限的TGroup都被视为吵闹邻居&lt;/p&gt;
&lt;p&gt;Remedy策略，Regulator首先根据Monitor提供的信息检查是否存在严重的内存干扰，当内存干扰情况严重时，算法会根据每个TGroup的带宽使用情况识别吵闹邻居，根据上文的分析，包含NVM write操作最多的TGroup最有可能是吵闹邻居，因此算法要选取这类TGroup并进行限流。&lt;/p&gt;
&lt;h4 id=&#34;调节内存带宽&#34;&gt;调节内存带宽&lt;/h4&gt;
&lt;p&gt;算法会根据内存访问的类型来选取调节方法。&lt;/p&gt;
&lt;p&gt;对于NVM访问带宽的限制，算法采用CPU限制&lt;/p&gt;
&lt;p&gt;对于DRAM访问带宽的限制，算法首先削减其TGroup的MBA，如果MBA已经设定为最小值，算法将使用CPU限流来进一步调节。&lt;/p&gt;
&lt;h4 id=&#34;释放内存限制&#34;&gt;释放内存限制&lt;/h4&gt;
&lt;p&gt;一旦内存干扰消失，算法就会尝试放宽带宽限制，操作过程和施加内存限制的方式相反。&lt;/p&gt;
&lt;p&gt;当Regulator结束了一次调节/释放操作时，就会继续等待下一步操作，取决于Monitor提供的信息。&lt;/p&gt;
&lt;p&gt;整体的工作流程在作者的Slides上有一张图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223191251147.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230223191251147&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x03-implementation&#34;&gt;0x03 Implementation&lt;/h1&gt;
&lt;h2 id=&#34;cgroup接口&#34;&gt;Cgroup接口&lt;/h2&gt;
&lt;p&gt;Cgroup是Linux内核中用于资源限制的工具，常用于容器化技术，目前正在整理有关内容计划分享一下。&lt;/p&gt;
&lt;p&gt;Cgroup通过暴露伪文件系统的文件实现其接口，Cgroupfs，用户通过修改文件的内容完成配置工作，MT2遵循Cgroup规范添加了TGroup，用于限流和管理的主要有以下几个文件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;priority&lt;/code&gt;：用于设置TGroup的优先级，包括&lt;code&gt;high&lt;/code&gt;和&lt;code&gt;low&lt;/code&gt;，&lt;code&gt;high&lt;/code&gt;优先级的TGroup不会被Regulator限制，&lt;code&gt;low&lt;/code&gt;优先级则会在内存干扰情况发生时被限制。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bandwidth&lt;/code&gt;：该文件只读，返回TGroup当前的带宽使用情况&lt;/li&gt;
&lt;li&gt;&lt;code&gt;limit&lt;/code&gt;用于获取和设置TGroup的绝对带宽，四个逗号分隔的数字来代表四种内存访问带宽上限，一旦带宽超过该值TGroup就会被限流。Zero Value表示无限制，该配置立即生效。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;线程创建问题&#34;&gt;线程创建问题&lt;/h2&gt;
&lt;p&gt;所有子进程都与父进程放在同一个TGroup中，除非管理员手动将其放入另一个TGroup。为此作者在进程/线程创建程序中添加了一个hook，即Linux内核中的&lt;code&gt;fork&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;mba&#34;&gt;MBA&lt;/h2&gt;
&lt;p&gt;MBA支持10个限流值，由于70、80、90的效果较差，作者只使用其中的8个值，并将每个值分配到一个CLOS中，将TGroup分配到相应的CLOS来实现不同等级的限流。在没有干扰的情况下，具有相同CLOS的TGroups有相同的请求速率。&lt;/p&gt;
&lt;h2 id=&#34;上下文切换&#34;&gt;上下文切换&lt;/h2&gt;
&lt;p&gt;为了给每个线程设置MT2上下文，包括设置PMU相关的上下文、写入与MBA相关的MSR寄存器以及设置CPU配额，作者给调度器添加了一个hook。每次发生上下文切换时，我们就为将要在这个CPU核上运行的新线程设置相应的MT2上下文。&lt;/p&gt;
&lt;h2 id=&#34;pmu&#34;&gt;PMU&lt;/h2&gt;
&lt;p&gt;用于计算没有命中Cache而进行NVM和DRAM访问的所有read指令。对于DRAM和NVM的read操作延迟也通过PMU获取。&lt;/p&gt;
&lt;h2 id=&#34;pebs&#34;&gt;PEBS&lt;/h2&gt;
&lt;p&gt;采样频率为10007，PEBS将每10007个事件记录一个线性地址，不会对系统产生太大的性能影响。&lt;/p&gt;
&lt;h2 id=&#34;专有内核线程&#34;&gt;专有内核线程&lt;/h2&gt;
&lt;p&gt;在MT2内核模块的初始化阶段创建一个内核线程，定期检测内存干扰，监控带宽并执行动态带宽限流。&lt;/p&gt;
&lt;p&gt;内核线程的伪代码如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;kthread_main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;current_time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;interference&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;detect_interference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TGroups&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;aggregate_bandwidths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;adjust_bandwidths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;interference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;sleep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;INTERVAL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;current_time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;0x04-thinking&#34;&gt;0x04 Thinking&lt;/h1&gt;
&lt;p&gt;作者在内核中实现了线程粒度的内存带宽监控，同时利用了多种方法来实现内存带宽的限流。&lt;/p&gt;
&lt;p&gt;同时利用了分析DRAM/NVM读写带宽之间的相互影响，从而确定了优化的方向和方法。&lt;/p&gt;
&lt;p&gt;本文章来自SJTU的IPADS实验室，文章中的论述非常有条理，采取的每一项措施都有实验验证，有例可循，例如选取NVM write操作较多的TGroup作为干扰项，以及MBA和CPU限额综合调整带宽，都有相应的实验分析。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>[ACM Trans. On Storage] HintStor: A Framework to Study I/O Hints in Heterogeneous Storage</title>
        <link>https://blog.ipandai.club/p/acm-trans.-on-storage-hintstor-a-framework-to-study-i/o-hints-in-heterogeneous-storage/</link>
        <pubDate>Wed, 30 Nov 2022 10:58:22 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/acm-trans.-on-storage-hintstor-a-framework-to-study-i/o-hints-in-heterogeneous-storage/</guid>
        <description>&lt;h1 id=&#34;0x00-everyday-english&#34;&gt;0x00 Everyday English&lt;/h1&gt;
&lt;p&gt;Heterogeneous 各种各样的&lt;/p&gt;
&lt;p&gt;semantic 语义上的&lt;/p&gt;
&lt;p&gt;conservative 保守的、传统的 &amp;mdash;aggressive 激进的&lt;/p&gt;
&lt;p&gt;aggregate 合计&lt;/p&gt;
&lt;p&gt;orchestrate 精心策划&lt;/p&gt;
&lt;p&gt;proactive 积极主动的&lt;/p&gt;
&lt;p&gt;by means of 通过，借助于&lt;/p&gt;
&lt;p&gt;off-the-shelf 现成的&lt;/p&gt;
&lt;p&gt;elaborate 详细阐述&lt;/p&gt;
&lt;h1 id=&#34;0x01-intro&#34;&gt;0x01 Intro&lt;/h1&gt;
&lt;p&gt;随着存储技术的发展，由SCM、SSD、HDD以及一系列云存储构成了目前异构存储系统&lt;/p&gt;
&lt;p&gt;异构存储系统将冷数据放在更慢的层次，热数据放在更快的层次，存储机制的激进和保守都会带来一些性能损失，而且异构存储系统由不同速度和特点的存储设备组成&lt;/p&gt;
&lt;p&gt;在异构存储系统中，还存在前台和后台I/O的干扰问题，带来明显的延迟，硬盘的管理层往往不能分辨出I/O请求的优先级高低，并决定将其数据存储到哪一层次&lt;/p&gt;
&lt;p&gt;这种情况被称为I/O栈高层次与底层存储系统之间的语义鸿沟，其中一个解决方案为使用I/O access hints，当app读取一个文件时，文件块可能散落在不同设备中，上层可以告知存储系统，使得存储系统提前将该文件的数据汇总到读取速度更高的层次，本文提出了一个分类器，允许存储后端控制器针对不同的I/O指令实施不同的I/O策略，例如SSD可以优先处理元数据和小文件，使其先缓存至文件系统中&lt;/p&gt;
&lt;p&gt;本文提出了一个通用灵活的框架，HintStor，为异构存储系统提供access hints&lt;/p&gt;
&lt;p&gt;设计和实现了一套新的用户层面的接口，文件系统插件，块存储数据管理器，在存储管理方面，HintStror通过在block级别进行统计（热力图等）来触发数据迁移，并通过FIFO队列处理I/O；HintStor还提供了access hint的评估，有以下几个要点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新的app/user层面的接口允许用户定义和配置新的hint&lt;/li&gt;
&lt;li&gt;VFS中的文件系统插件提取文件布局信息并建立文件层面的数据分类器，用于下层各种文件系统&lt;/li&gt;
&lt;li&gt;在基于DM的Linux中，块存储数据管理器实现了四个原子access hint操作，触发数据迁移和I/O调度，因此可以执行和分析与上层hint有关的不同策略&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者基于SSD、HDD、SCM以及云存储进行了评估，以体现系统的灵活性，实现并分析了以下四个hints：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用文件系统内部的数据进行分类（元数据，文件数据，文件大小）&lt;/li&gt;
&lt;li&gt;Stream ID，该ID用于对不同数据进行分类，并将相关数据存储在一起或紧密地存储在同一个设备上&lt;/li&gt;
&lt;li&gt;云预取，cloud prefetch，调研了在access hints情况下，如何帮助有效地集成本地存储和云存储。&lt;/li&gt;
&lt;li&gt;I/O任务调度，用户可以在应用层面向块存储设备发起hints，来对前台和后台任务进行区分&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x02-bg&#34;&gt;0x02 Bg&lt;/h1&gt;
&lt;p&gt;在目前的hints机制中，主要考虑的是宿主机的page cache和预取机制，目前system中的系统调用可以通过指定一个随机访问flag来告知内核选取正确的预取和page cache策略以优化对某个文件的访问。&lt;/p&gt;
&lt;p&gt;目前很少有对异构存储系统的优化，需要解决的问题是在不同的存储设备上的智能数据移动，&lt;code&gt;fadvise()&lt;/code&gt;和&lt;code&gt;inoice()&lt;/code&gt;系统调用可以改善prefetching，但是不能解决数据移动问题。&lt;/p&gt;
&lt;p&gt;MPI-I/O hints在高性能计算系统中优化文件系统，目前这些研究着眼于优化存储系统中的buffer/cache部分，red hat通过在用户空间限制I/O来对不同供应商的存储设备进行block对齐。&lt;/p&gt;
&lt;p&gt;目前有些文件系统支持自定义类别，btrfs可以在同一文件系统中支持不同的卷，同时需要用户为存储设备静态配置卷，用户可能让多个应用运行在一个逻辑卷中，为了实现高效的数据管理，作者考虑在卷上支持动态的access hints。&lt;/p&gt;
&lt;h1 id=&#34;0x03-hintstor-design&#34;&gt;0x03 HintStor Design&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221202204914922.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221202204914922&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;HintStor的架构图如上图所示&lt;/p&gt;
&lt;p&gt;Device Mapper是一个开源框架，为由多种块设备组成的卷提供映射&lt;/p&gt;
&lt;p&gt;整体包含三层，在应用层、文件系统层、块层提供了新的接口和插件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在用户层通过接口连接Block Storage Data Manager和文件系统，使得应用可以接受数据和发送access hints&lt;/li&gt;
&lt;li&gt;为了提取chunk的文件系统语义，HintStor将一个插件绑定到文件系统层，来利用内部文件的文件数据结构和文件数据布局&lt;/li&gt;
&lt;li&gt;为了控制对大chunk的数据管理，HintStor实现了新的block storage manager，可以实现对存储设备的access hints策略&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在Device Mapper中实现了两个功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redirector 将I/O请求转发到相应的设备&lt;/li&gt;
&lt;li&gt;Migrator 提供块设备间的异步数据拷贝，为了防止拷贝过程中对带宽的占用，会对流量进行限制，迁移后会通过一个flag来决定是否删除拷贝的原始数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interface&#34;&gt;Interface&lt;/h2&gt;
&lt;p&gt;HintStor使用sysfs接口为I/O请求设置一些属性，与块设备和文件系统通信，类似于一个伪文件系统，可以从其他内核的子系统中提取信息。&lt;/p&gt;
&lt;p&gt;在内核实现了部分系统调用，使用JSON格式的请求进行通信，JSON的编解码可能有性能损失？&lt;/p&gt;
&lt;p&gt;一些应用程序可能要对代码进行少量修改&lt;/p&gt;
&lt;h2 id=&#34;file-system-plugin&#34;&gt;File System Plugin&lt;/h2&gt;
&lt;p&gt;HintStor可以获取与请求相关的data block信息，提前触发数据迁移，优化后续的访问&lt;/p&gt;
&lt;p&gt;HintStor可以区分前台和后台I/O ，优先对前台I/O进行满足&lt;/p&gt;
&lt;p&gt;HintStor在VFS提供了一个FS_HINT调用，用于获取文件的数据布局信息，获取文件系统元数据，该接口供用户空间和应用进行调用，同时支持查询多个文件的映射。&lt;/p&gt;
&lt;p&gt;该功能提供在ext2、ext4和btrfs等Linux中主流的文件系统中&lt;/p&gt;
&lt;p&gt;FS_HINT会维护ext4预分配的block group，btrfs可以将一个块设备挂载为sub-Volume，用于跨设备管理数据&lt;/p&gt;
&lt;p&gt;通过ioctl接口来获取文件的边界，来触发数据迁移或者I/O调度&lt;/p&gt;
&lt;h2 id=&#34;block-storage-data-manager&#34;&gt;Block Storage Data Manager&lt;/h2&gt;
&lt;p&gt;主要包含：&lt;/p&gt;
&lt;p&gt;chunk的映射表：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维护LBA到Physical Block Address的映射&lt;/li&gt;
&lt;li&gt;chunk size 可自定义&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;chunk-level I/O 分析器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于监控每个chunk上的I/O访问统计信息&lt;/li&gt;
&lt;li&gt;估算每个chunk的访问频率，生成热力图，该信息用于周期性的进行数据迁移&lt;/li&gt;
&lt;li&gt;虽然这些信息可能不准确，仍然可以用来作为数据移动的正确性评估指标&lt;/li&gt;
&lt;li&gt;当chunk被迁移到更低速的层次，访问频率可能会升高，进而将数据转移到更快的层次&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;chunk 调度器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于评估不同任务的优先级进行调度&lt;/li&gt;
&lt;li&gt;目前支持一个工作队列和一个等待队列，默认使用FIFO机制来维护原有的请求顺序&lt;/li&gt;
&lt;li&gt;当任务优先级相关的Hint发起时，会将低优先级的请求放到等待队列尾部&lt;/li&gt;
&lt;li&gt;应用可以设置一个定时器来调用一个请求，用于将接近deadline的请求从等待队列移动到工作队列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;access hinit原子操作的分类模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HintStor将数据的布局和移动视作改善异构存储系统的基本功能&lt;/li&gt;
&lt;li&gt;每一个access hint指令是一个四元组&lt;code&gt;(op,chunk_id,src_addr,dest_addr)&lt;/code&gt;，包括操作类型、chunk ID、在逻辑卷中的源地址和目的地址&lt;/li&gt;
&lt;li&gt;共有四个原子操作：
&lt;ul&gt;
&lt;li&gt;Redirect：当最初请求的原地址被重定向到其他地址时触发，例如在一个由多个SSD和HDD组成的逻辑卷中，可以将存储到HDD上的较小的文件重定向到SSD中。该处理函数会调用DM中的驱动并管理被重定向的请求，当bio到达DM时，将对data chunk重新赋值为目的地址&lt;/li&gt;
&lt;li&gt;Migrate：作为HintStor中关键的部分，在后台会保留一个migration daemon后台驻留程序，起初当数据块存放于不同的存储设备时，数据的访问频率会时不时发生变化，Migration操作通过调用DM中的migrator target driver，将data chunk从原有的位置转移到目的位置。为了保证一致性，迁移过程中该chunk将加锁进行保护，忽略对chunk的访问。HintStor提供主动和策略两种迁移方式，主动即用户主动触发，同时实现了基于时间的启发式迁移策略，可以配置为每两个小时迁移top-k访问最频繁的chunk到更快的存储设备中&lt;/li&gt;
&lt;li&gt;Replicate：用于将数据的副本进行保留，使用DM中的migrator target driver，保留原来数据的指针，使得热点数据分布在多个存储设备上，提高数据访问的平均时间，同时可以利用Replicate创建数据的多个副本，需要加锁&lt;/li&gt;
&lt;li&gt;Prefetch：类似于Buffering， HintStor预留一个buffer space空间用于预取，将data chunk加载到buffer space中，实现类似于Migrate和Replicate，主要区别是Prefetch在拷贝时不需要加锁进行保护&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;classification&#34;&gt;Classification&lt;/h2&gt;
&lt;p&gt;I/O access hints可以分为两种，分别为从操作系统动态地进行捕获和从文件系统静态地获取&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221209005726034.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221209005726034&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;动态的access hints主要帮助块存储设备在运行时管理数据的布局，对于冷数据，便于文件系统进行预取。&lt;/p&gt;
&lt;p&gt;HintStor在chunk级别处理I/O请求，并且绑定一些额外的信息，例如文件结构和用户的提示，任务调度则在进程level实现&lt;/p&gt;
&lt;h1 id=&#34;conclusion--thinking&#34;&gt;Conclusion &amp;amp;&amp;amp; Thinking&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;对比Flash Share，HintStor主要针对Linux I/O栈和文件系统层面，Flash Share 还考虑到了NVMe协议方面和SSD层面&lt;/li&gt;
&lt;li&gt;在处理前台/后台I/O部分二者比较类似&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[ATC&#39;20] PinK: High-speed In-storage Key-value Store with Bounded Tails</title>
        <link>https://blog.ipandai.club/p/atc20-pink-high-speed-in-storage-key-value-store-with-bounded-tails/</link>
        <pubDate>Sat, 26 Nov 2022 13:40:59 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/atc20-pink-high-speed-in-storage-key-value-store-with-bounded-tails/</guid>
        <description>&lt;h1 id=&#34;0x00-everyday-english&#34;&gt;0x00 Everyday English&lt;/h1&gt;
&lt;p&gt;preferable 更可取的&lt;/p&gt;
&lt;p&gt;consequently 因此&lt;/p&gt;
&lt;p&gt;inevitably 必然的&lt;/p&gt;
&lt;p&gt;conventional 常规的、传统的&lt;/p&gt;
&lt;p&gt;exacerbates 加剧&lt;/p&gt;
&lt;p&gt;deteriorates 恶化&lt;/p&gt;
&lt;p&gt;deterministic 确定性&lt;/p&gt;
&lt;h1 id=&#34;0x01-intro&#34;&gt;0x01 Intro&lt;/h1&gt;
&lt;p&gt;本文主要提出了Pink，基于LSM tree的KV-SSD，主要通过避免使用布隆过滤器，而是使用少量DRAM来进行优化&lt;/p&gt;
&lt;p&gt;传统的KV-SSD主要通过在控制器的DRAM中维护一个hash table来寻址，受到DRAM的容量限制，超出容量的部分会暂时存放在闪存中，因此在寻址时带来了访问闪存的额外开销，同时当发生哈希碰撞时，可能会操作多个闪存，造成了大量不可预测的开销&lt;/p&gt;
&lt;p&gt;单纯使用LSM做为替换的话，可以减少DRAM的存储，但是存在一些性能问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尾延迟：很多LSM-tree使用布隆过滤器来快速检索，可靠性差，存在尾延迟问题&lt;/li&gt;
&lt;li&gt;写放大：由于LSM对KV索引的排序和合并操作，带来了很多对闪存额外的访问，还增大了FTL进行GC的负担&lt;/li&gt;
&lt;li&gt;布隆过滤器的重建和KV排序：消耗了大量的CPU资源，带来I/O性能的大幅下降&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文提出了一种基于LSM-tree的KV存储引擎-PinK，解决了上述三个问题。PinK使用了四种技术。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;PinK的核心是level pinning，将固定在LSM-tree最顶层的KV索引固定到DRAM中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由于部分level固定在DRAM中，因此可以直接进行compaction，无需占用闪存的I/O，被固定的索引通过电容进行保护，因此也不需要定期进行刷新，这里要注意作者使用DRAM容量很小，使用电容足以保证持久化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GC的主要I/O开销来自对LSM-tree索引的更新，Pink通过延迟LSM-tree中索引更新到compaction阶段来减少GC的I/O开销&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在SSD控制器和NAND芯片之间添加比较器，PinK在读取KV对象时执行KV排序，完全消除了compaction的CPU成本&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x02-bg&#34;&gt;0x02 Bg&lt;/h1&gt;
&lt;p&gt;了解一些有关NAND闪存SSD、KV-SSD、Hash-based KV-SSD、LSM-Tree-based KV-SSD的现状，并对Hash和LSM-tree进行比较&lt;/p&gt;
&lt;p&gt;作者对LSM-tree的设计，L0位于DRAM中，并作为write buffer，其余各层存储在闪存中&lt;/p&gt;
&lt;h1 id=&#34;0x03-design&#34;&gt;0x03 Design&lt;/h1&gt;
&lt;p&gt;Pink有四个主要的数据结构&lt;/p&gt;
&lt;p&gt;位于DRAM中的level lists和Skiplist&lt;/p&gt;
&lt;p&gt;位于闪存中的meta segments和data segments&lt;/p&gt;
&lt;p&gt;在LSM-tree KV 文件系统方面，作者主要参考了&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast16/technical-sessions/presentation/lu&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;WiscKey FAST&#39;16&lt;/a&gt;，取代FTL进行GC、索引和磨损均衡&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221128145842322.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221128145842322&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;skiplist&#34;&gt;Skiplist&lt;/h2&gt;
&lt;p&gt;Skiplist作为LSM-tree中的L0，起到类似write buffer的作用，暂时保存KV对象，Skiplist中的对象为：&lt;code&gt;&amp;lt;key size, key, value size, value&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;当Skiplist满时，会将缓存的对象以meta segment和data segment的形式刷新到L1中，在L1和更低的level中，Key和Value将被分离存储到meta/data segment中，data segment中存储value、key和key size用于GC&lt;/p&gt;
&lt;h2 id=&#34;level-list&#34;&gt;Level list&lt;/h2&gt;
&lt;p&gt;用于跟踪闪存中每个level的meta segment&lt;/p&gt;
&lt;p&gt;每个level list是一个保存固定大小的指针对的数组，每个指针4B，第一个指针指向meta segment在闪存中的物理地址，第二个指针指向meta segment的start key&lt;/p&gt;
&lt;p&gt;meta segment的start key单独存储在DRAM中，可以支持动态大小的key，同时可以实现在level list查找meta segment时进行二分查找&lt;/p&gt;
&lt;p&gt;如果LSM-tree有5个Level，即h=5，除了L0之外，每个level都会有一个level list&lt;/p&gt;
&lt;p&gt;L0和Level list通过电容来保护，Pink不需要使用日志来维护原子性和数据的持久性&lt;/p&gt;
&lt;h2 id=&#34;level-pinning&#34;&gt;Level Pinning&lt;/h2&gt;
&lt;p&gt;消除尾延迟，将$top-k\ \  level(k&amp;lt;=h-1)$ 的meta segment保存在DRAM中，减少了read过程的尾部延迟&lt;/p&gt;
&lt;p&gt;当处理GET指令时，首先在DRAM中对Key进行查找，当DRAM中没有命中时，将对剩余的几个level在闪存中进行查找，在使用布隆过滤器时，最坏的查询时间复杂度为$O(h-1)$，该策略下可以优化为$O(h-k-1)$&lt;/p&gt;
&lt;p&gt;因此LSM-tree各level按照参数T进行增长，因此topk策略下空间占用不会过大&lt;/p&gt;
&lt;p&gt;同时可以直接在DRAM中进行compaction操作，无需占用闪存I/O，因为有电容的存在，无需定期刷新脏segment到闪存中&lt;/p&gt;
&lt;h2 id=&#34;optimizing-search-path&#34;&gt;Optimizing Search Path&lt;/h2&gt;
&lt;p&gt;在对Key进行查询时，LSM-tree将对各个Level进行二分查找&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221129190239823.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221129190239823&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;朴素的LSM-tree最坏的查询时间复杂度为$O(h^2\cdot log(T))$，使用布隆过滤器的情况下可以更低&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;PinK通过prefix来减少字符串比较的开销，并且先比较key的前4个字节，如果匹配成功才会比较后面的部分&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在level list上每个记录引入一个4字节的范围指针，实现多级cascading范围查找&lt;/p&gt;
&lt;p&gt;指针指向下一级有最大的start key同时key 小于等于上一级中的记录，在一级一级的传递中不断减少搜索的范围，如上图b所示，平均时间复杂度为$O(h\cdot log(T))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;speeding-up-compaction&#34;&gt;Speeding up Compaction&lt;/h2&gt;
&lt;p&gt;在compaction阶段，引入一个硬件加速器，位于闪存和数据主总线之间，可以方便的合并两个闪存中的level&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221129225719057.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221129225719057&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;工作流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;PinK向加速器发起compaction请求，请求参数为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两个level的meta segment的闪存地址$(L_i,L_{i+1})$&lt;/li&gt;
&lt;li&gt;回写的闪存地址$L_{i+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向多个闪存发起读取请求，同时还要对不同channel返回的数据进行重新排序，序列化meta segment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上图中灰色的比较器，会不断的获取两个level中的数据，输出key较小的那一个&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于无效的key，将通知PinK进行GC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输出的排序后结果将保存在write buffer中进行回写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;操作结束后，加速器通知PinK当前$L_{i+1}$使用的闪存页的数量，便于和此前提供的进行比较，用于GC&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PinK在DRAM的pinned level中也使用了类似的加速器，基于DMA&lt;/p&gt;
&lt;h2 id=&#34;optimizing-gc&#34;&gt;Optimizing GC&lt;/h2&gt;
&lt;p&gt;需要针对meta/data segment分别制定GC策略&lt;/p&gt;
&lt;h2 id=&#34;durability--scalability&#34;&gt;Durability &amp;amp;&amp;amp; Scalability&lt;/h2&gt;
&lt;p&gt;使用电容来保护整个DRAM，对于比较低端的SSD，没有足够的电容来维持，只能继续使用定期写回到闪存的机制（要保护level lists，pinned level，L0），同时还要记录日志，由于LSM-tree的特性，该操作比基于hash的KV-SSD开销略小&lt;/p&gt;
&lt;h1 id=&#34;conclusion--thinking&#34;&gt;Conclusion &amp;amp;&amp;amp; Thinking&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;与之前阅读的KV-SSD相比，PinK将部分Level固定到DRAM中，一定程度上降低了延迟&lt;/li&gt;
&lt;li&gt;PinK弃用了布隆过滤器，使用一种层级缩小查找范围的方式，优化查询效率&lt;/li&gt;
&lt;li&gt;引入了一个加速器来减少LSM-tree compaction操作的开销&lt;/li&gt;
&lt;li&gt;论文都提高了布隆过滤器使用时重建的开销略高，是否可以考虑使用布谷鸟过滤器，这部分主要着眼于LSM-tree的优化，可能并不是真正的性能瓶颈&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[FAST&#39;18] FEMU闪存模拟系统介绍</title>
        <link>https://blog.ipandai.club/p/fast18-femu%E9%97%AA%E5%AD%98%E6%A8%A1%E6%8B%9F%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/</link>
        <pubDate>Wed, 23 Nov 2022 19:44:51 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/fast18-femu%E9%97%AA%E5%AD%98%E6%A8%A1%E6%8B%9F%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/</guid>
        <description>&lt;h1 id=&#34;0x00-everyday-english&#34;&gt;0x00 Everyday English&lt;/h1&gt;
&lt;p&gt;LOC == line of code&lt;/p&gt;
&lt;p&gt;Guest OS == VM&lt;/p&gt;
&lt;p&gt;intricacies 错综复杂&lt;/p&gt;
&lt;p&gt;drop-in replacement 通常表示直接替换，并且替换后不会产生影响，甚至会有一些新的功能&lt;/p&gt;
&lt;p&gt;leverage 对&amp;hellip;产生影响；一般也可以理解为 充分利用，比utilize语气强一些&lt;/p&gt;
&lt;h1 id=&#34;0x01-intro&#34;&gt;0x01 Intro&lt;/h1&gt;
&lt;p&gt;FEMU是用于软硬件全栈的SSD研究模拟平台，基于QEMU开发，相比于OpenChannel SSD实现了较高的准确率&lt;/p&gt;
&lt;p&gt;此前的SSD研究模拟工具大多不开源，可扩展性差，研究成本高，并且已经过时。&lt;/p&gt;
&lt;p&gt;FEMU首先开源免费，并且有较高的准确度，同时可伸缩性强，最大支持32 I/O 线程并模拟32个并行的SSD channels/chips，可扩展性高，得益于基于QEMU的优点，FEMU可以支持针对于SSD的研究，针对os 内核的研究，或者基于os内核和SSD的研究。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/vtess/FEMU&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;FEMU GitHub 仓库地址： https://github.com/vtess/FEMU&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x02-femu&#34;&gt;0x02 FEMU&lt;/h1&gt;
&lt;p&gt;FEMU本身仅有3929行代码，基于QEMU v2.9，FEMU近些年的更新也主要为修复一些bug、合并QEMU的版本，将传统的App+宿主机+SSD的研究架构转变为了App+虚拟机+FEMU。&lt;/p&gt;
&lt;p&gt;FEMU中的FTL模块、GC、I/O调度主要基于VSSIM&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/6558443&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;VSSIM: Virtual machine based SSD simulator | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;可伸缩性&#34;&gt;可伸缩性&lt;/h2&gt;
&lt;p&gt;对于当前的高并行化的SSD，可伸缩性对模拟闪存至关重要。通过virtio和dataplane接口进行模拟，不能达到足够的可伸缩性，同时存在较高的延迟。&lt;/p&gt;
&lt;p&gt;在QEMU中存在的两个主要问题是&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;QEMU使用传统的trap-and-emulate方法进行I/O模拟，虚拟机的NVMe驱动通过doorbell寄存器告知QEMU模拟的I/O设备。该doorbell将引起开销较大的VM-exit，同时在I/O完成阶段，也会产生该调用。&lt;/li&gt;
&lt;li&gt;QEMU使用异步I/O来实现read/write，AIO的执行需要避免QEMU的I/O被阻塞，当在存储后端是基于RAM的镜像时，AIO产生的负载尤为明显&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;解决方案为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用基于轮询的方案，并且禁用了虚拟机的doorbell写操作，通过一个线程来轮询存储设备队列的状态，避免了VM-exit&lt;/li&gt;
&lt;li&gt;不使用虚拟的镜像文件，使用自定义的以RAM为后端的存储模拟，定义在QEMU的堆空间中，将QEMU中DMA修改为从QEMU heap中读写数据，这个改变对VM来说是透明的&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;准确度&#34;&gt;准确度&lt;/h2&gt;
&lt;h3 id=&#34;延迟模拟&#34;&gt;延迟模拟&lt;/h3&gt;
&lt;p&gt;当I/O请求到达后，FEMU发起DMA R/W，并用模拟的完成时间$(T_{endio})$对I/O请求进行标记，添加到endio queue中，队列按照I/O的完成时间进行排序。一旦I/O请求的预估完成时间大于当前时间，会由专门的end I/O 处理线程负责将其通过中断发送给虚拟机。&lt;/p&gt;
&lt;p&gt;FEMU对每个I/O请求都设置了&lt;code&gt;+50us&lt;/code&gt;的延迟。&lt;/p&gt;
&lt;h3 id=&#34;延迟模型&#34;&gt;延迟模型&lt;/h3&gt;
&lt;p&gt;对于$T_{endio}$的计算：&lt;/p&gt;
&lt;p&gt;标记每个plane和channel的下一次空闲时间$T_{free}$，&lt;/p&gt;
&lt;p&gt;例如，一个page写入需要channel #1 和plane #2来执行，那么channel的下一次空闲时间将为$T_{freeOfChannel1}=T_{now}+T_{transfer}$，其中的$T_{transfer}$为一个可配置的page在channel中的传输时间，plane的下一次空闲时间则为$T_{freeOfPlane2}+=T_{write}$，其中$T_{write}$是可配置的NAND page写入时间。&lt;/p&gt;
&lt;p&gt;因此该写入操作的$T_{endio}=T_{freeOfPlane2}$。&lt;/p&gt;
&lt;p&gt;在一个写入过程正在执行时，对于新到达的page read，将令$T_{freeOfPlane2} += T_{read}$，其中$T_{read}$为可配置的参数NAND page读取时间，并且$T_{freeOf Channel1} += T_{transfer}$，&lt;/p&gt;
&lt;p&gt;因此该page write的$T_{endio}=T_{freeOf Channel1}$。&lt;/p&gt;
&lt;p&gt;由于普通的SSD plane只有一个寄存器，单个plane不能实现I/O并行，所以该模型可以满足实际要求&lt;/p&gt;
&lt;p&gt;该模型可以通过添加一个$T_{erase}$来模拟GC的延迟&lt;/p&gt;
&lt;h3 id=&#34;openchannel延迟模型&#34;&gt;OpenChannel延迟模型&lt;/h3&gt;
&lt;p&gt;OpenChannel SSD，OC使用双寄存器的plane，data register和cache register，因此在一个plane中对一个NAND page的read/write可以并行，因此效率高，下图直观的展示了二者的区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221125195232028.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221125195232028&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;OC有着不统一的延迟模型，映射到MLC颗粒cell高位bits的page有着更高的延迟，实际上一个NAND中的512个page以一种特别的方式进行组织，&lt;code&gt;LLLLLLuLLuLLuuLLuu...(repeate of LLuu)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;FEMU对OC的实现方式进行了整合，实现了较好的效果&lt;/p&gt;
&lt;h2 id=&#34;其他功能&#34;&gt;其他功能&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FTL和GC：默认使用动态映射FTL，channel阻塞的GC。同时还支持channel、controller、plane层面进行阻塞的GC策略&lt;/li&gt;
&lt;li&gt;白盒模式/黑盒模式&lt;/li&gt;
&lt;li&gt;多设备支持：支持虚拟机连接多个SSD，每个SSD包含独立的NVMe实例和FTL&lt;/li&gt;
&lt;li&gt;扩展NVMe指令&lt;/li&gt;
&lt;li&gt;Page-Level延迟可自定义：模拟闪存芯片的良品率带来的延迟不统一性&lt;/li&gt;
&lt;li&gt;分布式SSD：评估类似Hadoop的应用&lt;/li&gt;
&lt;li&gt;Page-Level故障注入：进行闪存可靠性研究&lt;/li&gt;
&lt;li&gt;一些限制：FEMU基于DRAM，不能模拟容量太大的SSD，必须手动模拟SSD崩溃的情景，因为DRAM重启将会清空所有数据&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[OSDI&#39;21] Modernizing File System through In-Storage Indexing</title>
        <link>https://blog.ipandai.club/p/osdi21-modernizing-file-system-through-in-storage-indexing/</link>
        <pubDate>Sat, 29 Oct 2022 16:48:32 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi21-modernizing-file-system-through-in-storage-indexing/</guid>
        <description>&lt;h1 id=&#34;questions&#34;&gt;Questions&lt;/h1&gt;
&lt;p&gt;核心思想部分需要再思考和梳理下，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么采用 KV SSD 替代传统 block SSD 能够解决上述问题？&lt;/li&gt;
&lt;li&gt;其中，利用了 KV SSD 的什么特性？&lt;/li&gt;
&lt;li&gt;对文件系统哪些地方做了什么以优化？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;论文试图解决的问题：&lt;/p&gt;
&lt;p&gt;随着存储设备的不断发展，存储设备的性能越来越高，但当前操作系统内核的文件系统在一些操作上并不能够充分利用如今存储设备的性能。&lt;/p&gt;
&lt;p&gt;文件系统在执行数据写入时，需要执行大量操作维护元数据、进行硬盘的空间管理、维护文件系统的一致性，工作量大。&lt;/p&gt;
&lt;p&gt;核心思想：&lt;/p&gt;
&lt;p&gt;使用Key-Value存储接口取代传统的快设备接口。&lt;/p&gt;
&lt;p&gt;具体实现：&lt;/p&gt;
&lt;p&gt;提出Kevin，分为Kevin=KevinFS + KevinSSD&lt;/p&gt;
&lt;p&gt;KevinFS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维护文件和目录到KV对象的映射关系&lt;/li&gt;
&lt;li&gt;将POSIX系统调用转译成KV操作指令&lt;/li&gt;
&lt;li&gt;保证KV-SSD的一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KevinSSD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在存储设备中索引KV对象&lt;/li&gt;
&lt;li&gt;对多个KV对象提供事务操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;Kevin避免了大量元数据的维护带来的I/O放大&lt;/p&gt;
&lt;p&gt;不需要日志即可完成崩溃一致性的维护&lt;/p&gt;
&lt;p&gt;可以抵御文件分段后造成的性能下降&lt;/p&gt;
&lt;p&gt;存储的文件块通过LSM进行分部排序和索引&lt;/p&gt;
&lt;h1 id=&#34;0x01-bg--related-work&#34;&gt;0x01 BG &amp;amp;&amp;amp; Related Work&lt;/h1&gt;
&lt;h2 id=&#34;传统块设备&#34;&gt;传统块设备&lt;/h2&gt;
&lt;p&gt;提供块粒度（512B or 4KB）的访问&lt;/p&gt;
&lt;p&gt;HDD通过维护一个间接表来处理坏块&lt;/p&gt;
&lt;p&gt;基于闪存的SSD通过FTL来维护逻辑块到物理地址的映射索引表，以便在异地更新的NAND上模拟可重写介质并且排除坏块。&lt;/p&gt;
&lt;p&gt;现有研究缓解了I/O调度、文件碎片化、日志相关问题，没能消除元数据的修改带来的I/O流量&lt;/p&gt;
&lt;p&gt;DevFS实现了在存储设备内部的文件系统，直接将接口暴露给应用，调用时不用发生Trap，对于元数据的维护都在存储设备端执行，移除了I/O 栈减少了通信接口的开销。缺点是需要大量的DRAM和多核心的CPU、能提供的功能有限，还限制了快照、重复数据删除等文件系统高级功能的实现。&lt;/p&gt;
&lt;p&gt;KV存储可以高效处理元数据和小文件的写入&lt;/p&gt;
&lt;h2 id=&#34;lsm-tree&#34;&gt;LSM-Tree&lt;/h2&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20221102145019013.png&#34; alt=&#34;image-20221102145019013&#34; style=&#34;zoom:70%;&#34; /&gt;
&lt;p&gt;LSM Tree有多级，包括$L_1,L_2,&amp;hellip;,L_{h-1},L_{h}$，h是树的高度，对于各级的大小有如下关系，$Size(L_{i+1})&amp;gt;=T*Size(L_i)$&lt;/p&gt;
&lt;p&gt;每层都有按Key排序的唯一KV对象。各层之间的Key范围可能会出现重叠。&lt;/p&gt;
&lt;p&gt;KV对象首先写入DRAM中的Memtable，当Memtable不为空时，缓存的KV对象将刷新到L1中进行持久化，当L1不为空时，KV对象刷新到L2中，依此类推。在刷新过程中会执行压缩操作来对两层之间的KV对象进行合并和排序，在排序后写回磁盘中。&lt;/p&gt;
&lt;p&gt;为了改善压缩操作的I/O开销，可以将所有的value存储在value log中，在LSM tree中只保存value指针，存储&lt;code&gt;&amp;lt;key, value pointer&amp;gt;&lt;/code&gt;形式。对于失效的value要考虑垃圾回收问题。&lt;/p&gt;
&lt;p&gt;在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。&lt;/p&gt;
&lt;h1 id=&#34;0x02-architecture&#34;&gt;0x02 Architecture&lt;/h1&gt;
&lt;p&gt;KEVIN分为两个部分&lt;/p&gt;
&lt;p&gt;KEVINFS：维护文件、目录到KV对象映射的文件系统&lt;/p&gt;
&lt;p&gt;KEVINSSD：索引KV对象到闪存的LSM-tree&lt;/p&gt;
&lt;h2 id=&#34;kv-command&#34;&gt;KV Command&lt;/h2&gt;
&lt;p&gt;支持多种KV指令，同时支持事务&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221102154423353.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102154423353&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者将Key限制为256B，value大小没有限制&lt;/p&gt;
&lt;h2 id=&#34;文件和目录的映射&#34;&gt;文件和目录的映射&lt;/h2&gt;
&lt;p&gt;KevinFS只使用三个类型的KV对象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$superblock$：保存文件系统的信息，大小128B&lt;/li&gt;
&lt;li&gt;$meta$：保存文件、目录的元数据，大小256B&lt;/li&gt;
&lt;li&gt;$data$：保存文件数据，unlimited，不超过文件大小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于目录的遍历通过ITERATE&lt;/p&gt;
&lt;p&gt;Key的命名遵循如下规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;meta对象的key组成：
&lt;ul&gt;
&lt;li&gt;前缀&lt;code&gt;m:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;父目录的inode number&lt;/li&gt;
&lt;li&gt;分隔符&lt;code&gt;:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;文件/目录名&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data对象的key组成：
&lt;ul&gt;
&lt;li&gt;前缀&lt;code&gt;d:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;文件的inode number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key命名规范参考了其他论文（Kai Ren and Garth Gibson. TABLEFS: Enhancing Metadata Efficiency in the Local File System. In Proceedings of the USENIX Annual Technical Conference, pages 145–156, 2013.），本篇文章将其扩展到了存储设备层面&lt;/p&gt;
&lt;p&gt;KevinFS没有实现dentry，如果需要访问一个目录内的所有项目，可以通过&lt;code&gt;ITERATE(m:50:, 2)&lt;/code&gt;，来获取两个父目录inode为50（前缀匹配给定的pattern）的子目录/文件的元数据。&lt;/p&gt;
&lt;p&gt;一个优化：为了防止ITERATE消耗太长时间，建议指定cnt&lt;/p&gt;
&lt;p&gt;一个优化：为了高效处理小文件，KevinFS将总体小于4KB的小文件的元数据和数据内容打包，I/O直接操作meta对象GET/SET来进行读写&lt;/p&gt;
&lt;p&gt;@TODO 一个优化：使用全路径索引来取代基于inode的索引，这提高了基于排序算法的KV存储的扫描性能，主要对seek time开销大的HDD优化比较明显&lt;/p&gt;
&lt;h2 id=&#34;kv对象索引&#34;&gt;KV对象索引&lt;/h2&gt;
&lt;p&gt;KevinSSD基本实现了传统SSD 中FTL的所有功能，将KV对象映射到闪存、分配和释放闪存的空间。&lt;/p&gt;
&lt;p&gt;FTL只做坏块管理和磨损均衡等简单工作。&lt;/p&gt;
&lt;p&gt;每个Level维护一个内存表，来记录Flash中的KV对象。表的每个记录都有&lt;code&gt;&amp;lt;start key, end key, pointer&amp;gt;&lt;/code&gt;，其中指针指向保存KV对象的闪存页面的位置，start key和end key是页面中key的范围。key的范围可以在多个Level上重叠。为了快速查找，所有记录都按开始键排序。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221102165350981.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102165350981&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;meta和data对象的Key和Value会被分离存储。通过&lt;code&gt;&amp;lt;key, value pointer&amp;gt;&lt;/code&gt;的方式进行存储，会有专门的flash page来分别存储这些信息（meta和data对象使用不同的flash page），key-index pages&lt;/p&gt;
&lt;h3 id=&#34;meta-object&#34;&gt;Meta object&lt;/h3&gt;
&lt;p&gt;在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。&lt;/p&gt;
&lt;p&gt;对目录项的更新只需要修改其元数据即可完成，而不需要修改4KB的块&lt;/p&gt;
&lt;h3 id=&#34;data-object&#34;&gt;Data object&lt;/h3&gt;
&lt;p&gt;为了避免对大数据的小范围更新带来的I/O高负载，KevinSSD将data object分割为4KB大小的子对象，使用单独的后缀来对他们进行表示，后缀从0开始，拼接在原始的data object Key后，因此data object的key最终构成为
$$
d:{inode\ \ \ number}:{subobject \ \ number,start\ with \ 0}
$$
不需要中间表进行索引，只需要通过偏移量即可定位要修改的对象&lt;/p&gt;
&lt;p&gt;Data object同样使用保存指针的方式将映射存储在key-index pages中，key-index pages会对键进行排序，因此属于同一个文件的subobject一般会在同一个flash page中。&lt;/p&gt;
&lt;p&gt;==优化==：在查找指针时，会将一整个闪存页内的数据全部取出，存储在控制器的DRAM中，减轻后续查找可能带来的负载&lt;/p&gt;
&lt;h2 id=&#34;缓解索引负载&#34;&gt;缓解索引负载&lt;/h2&gt;
&lt;p&gt;使用LSM-tree由于多级查找带来额外的I/O，传统FTL的映射都保存在DRAM，因此没有额外开销。作者介绍了三种主要原因和解决方案。&lt;/p&gt;
&lt;h3 id=&#34;压缩操作的开销&#34;&gt;压缩操作的开销&lt;/h3&gt;
&lt;p&gt;同上文介绍的方案，通过分离Value可以显著缓解I/O的数据传输负载，对data进行分片我个人感觉反而会加大压缩操作的合并过程，即使分片后键值都是有序的。&lt;/p&gt;
&lt;h3 id=&#34;层级查找的开销&#34;&gt;层级查找的开销&lt;/h3&gt;
&lt;p&gt;由于LSM-tree的特性，需要逐层进行查找，为了防止大量的顺序查找，作者使用布隆过滤器进行了优化。&lt;/p&gt;
&lt;p&gt;同时还缓存了热点K2V索引数据（与目标索引处在同一个flash page中的）&lt;/p&gt;
&lt;p&gt;为了利用大容量SSD中提供的超大DRAM，作者采用压缩存储K2V索引，并在其中插入没有压缩的K2V索引来作为二分查找的参照物。（快速在缓存中查找？不如继续用布隆过滤器）&lt;/p&gt;
&lt;h3 id=&#34;分散的对象带来的开销&#34;&gt;分散的对象带来的开销&lt;/h3&gt;
&lt;p&gt;LSM-tree允许各层之间的key范围重叠，因此有同一个前缀的目录或者文件可能被分配到不同的Level中，对于获取一个目录中所有目录、文件的操作，需要对多个闪存页进行访问。这个问题作者没有给出一个明确的方案（在压缩合并时隐式解决），但是提供了一个用户工具来主动触发合并，效率较高。&lt;/p&gt;
&lt;p&gt;作者对比了各种方法在随机读、局部读（？）、顺序读情况下对闪存页的读取次数，使用布隆过滤器的时候稳定会有一次读取&lt;/p&gt;
&lt;p&gt;使用了KevinSSD后对于SSD而言，I/O延迟有轻微的增加，整体而言效率提高了&lt;/p&gt;
&lt;h1 id=&#34;0x03-implement-vfs&#34;&gt;0x03 Implement VFS&lt;/h1&gt;
&lt;h2 id=&#34;write&#34;&gt;write&lt;/h2&gt;
&lt;p&gt;所有的写相关系统调用可以通过SET和DELETE来实现。&lt;/p&gt;
&lt;p&gt;例如unlink，只需要两次DELETE指令，移除meta和data object。&lt;/p&gt;
&lt;p&gt;SET时现在Memtable中保存一个KV对象，然后持久化到flash中，若Key已存在则丢弃旧的对象&lt;/p&gt;
&lt;p&gt;DELETE时在树上写入一个4B大小的墓碑&lt;/p&gt;
&lt;p&gt;失效的对象（被SET覆盖）和删除的对象在压缩期间被永久删除&lt;/p&gt;
&lt;h2 id=&#34;read&#34;&gt;read&lt;/h2&gt;
&lt;p&gt;通过GET和ITERATE实现&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;p&gt;执行open系统调用时，查询meta object获取文件的inode，通过GET命令实现&lt;/p&gt;
&lt;p&gt;执行lookup系统调用时，给定一个完整的路径&lt;code&gt;/home/alice/&lt;/code&gt;，从根目录开始获取多个meta object，最终获取目标文件的inode&lt;/p&gt;
&lt;p&gt;执行read系统调用时，将GET一个data object&lt;/p&gt;
&lt;p&gt;执行readdir系统调用时，使用ITERATE，批量获取一组meta objects来获取inode&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221102210344432.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102210344432&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;相较于Ext4，KevinFS可以更好的防止文件碎片化带来的影响。&lt;/p&gt;
&lt;h1 id=&#34;0x04-crash-consistency&#34;&gt;0x04 Crash Consistency&lt;/h1&gt;
&lt;h2 id=&#34;一致性维护&#34;&gt;一致性维护&lt;/h2&gt;
&lt;p&gt;通过事务即可实现原子化的操作&lt;/p&gt;
&lt;p&gt;在传统的日志结构中，由于事务的大小受日志大小的限制，一个系统调用可能会被划分到多个事务中。KVFS通过强制限制对一个文件的操作驻留在同一个事务中来确保原子性。&lt;/p&gt;
&lt;p&gt;事务的隔离？对==fsync==指令单独创建一个小型事务来提高效率。&lt;/p&gt;
&lt;p&gt;KevinFS基于KV事务的特性来维护一致性，不使用日志系统。&lt;/p&gt;
&lt;h2 id=&#34;kv事务的实现&#34;&gt;KV事务的实现&lt;/h2&gt;
&lt;p&gt;使用了三个数据结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务表$TxTable$：记录事务基本信息&lt;/li&gt;
&lt;li&gt;事务日志$TxLogs$：维护事务对象的K2V索引，存储在DRAM或者闪存中&lt;/li&gt;
&lt;li&gt;恢复日志$TxRecovery$：用于恢复和终止事务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是收到BeginTx指令时，KevinSSD在TxTable中创建一个对象，包含TID，当前事务的状态，与事务相关K2V索引的位置，初始状态为RUNNING（如①）。当后续指令到达后，KevinSSD将KV索引记录在TxLogs，并缓存在Memtable中。&lt;/p&gt;
&lt;p&gt;当收到EndTx指令后，完成事务的提交，标记事务状态为COMMITTED（如②）。将提交的KV索引同步到LSM-tree中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221103102304181.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221103102304181&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;为了减少在COMMITTED阶段，将TxLogs中的KV索引同步到LSM tree L1中时的负载，对每个事务在DRAM中维护了一个Skiplist，便于快速查找KV对象在TxLogs中的索引，在L1和L2需要进行压缩时再将skiplist中的数据同步到checkpoint，当同步完成后，事务的状态将变为CHECKPOINTED（如③），最终即将对DRAM中的TxTable和TxLogs进行回收。&lt;/p&gt;
&lt;h2 id=&#34;recovery&#34;&gt;Recovery&lt;/h2&gt;
&lt;p&gt;若发生掉电，KVSSD会利用电容将KV索引持久化到闪存中的TxLogs，TxTable更新内部的指针，在TxRecovery中进行记录。在系统重启时，KVSSD会扫描Tx Recovery，加载最新的TxTable，对于处于COMMITTED状态的事务，将其TxLogs索引加载到Skiplist中，对于处于RUNNING状态的事务，直接进行Abort并回收其资源。&lt;/p&gt;
&lt;h1 id=&#34;conclusion--thinking&#34;&gt;Conclusion &amp;amp;&amp;amp; Thinking&lt;/h1&gt;
&lt;p&gt;本文提出了一种基于K/V存储架构的文件系统，通过对Key的设计保证了在I/O操作中的高效性。通过本文了解了一些KV存储的trick，对于KV存储的各种场景，无论是在Redis中还是在KV文件系统中，对于Key的设计都至关重要，好的设计往往能够带来极大的效率提升，例如布隆过滤器的应用，有很多经验值得学习。同时了解了对事务的一些控制机制。&lt;/p&gt;
&lt;p&gt;此篇论文让我惊讶于文件系统还能通过KV存储来实现的通知，还让我回想起了TiDB，通过KV存储系统来实现了分布式关系型数据库，并完全兼容MySQL，说明在利用好KV存储的特性的情况下，可以有效的解决问题，要多去学习他们的设计思想。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>[ATC&#39;19] Asynchronous I/O Stack: A Low-latency Kernel I/O Stack for Ultra-Low Latency SSDs</title>
        <link>https://blog.ipandai.club/p/atc19-asynchronous-i/o-stack-a-low-latency-kernel-i/o-stack-for-ultra-low-latency-ssds/</link>
        <pubDate>Wed, 17 Aug 2022 10:30:41 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/atc19-asynchronous-i/o-stack-a-low-latency-kernel-i/o-stack-for-ultra-low-latency-ssds/</guid>
        <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;优化I/O的方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户层面直接调用外部存储设备，需要应用包含文件系统的调用，臃肿，同时不同应用和用户间的冲突问题。&lt;/li&gt;
&lt;li&gt;优化操作系统内核的I/O 栈
&lt;ul&gt;
&lt;li&gt;使用轮询来减少上下文切换的开销&lt;/li&gt;
&lt;li&gt;在底层减少一半的中断处理&lt;/li&gt;
&lt;li&gt;分散I/O指令&lt;/li&gt;
&lt;li&gt;I/O block调度机制&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;减少内核的开销&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少中断处理的后半部分&lt;/li&gt;
&lt;li&gt;是用轮询技术而非中断，减少上下文切换&lt;/li&gt;
&lt;li&gt;混合轮询&lt;/li&gt;
&lt;li&gt;基于SSD的闪存随机读写简化调度策略&lt;/li&gt;
&lt;li&gt;在NVMe固件中进行调度&lt;/li&gt;
&lt;li&gt;对高优先级的任务，提供不同的IO path支持，最小化IO path的开销&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;修改存储接口&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分散/分散 IO合并多个IO到一个指令，减少往返次数&lt;/li&gt;
&lt;li&gt;移除doorbell机制和完成信号&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;改善fsync&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;冲fsync请求发出到收到response，延长数据持久化的时间&lt;/li&gt;
&lt;li&gt;在日志提交记录中使用校验和，有效的重叠日志写入和块写入&lt;/li&gt;
&lt;li&gt;提出&lt;code&gt;写守序系统调用&lt;/code&gt;，重叠的fsync效果相同，当应用需要使用fsync时，关于IO的操作将同步进行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户层直接访问外设，存在隔离、保护等安全问题&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;现状：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I/O 请求过程中太多的步骤&lt;/li&gt;
&lt;li&gt;页面缓存分配和索引&lt;/li&gt;
&lt;li&gt;DMA，一系列数据结构的创建&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前的 ULL SDD实现了低于10微妙的IO延迟，然而操作系统内核产生的延迟没有明显变化&lt;/p&gt;
&lt;p&gt;本文专注于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux内核中的&lt;code&gt;read()&lt;/code&gt;和&lt;code&gt;write()+fsync()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;基于NVMe SSD的Ext4文件系统&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;for-read-path&#34;&gt;For Read Path&lt;/h2&gt;
&lt;p&gt;研究发现许多剩余的操作不必在设备I/O之前或之后执行&lt;/p&gt;
&lt;p&gt;此类操作可以在设备I/O操作进行时执行&lt;/p&gt;
&lt;p&gt;因为此类操作大多独立于设备I/O操作，因此考虑让这些操作与IO重叠&lt;/p&gt;
&lt;h2 id=&#34;for-write-path&#34;&gt;For Write Path&lt;/h2&gt;
&lt;p&gt;缓冲写&lt;code&gt;write()&lt;/code&gt;，并不发起IO请求，不能异步处理&lt;/p&gt;
&lt;p&gt;由于fsync的回写机制和文件系统崩溃一致性（日志系统），包含部分IO请求&lt;/p&gt;
&lt;p&gt;由于文件系统带来的三次IO操作&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据块写&lt;/li&gt;
&lt;li&gt;jbd2发起写入日志Block I/O&lt;/li&gt;
&lt;li&gt;提交Block I/O&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些IO的创建，涉及众多过程（block分配，请求缓冲页，创建和提交bio，设置DMA地址），因此可以让CPU将这些前置操作在IO请求发起前预执行。&lt;/p&gt;
&lt;h2 id=&#34;for-lightweight-block-layer&#34;&gt;For Lightweight Block Layer&lt;/h2&gt;
&lt;p&gt;传统Block Layer涉及过多过程，推迟了IO指令提交给设备的时间&lt;/p&gt;
&lt;p&gt;因为ULL SSD的高速随机IO性能和低速的顺序IO，请求重排的效果很低&lt;/p&gt;
&lt;p&gt;简化block layer，针对异步IO stack进行优化&lt;/p&gt;
&lt;h1 id=&#34;design&#34;&gt;Design&lt;/h1&gt;
&lt;h2 id=&#34;轻量化的block-io-layer&#34;&gt;轻量化的Block I/O Layer&lt;/h2&gt;
&lt;p&gt;LBIO，为LL NVMe SSD而设计，只支持IO submission/completion和IO指令tagging&lt;/p&gt;
&lt;p&gt;只使用&lt;code&gt;lbio&lt;/code&gt;来表示一个block I/O请求，减少了&lt;code&gt;bio-to-request&lt;/code&gt;的时间&lt;/p&gt;
&lt;p&gt;每个lbio包括&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LBA&lt;/li&gt;
&lt;li&gt;I/O 长度&lt;/li&gt;
&lt;li&gt;复制的页面&lt;/li&gt;
&lt;li&gt;页面的DMA地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用全局的lbio二位数组来记录&lt;/p&gt;
&lt;p&gt;行的个数为CPU核心数，行成组被分配到一个NVMe队列&lt;/p&gt;
&lt;p&gt;例如8核心，4NVMe队列，每个队列分配2个核心的lbio
当核心数等于队列数时，可以实现无锁的命令提交&lt;/p&gt;
&lt;p&gt;lbio在全局数组中的索引用作NVMe指令的tag，减少了之前赋tag的过程&lt;/p&gt;
&lt;p&gt;lbio提交后，调用&lt;code&gt;nvme_queue_lbio&lt;/code&gt;来提交I/O指令&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LBIO不会合并和调度IO请求&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;read-path&#34;&gt;Read Path&lt;/h2&gt;
&lt;p&gt;Ext4文件系统中，由extent status tree保存缓存到物理文件block的映射&lt;/p&gt;
&lt;p&gt;预加载映射到内存中，当树太大时，可以只对某个文件预加载&lt;/p&gt;
&lt;h3 id=&#34;异步页面申请和dma分配&#34;&gt;异步页面申请和DMA分配&lt;/h3&gt;
&lt;p&gt;提前分配空闲页池&lt;/p&gt;
&lt;p&gt;为了减少页面DMA的分配，为每个核维护一个DMA映射空闲页（4KB DMA映射页的链表）&lt;/p&gt;
&lt;p&gt;当空闲页池不够用时，将退化为同步进行（origin）&lt;/p&gt;
&lt;h3 id=&#34;缓存页索引&#34;&gt;缓存页索引&lt;/h3&gt;
&lt;p&gt;自旋锁防止并发问题，影响效率&lt;/p&gt;
&lt;p&gt;在请求发出，但是页面还没有更新时，可能重复请求更新页面&lt;/p&gt;
&lt;p&gt;解决方案是不限制request，在request completion阶段解决问题&lt;/p&gt;
&lt;p&gt;尽管多个block请求，但是只能有一个页面被索引&lt;/p&gt;
&lt;p&gt;对于其他页面，标记为abandoned，中断发生之后，如果标记为abandoned，则清除已经完成的页面&lt;/p&gt;
&lt;h3 id=&#34;dma解除映射&#34;&gt;DMA解除映射&lt;/h3&gt;
&lt;p&gt;原本使用中断来处理，改为当系统空闲或等待一个IO请求时处理&lt;/p&gt;
&lt;p&gt;该方式可能会产生漏洞窗口，若不受到恶意访问，不会产生影响，否则用户可以自行选择关闭惰性DMA映射接触&lt;/p&gt;
&lt;h2 id=&#34;write--fsync-path&#34;&gt;Write &amp;amp;&amp;amp; fsync Path&lt;/h2&gt;
&lt;p&gt;当fsync涉及文件系统中事务时，可以将jbd2日志处理重叠处理&lt;/p&gt;
&lt;h1 id=&#34;experiment&#34;&gt;Experiment&lt;/h1&gt;
&lt;p&gt;基于Linux内核5.0.5版本&lt;/p&gt;
&lt;p&gt;使用文件描述符&lt;code&gt;O_AIOS&lt;/code&gt;&lt;/p&gt;
&lt;h1 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h1&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;提出了AIOS&lt;/p&gt;
&lt;p&gt;LBIO层&lt;/p&gt;
&lt;p&gt;AIOS将I/O路径中的同步操作替换为异步操作，以将与读取和fsync相关的计算与设备I/O访问重叠。&lt;/p&gt;
&lt;p&gt;AIOS在Optane SSD上实现了一位数微秒的I/O延迟。&lt;/p&gt;
&lt;p&gt;此外，AIOS通过Z-SSD和Optane SSD上的模拟实验和实际测试显著降低延迟和性能改进。&lt;/p&gt;
&lt;h1 id=&#34;一些启发&#34;&gt;一些启发&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;优化I/O可以从CPU的空闲时间分析，需要考虑内核和文件系统的工作流程，最大程度的利用CPU资源，减少空闲。&lt;/li&gt;
&lt;li&gt;硬件设备在发展的同时，软件应该提供必要适配&lt;/li&gt;
&lt;li&gt;减少内核中与I/O相关的结构类型转化，可以有效节省时间开销&lt;/li&gt;
&lt;li&gt;惰性修改会存在安全问题，在保证安全的情况下，可以提高效率&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>[OSDI&#39;19] Flashshare: Punching Through Server Storage Stack from Kernel to Firmware for Ultra-Low Latency SSDs</title>
        <link>https://blog.ipandai.club/p/osdi19-flashshare-punching-through-server-storage-stack-from-kernel-to-firmware-for-ultra-low-latency-ssds/</link>
        <pubDate>Wed, 17 Aug 2022 10:30:30 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi19-flashshare-punching-through-server-storage-stack-from-kernel-to-firmware-for-ultra-low-latency-ssds/</guid>
        <description>&lt;p&gt;超低延迟固态硬盘从内核到固件的服务器存储堆栈&lt;/p&gt;
&lt;h1 id=&#34;个别名词解释&#34;&gt;个别名词解释&lt;/h1&gt;
&lt;p&gt;the 99^th percentile 超过统计数据99%的数是多少&lt;/p&gt;
&lt;p&gt;blk-mq Linux Multiqueue block layer 内核对ssd随机I/O的优化&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;message signaled interrupt (MSI)&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;1摘要&#34;&gt;1.摘要&lt;/h1&gt;
&lt;p&gt;flash share&lt;/p&gt;
&lt;p&gt;在内核中，扩展了系统堆栈的数据结构，传递应用程序的属性（？），包括内核层到SSD固件。&lt;/p&gt;
&lt;p&gt;对于给定的属性，FlashShare的块层管理IO调度并处理NVMe中断。&lt;/p&gt;
&lt;p&gt;评估结果表明，FLASHSHARE可以将共同运行应用程序的平均周转响应时间分别缩短22%和31%。&lt;/p&gt;
&lt;h1 id=&#34;10-intro&#34;&gt;1.0 Intro&lt;/h1&gt;
&lt;h2 id=&#34;11-现状&#34;&gt;1.1 现状&lt;/h2&gt;
&lt;p&gt;网络服务提供商，满足服务级别协议SLA，延迟敏感&lt;/p&gt;
&lt;p&gt;某个段时间短可能有大量请求涌入，供应商会超额配置机器以满足SLA&lt;/p&gt;
&lt;p&gt;现状：该场景并不常见，因此大部分情况下服务器的资源占用率非常低，能耗比低。&lt;/p&gt;
&lt;p&gt;为了解决利用率低，服务器会运行离线的数据分析应用，延迟不敏感，以吞吐量为导向。&lt;/p&gt;
&lt;p&gt;因此，在运行了多个进程的服务器上，I/O延迟增高，满足SLA非常困难。&lt;/p&gt;
&lt;p&gt;现有的ULL SSD相较于NVMe SSD可以减少10倍的延迟&lt;/p&gt;
&lt;p&gt;但是这些ULL SSD在同时运行多个进程下高强度压榨服务器的时候，不能充分利用ULL SSD的优势/表现一般。&lt;/p&gt;
&lt;p&gt;the 99th percentile 是0.8ms（apache）&lt;/p&gt;
&lt;p&gt;但是当服务器同时运行pagerank的时候，延迟会增加228.5%。&lt;/p&gt;
&lt;p&gt;原因：略&lt;/p&gt;
&lt;p&gt;从固件到内核优化堆栈的存储。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;内核级别的增强：&lt;/p&gt;
&lt;p&gt;两个挑战&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux的blk-mq导致I/O请求队列化，引入延迟&lt;/li&gt;
&lt;li&gt;NVMe的队列机制没有对I/O优先级的策略，因此，来自离线应用的IO请求容易阻塞在线应用的紧急请求，造成延迟。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于latency critical的请求，绕过NVMe的请求队列。同时令NVMe的驱动通过知晓每个应用的延迟临界匹配NVMe的提交和请求队列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固件层设计：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​		即使内核级的优化保证了延迟敏感的请求可以获得高优先级，但如果基础固件不了解延迟临界值，ULL特性（类似内存的性能）无法完全暴露给用户。本文中重新设计了I/O调度和缓存的固件，以直接向用户暴露ULL特性。将ULL SSD的集成缓存进行分区，并根据工作负载的属性对每个I/O服务独立的分配缓存。固件动态的更新分区大小并以精细粒度调整预取I/O粒度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ULL SSD的新中断处理服务：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​	当前的NVMe中断机制没有对ULL I/O服务优化。轮询方法（Linux 4.9.30）消耗了大量的CPU资源去检查I/O服务的完成情况。当轮询在线交互服务的IO请求完成状态时，flashShare使用一个仅对离线应用程序使用消息信号中断的选择性中断服务程序Select-ISR。&lt;/p&gt;
&lt;p&gt;​	通过将NVMe队列和ISR卸载到硬件加速器中来进一步优化NVMe completion routine。&lt;/p&gt;
&lt;p&gt;​	各种仿真实验后效果不错，效率提高了22%和31%。&lt;/p&gt;
&lt;h1 id=&#34;20-background&#34;&gt;2.0 Background&lt;/h1&gt;
&lt;h2 id=&#34;21-存储内核栈&#34;&gt;2.1 存储内核栈&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220810145032258.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220810145032258&#34;
	
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Linux文件系统IO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bio&lt;/li&gt;
&lt;li&gt;request&lt;/li&gt;
&lt;li&gt;nvme_rw_command&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;存储堆栈中，NVMe驱动发起的请求通过nvme_rw_command的形式传递到PCI/PCIe设备驱动中。&lt;/p&gt;
&lt;p&gt;当I/O请求完成后，发送信号中断，中断直接被写入到中断处理器的中断向量中。被中断的核心选择ISR处理该中断请求，随后NVMe驱动再SQ/CQ中清空相应的记录并将结果返回至上一层（比如blk-mq和文件系统）。&lt;/p&gt;
&lt;h2 id=&#34;22-设备固件栈&#34;&gt;2.2 设备固件栈&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220810232230437.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220810232230437&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;收到request&lt;/li&gt;
&lt;li&gt;SQ tail++入队&lt;/li&gt;
&lt;li&gt;写入SQ门铃寄存器&lt;/li&gt;
&lt;li&gt;通过DMA读取数据的物理位置&lt;/li&gt;
&lt;li&gt;SQ head++出队&lt;/li&gt;
&lt;li&gt;将请求转发至嵌入式缓存层或者FTL&lt;/li&gt;
&lt;li&gt;当出现缺页或者页面替换时，FTL将目标LBA转换成Z-NAND中相应的物理地址，必要时自行GC&lt;/li&gt;
&lt;li&gt;在完成I/O请求之后，NVMe控制器增加这个CQ的tail，入队&lt;/li&gt;
&lt;li&gt;通过DMA传输数据，并修改phase tag&lt;/li&gt;
&lt;li&gt;主机ISR通过搜索队列中检查phase tag，对于有效的phase tag，ISR清除tag位，并且处理剩余的I/O完成请求程序。&lt;/li&gt;
&lt;li&gt;CQ head++出队，在SQ中移除相应的记录，并且写入CQ的head doorbell&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;30-跨层设计&#34;&gt;3.0 跨层设计&lt;/h1&gt;
&lt;h2 id=&#34;31-快速存储的挑战&#34;&gt;3.1 快速存储的挑战&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220811110758918.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220811110758918&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;原因是存储栈无法区分来自Apache的I/O请求，及时两个应用需要不同级别I/O的响应。&lt;/p&gt;
&lt;h1 id=&#34;32-预知灵敏响应&#34;&gt;3.2 预知灵敏响应&lt;/h1&gt;
&lt;p&gt;为了让内核可以区分I/O 请求的优先级和紧迫程度，修改Linux的进程控制快&lt;code&gt;task_struct&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;为了保证有效性，在&lt;code&gt;address_space&lt;/code&gt;,&lt;code&gt;bio&lt;/code&gt;,&lt;code&gt;request&lt;/code&gt;,&lt;code&gt;nvme_rw_command&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;中都保存工作负载属性，在存储堆栈上打孔。&lt;/p&gt;
&lt;p&gt;FlashShare同时提供了一个可以在服务器上配置这些属性的工具。叫做&lt;code&gt;chworkload_attr&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;可以方便的修改每个应用的属性并绑定到&lt;code&gt;task_struct&lt;/code&gt;中&lt;/p&gt;
&lt;p&gt;修改了syscall表&lt;code&gt;arch/x86/entry/syscalls/syscall 64.tbl&lt;/code&gt;添加了两个系统调用，可以从&lt;code&gt;task_struct&lt;/code&gt;中set/get工作属性。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;/linux/syscall.h&lt;/code&gt;中进行注册，并带有&lt;code&gt;asmlinkage&lt;/code&gt;标签。&lt;/p&gt;
&lt;p&gt;用户通过shell给定特定进程，实现于&lt;code&gt;/sched/cores.c&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;33-内核优化&#34;&gt;3.3 内核优化&lt;/h2&gt;
&lt;p&gt;优化文件系统中的blk-mq和NVMe驱动&lt;/p&gt;
&lt;p&gt;blk-mq合并重排请求提高了带宽使用，但是引入了延迟&lt;/p&gt;
&lt;p&gt;跳过所有在线应用的I/O 请求&lt;/p&gt;
&lt;p&gt;如果离线应用程序的 I/O 请求被 blk-mq 调度到后续在线应用程序发出的同一 LBA，则可能发生危险。&lt;/p&gt;
&lt;p&gt;如果两个请求的操作类型不同，blk-mq会将两个请求串联。否则blk-mq会将两个请求合并为一个&lt;code&gt;request&lt;/code&gt;并交给NVMe驱动。&lt;/p&gt;
&lt;p&gt;为了防止延迟敏感的I/O 被NVMe控制器杀死：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为每个核心创建两个SQ队列和一个CQ队列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220811173922087.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220811173922087&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其中一个SQ保存来自在线应用的I/O请求。&lt;strong&gt;NVMe驱动程序通过管理员队列发送消息，通知NVMe控制器选择一种新的队列调度方法，该方法始终优先安排该SQ中的请求。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;未来避免因优先级带来的饥饿，当该队列中的请求数量大于阈值时，或者没有在规定时间内被满足，NVMe驱动会满足所有离线应用I/O 。&lt;/p&gt;
&lt;p&gt;实验表明，队列大小为8或者200us的阈值最好。&lt;/p&gt;
&lt;h1 id=&#34;40-io-completion和缓存&#34;&gt;4.0 I/O Completion和缓存&lt;/h1&gt;
&lt;p&gt;采用轮询机制时查询I/O Completion时，内核态占用97%。&lt;/p&gt;
&lt;p&gt;带来两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;没有为处理I/O 响应单独分配核心，对于多进程下低效&lt;/li&gt;
&lt;li&gt;我们要减轻处理I/O轮询的核心开销，进一步降低延迟&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;41-中断处理程序&#34;&gt;4.1 中断处理程序&lt;/h2&gt;
&lt;p&gt;flash share仅对来自在线应用的I/O 请求使用轮询&lt;/p&gt;
&lt;p&gt;使用信号处理离线应用&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220811231355037.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220811231355037&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;修改blk-mq中的&lt;code&gt;submit_bio()&lt;/code&gt;，将由文件系统或缓存的bio插入到mq&lt;/li&gt;
&lt;li&gt;如果bio是来自离线应用的，则插入队列，as normal&lt;/li&gt;
&lt;li&gt;如果bio是来自在线应用的，blk-mq则调用&lt;code&gt;queue_rq()&lt;/code&gt;将请求发送至NVMe驱动。&lt;/li&gt;
&lt;li&gt;NVMe驱动转换I/O 请求为NVMe指令并非插入到响应SQ队列中&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用Select-ISR，当请求为离线应用时，CPU核心可以通过上下文切换从NVMe驱动中释放。否则，blk-mq调用轮询机制&lt;code&gt;blk-poll()&lt;/code&gt;。&lt;code&gt;blk-poll()&lt;/code&gt;持续调用&lt;code&gt;nvme_poll()&lt;/code&gt;，检查有效的完成记录是否存在于目标NVMe CQ中。如果存在，blk-mq禁用此CQ的IRQ，以至于MSI信号无法再次捕获blk-mq程序。&lt;code&gt;nvme_poll()&lt;/code&gt;通过检查CQ中的phase tags查找CQ中的新记录。&lt;/p&gt;
&lt;p&gt;具体来说，&lt;code&gt;nvme poll()&lt;/code&gt;搜索一个CQ记录，其请求信息与&lt;code&gt;blk poll()&lt;/code&gt;等待完成的标签匹配。一旦检测到这样的新记录，blk-mq就会退出在&lt;code&gt;blk poll()&lt;/code&gt;中实现的无限迭代，并将上下文切换到其用户进程。&lt;/p&gt;
&lt;p&gt;提出&lt;code&gt;I/O-stack accelerator&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;主要目的是将blk-mq的任务迁移到附属于PCIe的加速器中&lt;/p&gt;
&lt;p&gt;可以使得通过上层文件系统生成的bio直接转换成&lt;code&gt;nvm_rw_command&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;通过特殊的tag索引搜索队列中的元素，并且代表CPU合并bio&lt;/p&gt;
&lt;p&gt;该方法可以减少36%的I/O completion时间。&lt;/p&gt;
&lt;h2 id=&#34;42-固件层&#34;&gt;4.2 固件层&lt;/h2&gt;
&lt;p&gt;创建两个内存分区，一个服务于在线应用，一个服务于离线应用。&lt;/p&gt;
&lt;p&gt;三种模式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;固定拆分缓存&lt;/li&gt;
&lt;li&gt;根据I/O动态划分&lt;/li&gt;
&lt;li&gt;数据可保留&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220812003132031.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220812003132031&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;43-io-stack-acceleration&#34;&gt;4.3 I/O-Stack Acceleration&lt;/h2&gt;
&lt;p&gt;添加了一个barrier logic，简单的MUX，作为硬件仲裁&lt;/p&gt;
&lt;p&gt;引入status bitmap来过滤SQ队列中的记录&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并逻辑插入一个行的nvme 指令，status bitmap设置为1&lt;/li&gt;
&lt;li&gt;如果监测到ULL SSD从I/O SQ中读取NVMe指令，status bitmap设置为0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果状态位图表明CAM中的请求条目（与目标SQ相关联）无效，CAM将跳过对这些条目的搜索。&lt;/p&gt;
&lt;h1 id=&#34;50-实验&#34;&gt;5.0 实验&lt;/h1&gt;
&lt;h2 id=&#34;51-实验步骤&#34;&gt;5.1 实验步骤&lt;/h2&gt;
&lt;p&gt;使用gem5系统结构模拟&lt;/p&gt;
&lt;p&gt;64位arm指令集&lt;/p&gt;
&lt;p&gt;Linux 4.9.30&lt;/p&gt;
&lt;p&gt;8核心2GHz&lt;/p&gt;
&lt;p&gt;L1 Cache 64KB&lt;/p&gt;
&lt;p&gt;2GB Memory&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20220812191343849.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220812191343849&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;将SSD固件转移到主机上，消除冗余的地址转换&lt;/p&gt;
&lt;p&gt;根据应用程序特征对缓存进行分区处理，然而不能发挥ULL SSD的作用&lt;/p&gt;
&lt;p&gt;从文件系统和block IO设备方面优化移动端操作系统，使其提高SQLite的性能，有局限性，应用程序、ULL SSD&lt;/p&gt;
&lt;p&gt;在内核的多个层对写请求进行调度，容易阻塞读请求和ULL操作&lt;/p&gt;
&lt;p&gt;根据前台任务和后台任务中的依赖关系，分配优先级，允许后台任务高优先级，IO通常情况下没有依赖关系，效果差，服务器大部分都是多进程&lt;/p&gt;
&lt;p&gt;考虑对在线应用设置高优先级，但是没有考虑对IO stack中其他部分的影响&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
