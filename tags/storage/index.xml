<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Storage - Tag - Coding Panda&#39;s Blog</title>
    <link>https://zhiwayzhang.github.io/tags/storage/</link>
    <description>Storage - Tag - Coding Panda&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</managingEditor>
      <webMaster>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</webMaster><lastBuildDate>Tue, 11 Oct 2022 13:44:49 &#43;0800</lastBuildDate><atom:link href="https://zhiwayzhang.github.io/tags/storage/" rel="self" type="application/rss+xml" /><item>
  <title>【存储系统】文件系统</title>
  <link>https://zhiwayzhang.github.io/posts/filesystem/</link>
  <pubDate>Tue, 11 Oct 2022 13:44:49 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/filesystem/</guid>
  <description><![CDATA[0x00 基本概念 文件的基本概念， 包含信息的比特流/块，计算机往往需要处理远高于主存容量的数据，需要使用文件来保存；文件的生命周期更长，不能存储在堆栈中；对文件的数据/信息对外进行分享的媒介。可以用于进程间的信息共享，自由性和容量上都优于共享内存的方式。
文件系统的功能 Namespace 命名空间
文件目录的建立、维护和检索（目录树） 存储管理
空间管理：存储空间的分配和管理（管理空闲的块） 文件块管理：逻辑地址与存储物理地址的映射（logical offset—Physical Block number） 数据保护
文件出现坏块 灾备 可靠性/一致性
0x01 用户视角 文件目录操作 文件操作：
Create/Delete Open/Close 返回File Handle Read/Write Append 追加写入 Seek 移动文件指针 offset Getattr/Setattr 读取/修改inode中保存的文件信息 Rename 目录操作：
Createdir/rmdir Link 软链接、硬链接 讲目录项目指向文件的inode Unlink Readdir 读取子目录以及目录项 Rename 文件目录的构成 文件的组成：
文件内容，字节序列 文件的元数据metadata，文件控制块，Unix系统中称为inode(index node) 目录的组成：
本质是一种特殊的文件，内容是子目录、文件等目录项目 目录也有inode File Handle 创建原文件的一个实例对象，对其进行读写，Offset可以共享给两个File Handle，也可以每个File Handle独有各自的Offset。
Linux VFS中为文件描述符fd，对应一个File Object，包含文件的信息，文件系统管理dentry、inode的缓存，用于查找文件在磁盘上的位置。关于Linux VFS的内容，计划再单独开个博客记录一下。
File Control Block (Inode) 每个文件唯一的id，包含以下结构
Block的信息，文件在磁盘上的位置 数据大小 Ctime创建时间，utime更新时间，access time最后一次访问时间 所有者信息，ACL(access control lists)访问控制 链接情况统计 Dirent - Directory Entry 目录项纪录子文件和子目录的名称]]></description>
</item>
<item>
  <title>【存储系统】多硬盘存储系统</title>
  <link>https://zhiwayzhang.github.io/posts/multidisksystems/</link>
  <pubDate>Wed, 05 Oct 2022 16:54:42 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/multidisksystems/</guid>
  <description><![CDATA[0x00 多硬盘系统 对多硬盘系统可靠性的度量，上图从故障产生，故障产生程序错误，错误检测，修复故障四个时间节点为一个周期。
三个可靠性的指标：
可靠性：通过错误产生到发现错误的时间间隔MTTF来衡量
可维护性：通过发现错误到错误修复的时间间隔来衡量，定义系统修复的能力
可用性：通过系统中非修复时间所占的比例，可以对外提供服务的时间占比，$EA=\frac{MTTF}{MTTF+MTTR}$
MTBF=MTTR+MTTF，两故障之间的时间间隔
恢复一块硬盘的代价很高，为了提高系统可用性，需要构建多盘系统，在某块磁盘故障时仍然能正常工作。
每秒的I/O请求峰值为1200，R/W比例为2:1，分别使用RAID 1/5，计算峰值的硬盘负载
总的R/W分别为800/400
RAID 1：写操作需要翻倍，总负载为800+400*2=1600
RAID 5：写操作翻四倍，总负载为800+400*4=2400
使用多盘系统的目的：
提高存储容量 提高性能 负载均衡 Disk Striping 带宽 吞吐量 提高可靠性 容错 基于奇偶校验的保护 创建副本 JBOD（Just a Bunch Of Disks）单纯对硬盘进行组合，只提高存储容量
负载均衡 静态：某一地址固定映射到某块硬盘
动态：热点数据分配到不同的硬盘中
按照数据块：将一个数据划分后存储到所有硬盘中（Disk Striping 条带化）
Disk Striping 按一定大小对数据分块（stripe unit/block），然后依次存储到各个硬盘中
分配规则使用轮询枚举，通过取模判断存储的磁盘编号，根据余数确定偏移量
Stripe Unit大小设置一般为2M、4M、16M，太大丧失负载均衡能力，太小会在寻址上花费太多时间
磁盘失效 对于JBOD、Striping某块盘失效，整个文件系统都将失效
可以通过备份来解决，要花费较长的时间
多盘系统的MTTF为第一块磁盘失效的时间，比单盘系统时间短很多
解决方案：Redundancy冗余存储
Redundancy 存储副本 存储两份或更多的副本，采用三副本的服务有HDFS/GFS/Cloud Storage
Disk Mirroring 磁盘镜像，分主从磁盘，从盘基于主盘进行同步，可以从任意盘进行读取
有较高的可用性
花费较高
写性能降低，磁盘之间需要同步，受限于最慢同步的磁盘
读性能提高，请求被分散到各个磁盘
磁盘镜像可以与结合条带化
先做Mirroring再做Striping (RAID 10)
若有磁盘故障，会损失1/2的数据
先做Striping再做Mirroring (RAID 01)]]></description>
</item>
<item>
  <title>【存储技术基础】主存</title>
  <link>https://zhiwayzhang.github.io/posts/mainmemory/</link>
  <pubDate>Sat, 24 Sep 2022 18:26:22 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/mainmemory/</guid>
  <description><![CDATA[0x00 主存 当前面临的问题：
容量问题、带宽问题、QoS保证
多核处理器、数据密集应用、云计算、GPU
HDFS（GFS）基于外存储器
Spark 内存中数据处理
能耗功率问题
40%功耗在DRAM，Refresh操作耗能
DRAM发展遇到瓶颈
制成方式限制
改进：
3D-Stack DRAM 提供更高的带宽 低延迟DRAM 低功耗DRAM NVM（e.g. PCM）容量更大，延迟较高 0x01 DRAM的组成 Channel DIMM 内存条 Rank 二维阵列最小单元 Chip 芯片 Bank Row/Column 行为字线 word line
列为位线，交点是cell 内存单元
DRAM row是一个DRAM page
Sense Amplifiers 也被叫做Row buffer
每个地址通过&lt;row,colum&gt;编址
访问一个closed row的过程：
Activate：将row放到row buffer中 Read/Write：读写row buffer中的column Precharge：从row buffer中的数据写回到选中的row中 Bank Operation 给定Row Address通过Row decoder选中一行
将行加载到Row Buffer
若读取的Row在Row Buffer中，则为命中Hit状态，通过Column Mutex直接获取数据
若读取的Row不在Row Buffer中，则为冲突Conflict状态，将Row Buffer回写，然后再选中新的Row，读取数据
Chip 由多个(2-16个)Bank组成，Bank共享总线（指令/地址/数据总线）
每次只能读4或16 Bit
Rank and Module Module为DIMM内存条，连接在主板上，由一个或多个rank组成]]></description>
</item>
<item>
  <title>【存储技术基础】固态硬盘</title>
  <link>https://zhiwayzhang.github.io/posts/storagessd/</link>
  <pubDate>Thu, 22 Sep 2022 09:36:42 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/storagessd/</guid>
  <description><![CDATA[主要由Flash Memory 和FTL组成
Non-Volatile Memory 提供低延迟持久性的内存/存储，也可以用来做内存
根据延迟数量级，一般用PCM做内存，Flash Memory做外存
0x00 Flash Memory 闪存原理 类型 NOR闪存
存储密度低 可字节改写 NAND闪存（主流）
存储密度高 不可覆盖写 用于外存需要较高的存储量级，一般用NAND
闪存单元 读：电压代表不同数值
写：电子注入
相比晶体管添加了浮栅门，保存电子
原理其实比较简单，非电子系就不做太详细的研究了
闪存页(4KB,8KB,16KB，读写单元)，阵列中的每一行
闪存块(擦除单元)，由多个页组成的单元
选中行和列，然后将数据加载到Sense Amplifiers
存储单元有两个阈值的电压，可以根据两个电压的中点作为读电压，2.5V读电压时左边通电，数据为1，右边则不通电，数据为0
Pass Through
选取一个较大的电压，使得所有的单元都接通，数据为1，不影响其他行的状态
如图所示的存储结构，在第二行施加2.5V电压，其他行施加5V，最终读取数据为0011
上述为SLC，Single Level Cell，单存储单元
多比特闪存 多比特闪存单元MLC，包含2Bits 4个Level的数据
TLC 3 Bits 8个Level
QLC 4 Bits 16个Level
多比特使用格雷码来编码
使用格雷码使得相邻单元只有一位差异，方便纠错
多比特提高了存储密度，但是提高了错误率，因为施加的电压差距很小。可靠性会降低。
对于多比特的写，MLC分为高比特和低比特，对于低比特的状态加偏移电压确定高比特，在低比特时需要加的电压较大，操作难度低，运行速度快，在高比特时需要加的电压小，波形的间距小，操作难度高，运行的速度较慢。
对于多比特的读，先看lower bit，加一次电压，即可筛选出低位的0，1，再加两次电压确定upper bit。因为upper bit为0的在中间部分，为1的在两侧，因此需要在两个分界线分别加一次电压来确定upper bit为多少。
闪存 Block的大小的一种配置：
一行有两个Page，Upper Page和Lower Page，每个单元中，低位构成Lower Page，高位构成Upper Page，有128个单元，128K/8=16KB
有64列bitlines，一个block的大小即为16KB*64*2=2MB，一般按照此比例配置Block
写入时按照固定顺序，写入高低页面相互独立，不能同时写，在写入加压时容易使相邻单元发生数据偏移，要降低错误率
写入是需要先擦除再写入，擦除整个块
特性 读写粒度 闪存页读写粒度：]]></description>
</item>
<item>
  <title>【存储技术基础】磁盘技术</title>
  <link>https://zhiwayzhang.github.io/posts/disktechnology/</link>
  <pubDate>Tue, 20 Sep 2022 16:13:29 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/disktechnology/</guid>
  <description><![CDATA[本讲内容HDD好弟弟：
磁盘的构成和各个部分的功能 磁盘的性能和评估方式 磁盘固件的功能，固件算法 推荐阅读：Memory System: Cache,DRAM,Disk.Author: Bruce Jacob, Spencer W. Ng, David T.Wang
0x00 组成 转轴，盘片，磁头
track 磁道，每个盘片的正面和背面的同心圆
对于盘片每个面是surface，每个track磁道分为多个sector（扇区）
所有盘片上同一个位置的track是一个cylinder（柱面）
寻址 物理编址：CHS，Cylinder-Head-Sector 定位一个块的位置
根据柱面-磁头-扇区来访问
右边图示表示逻辑块地址，每个柱面可以从上到下顺序编址，组成线性结构
0x01 性能评估 电子设备 电路驱动 机械设备 seek time 磁头运动速度 rotational latency 转轴旋转速度 data transfer rate 数据传输速率 $磁盘访问时间=磁头运动速度+转轴旋转速度+数据传输速度$
Seek time inner和outter track中进行移动，到达对应的track所消耗的时间
评价方法：
Full stroke 里圈到外圈 Average 移动到中间所花费的时间 Track-to-track 两个磁道间的移动时间 Rotational Latency 平均旋转速度：转半圈的时间
厂商使用转速来标称
rpm: round per min $$ {\rm Rotational\ Latency}=\frac{1}{2}*\frac{1}{\frac{x\ rpm}{60}} $$
Data Transfer Time 分为内部传输和外部传输]]></description>
</item>
<item>
  <title>【存储技术基础】概述</title>
  <link>https://zhiwayzhang.github.io/posts/storage/</link>
  <pubDate>Tue, 20 Sep 2022 14:41:57 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/storage/</guid>
  <description><![CDATA[0x00 Intro 对于场景制定存储方案：
存储结构 存储大小 可靠性和容错 文件访问类型，读/写频率 要学习硬件的选择和文件系统设计
0x01 现状 存储的挑战 数据量激增
存储是什么 分层的存储架构
L0-L1-L2-DRAM-Disk
存储的特性 可靠性：
多副本 纠删码 一致性：
原子+持久化 数据保护 容灾和备份：
解决单点故障 0x02 存储历史与发展 硬件上的发展 1956&ndash;HDD
1984&ndash;Flash Memory 早期用于嵌入式
2010s&ndash;Persistent Memory 3D-Point Intel
SSD NAND Flash：
SLC，MLC，TLC，QLC存储单元中电平的等级
对比HDD和SSD：
对于NAND，需要先擦出，再写
NAND Flash有擦除，HDD只能覆盖写
SSD多个颗粒可以并行I/O，总体上速度快
软件上的发展 网络存储 远程挂载目录NAS
分布式文件系统GFS，Hadoop HDFS
键值存储Key-Value
云存储 数据存在远端，商业化，软件协作Office
Assignment 有一个服务器集群（4台），针对I/O 500测试，需要考虑哪些问题？]]></description>
</item>
<item>
  <title>[ATC&#39;19] Asynchronous I/O Stack: A Low-latency Kernel I/O Stack for Ultra-Low Latency SSDs</title>
  <link>https://zhiwayzhang.github.io/posts/asyncio/</link>
  <pubDate>Wed, 17 Aug 2022 10:30:41 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/asyncio/</guid>
  <description><![CDATA[Intro 优化I/O的方法
用户层面直接调用外部存储设备，需要应用包含文件系统的调用，臃肿，同时不同应用和用户间的冲突问题。 优化操作系统内核的I/O 栈 使用轮询来减少上下文切换的开销 在底层减少一半的中断处理 分散I/O指令 I/O block调度机制 Related Work 减少内核的开销
减少中断处理的后半部分 是用轮询技术而非中断，减少上下文切换 混合轮询 基于SSD的闪存随机读写简化调度策略 在NVMe固件中进行调度 对高优先级的任务，提供不同的IO path支持，最小化IO path的开销 修改存储接口
分散/分散 IO合并多个IO到一个指令，减少往返次数 移除doorbell机制和完成信号 改善fsync
冲fsync请求发出到收到response，延长数据持久化的时间 在日志提交记录中使用校验和，有效的重叠日志写入和块写入 提出写守序系统调用，重叠的fsync效果相同，当应用需要使用fsync时，关于IO的操作将同步进行 用户层直接访问外设，存在隔离、保护等安全问题
Motivation 背景 现状：
I/O 请求过程中太多的步骤 页面缓存分配和索引 DMA，一系列数据结构的创建 目前的 ULL SDD实现了低于10微妙的IO延迟，然而操作系统内核产生的延迟没有明显变化
本文专注于：
Linux内核中的read()和write()+fsync() 基于NVMe SSD的Ext4文件系统 For Read Path 研究发现许多剩余的操作不必在设备I/O之前或之后执行
此类操作可以在设备I/O操作进行时执行
因为此类操作大多独立于设备I/O操作，因此考虑让这些操作与IO重叠
For Write Path 缓冲写write()，并不发起IO请求，不能异步处理
由于fsync的回写机制和文件系统崩溃一致性（日志系统），包含部分IO请求
由于文件系统带来的三次IO操作
数据块写 jbd2发起写入日志Block I/O 提交Block I/O 这些IO的创建，涉及众多过程（block分配，请求缓冲页，创建和提交bio，设置DMA地址），因此可以让CPU将这些前置操作在IO请求发起前预执行。
For Lightweight Block Layer 传统Block Layer涉及过多过程，推迟了IO指令提交给设备的时间
因为ULL SSD的高速随机IO性能和低速的顺序IO，请求重排的效果很低]]></description>
</item>
<item>
  <title>[OSDI&#39;19] Flashshare: Punching Through Server Storage Stack from Kernel to Firmware for Ultra-Low Latency SSDs</title>
  <link>https://zhiwayzhang.github.io/posts/flashshare/</link>
  <pubDate>Wed, 17 Aug 2022 10:30:30 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/flashshare/</guid>
  <description><![CDATA[超低延迟固态硬盘从内核到固件的服务器存储堆栈
个别名词解释 the 99^th percentile 超过统计数据99%的数是多少
blk-mq Linux Multiqueue block layer 内核对ssd随机I/O的优化
message signaled interrupt (MSI)
1.摘要 flash share
在内核中，扩展了系统堆栈的数据结构，传递应用程序的属性（？），包括内核层到SSD固件。
对于给定的属性，FlashShare的块层管理IO调度并处理NVMe中断。
评估结果表明，FLASHSHARE可以将共同运行应用程序的平均周转响应时间分别缩短22%和31%。
1.0 Intro 1.1 现状 网络服务提供商，满足服务级别协议SLA，延迟敏感
某个段时间短可能有大量请求涌入，供应商会超额配置机器以满足SLA
现状：该场景并不常见，因此大部分情况下服务器的资源占用率非常低，能耗比低。
为了解决利用率低，服务器会运行离线的数据分析应用，延迟不敏感，以吞吐量为导向。
因此，在运行了多个进程的服务器上，I/O延迟增高，满足SLA非常困难。
现有的ULL SSD相较于NVMe SSD可以减少10倍的延迟
但是这些ULL SSD在同时运行多个进程下高强度压榨服务器的时候，不能充分利用ULL SSD的优势/表现一般。
the 99th percentile 是0.8ms（apache）
但是当服务器同时运行pagerank的时候，延迟会增加228.5%。
原因：略
从固件到内核优化堆栈的存储。
内核级别的增强：
两个挑战
Linux的blk-mq导致I/O请求队列化，引入延迟 NVMe的队列机制没有对I/O优先级的策略，因此，来自离线应用的IO请求容易阻塞在线应用的紧急请求，造成延迟。 对于latency critical的请求，绕过NVMe的请求队列。同时令NVMe的驱动通过知晓每个应用的延迟临界匹配NVMe的提交和请求队列。
固件层设计：
​	即使内核级的优化保证了延迟敏感的请求可以获得高优先级，但如果基础固件不了解延迟临界值，ULL特性（类似内存的性能）无法完全暴露给用户。本文中重新设计了I/O调度和缓存的固件，以直接向用户暴露ULL特性。将ULL SSD的集成缓存进行分区，并根据工作负载的属性对每个I/O服务独立的分配缓存。固件动态的更新分区大小并以精细粒度调整预取I/O粒度。
ULL SSD的新中断处理服务： ​	当前的NVMe中断机制没有对ULL I/O服务优化。轮询方法（Linux 4.9.30）消耗了大量的CPU资源去检查I/O服务的完成情况。当轮询在线交互服务的IO请求完成状态时，flashShare使用一个仅对离线应用程序使用消息信号中断的选择性中断服务程序Select-ISR。
​	通过将NVMe队列和ISR卸载到硬件加速器中来进一步优化NVMe completion routine。
​	各种仿真实验后效果不错，效率提高了22%和31%。
2.0 Background 2.1 存储内核栈 Linux文件系统IO]]></description>
</item>
</channel>
</rss>
