<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>I/O on Coding_Panda&#39;s Blog</title>
        <link>https://blog.ipandai.club/tags/i/o/</link>
        <description>Recent content in I/O on Coding_Panda&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language><atom:link href="https://blog.ipandai.club/tags/i/o/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Using Cgroups v2 to limit system I/O resources</title>
        <link>https://blog.ipandai.club/p/using-cgroups-v2-to-limit-system-i/o-resources/</link>
        <pubDate>Fri, 07 Apr 2023 19:51:41 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/using-cgroups-v2-to-limit-system-i/o-resources/</guid>
        <description>&lt;h1 id=&#34;introduction-to-cgroups&#34;&gt;Introduction to Cgroups&lt;/h1&gt;
&lt;p&gt;Cgroups, which called control group in Linux to limit system resources for specify process group, is comonly used in many container tech, such as Docker, Kubernetes, iSulad etc.&lt;/p&gt;
&lt;p&gt;The cgroup architecture is comprised of two main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the cgroup core: which contains a pseudo-filesystem cgroupfs,&lt;/li&gt;
&lt;li&gt;the subsystem controllers: thresholds for system resources, such as memory, CPU, I/O, PIDS, RDMA, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/HighLevelArch.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;high level hierarchy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;the-hierarchy-of-cgroups&#34;&gt;The hierarchy of Cgroups&lt;/h1&gt;
&lt;p&gt;In Cgroups v1, per-resource (memory, cpu, blkio) have it&amp;rsquo;s own hierarchy, where each resource hierarchy contains cgroups for that resource. The location of cgroup directory is &lt;code&gt;/sys/fs/cgroup/&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; tree -L &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; /sys/fs/cgroup
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/sys/fs/cgroup
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- blkio
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- cpu -&amp;gt; cpu,cpuacct
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- cpuacct -&amp;gt; cpu,cpuacct
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- cpu,cpuacct
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- cpuset
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- devices
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- freezer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- hugetlb
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- memory
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- net_cls -&amp;gt; net_cls,net_prio
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- net_cls,net_prio
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- net_prio -&amp;gt; net_cls,net_prio
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- perf_event
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- pids
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;-- rdma
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;-- systemd
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;User can create sub cgroup in every resource diectory to limit the process usage of the resource.&lt;/p&gt;
&lt;p&gt;Directory tree above shows the system resources can be control. #{TODO}&lt;/p&gt;
&lt;p&gt;Unlike cgroup v1, v2 has only single hierarchy.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; ls -p /sys/fs/cgroup
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.controllers      cgroup.subtree_control  init.scope/      /sys/fs/cgroup/nvme/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.max.depth        cgroup.threads          io.cost.model    system.slice/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.max.descendants  cpu.pressure            io.cost.qos      user.slice/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.procs            cpuset.cpus.effective   io.pressure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.stat             cpuset.mems.effective   memory.pressure
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The &lt;code&gt;/sys/fs/cgroup&lt;/code&gt; is the root cgroup. In Figure below shows the relationship between parent cgroup and child cgroup.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/hierarchyEXTERNAL.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;hierarchy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;File &lt;code&gt;cgroup.controllers&lt;/code&gt; lists the controllers available in this cgroup, the root cgroup has all the controllers on the system.&lt;/p&gt;
&lt;p&gt;File &lt;code&gt;cgroup.subtree_control&lt;/code&gt;  has the resource can be limited in it&amp;rsquo;s child cgroup. Those limits can be inherited by sub cgroup.&lt;/p&gt;
&lt;p&gt;The child cgroup&amp;rsquo;s &lt;code&gt;cgroup.controllers&lt;/code&gt; will generate by it&amp;rsquo;s parent cgroup&amp;rsquo;s &lt;code&gt;cgroup.subtree_control&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; cat cgroup.controllers
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cpuset cpu io memory pids rdma
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; cat cgroup.subtree_control
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cpuset cpu io memory pids
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;This cgroup can limit cpuset, cpu, I/O, memory, rdma and pids, and it&amp;rsquo;s child cgroup can limit cpuset, cpu, I/O, memory and pids.&lt;/p&gt;
&lt;p&gt;To modify the &lt;code&gt;cgroup.subtree_control&lt;/code&gt;, can use plus sign(+) or minus sign(-) to enable or disable controllers.&lt;/p&gt;
&lt;p&gt;As in this example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;+cpu -memory&amp;#39;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sudo tee /sys/fs/cgroup/cgroup.subtree_control
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;enable-cgroups-v2-in-linux&#34;&gt;Enable cgroups v2 in Linux&lt;/h1&gt;
&lt;p&gt;First, check the current cgroups fs version with command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; stat -fc %T /sys/fs/cgroup/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;For cgroup v2, the output is &lt;code&gt;cgroup2fs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For cgroup v1, the output is &lt;code&gt;tmpfs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If the output is &lt;code&gt;tmpfs&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;Edit grub config &lt;code&gt;/etc/default/grub&lt;/code&gt;, append &lt;code&gt;systemd.unified_cgroup_hierarchy=1&lt;/code&gt; to config &lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;GRUB_CMDLINE_LINUX&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ip=dhcp console=ttyS0,115200 console=tty console=ttyS0 systemd.unified_cgroup_hierarchy=1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Then execute &lt;code&gt;sudo update-grub&lt;/code&gt;, after system reboot, the cgroups v2 will be enabled.&lt;/p&gt;
&lt;h1 id=&#34;configure-cgroup-to-limit-io-resource&#34;&gt;Configure cgroup to limit I/O resource&lt;/h1&gt;
&lt;p&gt;To create a new cgroup, use &lt;code&gt;mkdir&lt;/code&gt; in &lt;code&gt;/sys/fs/cgroup/&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; /sys/fs/cgroups/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; sudo mkdir nvme
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;In &lt;code&gt;nvme&lt;/code&gt; directory, we have&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; ls nvme/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.controllers      cpu.max                cpuset.mems            memory.low
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.events           cpu.pressure           cpuset.mems.effective  memory.max
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.freeze           cpu.stat               io.max                 memory.min
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.max.depth        cpu.uclamp.max         io.pressure            memory.oom.group
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.max.descendants  cpu.uclamp.min         io.stat                memory.pressure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.procs            cpu.weight             io.weight              memory.stat
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.stat             cpu.weight.nice        memory.current         pids.current
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.subtree_control  cpuset.cpus            memory.events          pids.events
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.threads          cpuset.cpus.effective  memory.events.local    pids.max
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cgroup.type             cpuset.cpus.partition  memory.high
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;To enable the control to &lt;code&gt;I/O&lt;/code&gt; resources, we need to enable the I/O controller in the &lt;code&gt;/sys/fs/cgroup.subtree_control&lt;/code&gt; of root cgroup.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ /sys/fs/cgroup&amp;gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;+io&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sudo tee cgroup.subtree_control
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ /sys/fs/cgroup&amp;gt; sudo &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;+io&amp;#34;&lt;/span&gt; &amp;gt; cgroup.subtree_control &lt;span class=&#34;c1&#34;&gt;# permission denied&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Dont&amp;rsquo;t use &lt;code&gt;&amp;gt;&lt;/code&gt; to redirect output to this file (&lt;strong&gt;unles in sudo mode&lt;/strong&gt;), because &lt;code&gt;&amp;gt;&lt;/code&gt;  shell process redirection first before running follwing command, so the &lt;code&gt;sudo&lt;/code&gt;  command did not take effect.&lt;/p&gt;
&lt;p&gt;We also need to find out the major and minor number of block devices.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; cat /proc/partitions
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;major minor  &lt;span class=&#34;c1&#34;&gt;#blocks  name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	...			...		....		...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;m&#34;&gt;8&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;83886080&lt;/span&gt; sda
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;m&#34;&gt;8&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       &lt;span class=&#34;m&#34;&gt;1024&lt;/span&gt; sda1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;m&#34;&gt;8&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;83883008&lt;/span&gt; sda2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; fd0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;m&#34;&gt;259&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4194304&lt;/span&gt; nvme0n1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;We will use the &lt;code&gt;major&lt;/code&gt; and &lt;code&gt;minor &lt;/code&gt;number of &lt;code&gt;nvme0n1&lt;/code&gt; device partition.&lt;/p&gt;
&lt;p&gt;There also have other control keys:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;riops&lt;/code&gt;: Max read I/O operations per second&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wiops&lt;/code&gt;: Max write I/O operations per second&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rbps&lt;/code&gt;: Max read bytes per second&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wbps&lt;/code&gt;: Max write bytes per second&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The lines of &lt;code&gt;io.max&lt;/code&gt; must keyed by &lt;code&gt;$MAJOR:$MINOR riops=? wiops=? rbps=? wbps=?&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can remove the limit of control by setting &lt;code&gt;riops=max&lt;/code&gt; etc.&lt;/p&gt;
&lt;p&gt;We limit the write bandwidth (wbps, write bytes per second) of &lt;code&gt;io.max&lt;/code&gt; to  1 MB/s:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; /sys/fs/cgroup/nvme&amp;gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;259:0 wbps=1048576&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sudo tee io.max
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;After configured the I/O limit of resources, now we need to add the process pid will be controled.&lt;/p&gt;
&lt;p&gt;When execute this command,  &lt;code&gt;$$&lt;/code&gt; means the pid of current shell :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; /sys/fs/cgroup/nvme&amp;gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$$&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sudo tee cgroup.procs
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Now, let&amp;rsquo;s use &lt;code&gt;dd&lt;/code&gt; to generate I/O workload, before we start testing, clear the page cache first!&lt;/p&gt;
&lt;p&gt;Clear the page cache:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$&amp;gt; free -h
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;              total        used        free      shared  buff/cache   available
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Mem:          3.8Gi       169Mi       3.6Gi       0.0Ki       107Mi       3.5Gi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Swap:         3.8Gi          0B       3.8Gi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$&amp;gt; sync &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sudo tee /proc/sys/vm/drop_caches
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$&amp;gt; free -h
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;              total        used        free      shared  buff/cache   available
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Mem:          3.8Gi       168Mi       3.6Gi       0.0Ki       105Mi       3.5Gi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Swap:         3.8Gi          0B       3.8Gi
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Use &lt;code&gt;dd&lt;/code&gt; to generate I/O workload:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; sudo dd &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&#34;nv&#34;&gt;of&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/tmp/test/file1 &lt;span class=&#34;nv&#34;&gt;bs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1M &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;100+0 records in
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;100+0 records out
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;m&#34;&gt;104857600&lt;/span&gt; bytes &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;105&lt;/span&gt; MB, &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt; MiB&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; copied, 0.0560001 s, 1.9 GB/s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Note that, the data had been writen to page cache immediately.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;iostat&lt;/code&gt; to monitor the writeback performance, we can found that the &lt;code&gt;MB_wrtn/s&lt;/code&gt; is  limited to &lt;code&gt;1.0MB/s&lt;/code&gt;, which is a difference between cgroups v1 and v2 that cgroups v2 can limit the writeback bandwidth.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; iostat -h -m -d &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Linux 5.4.0-64-generic &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;fvm&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; 	04/08/2023 	_x86_64_	&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; CPU&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;avg-cpu:  %user   %nice %system %iowait  %steal   %idle
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           0.8%    0.4%    1.5%   23.1%    0.0%   74.2%
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;										......................................
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      tps    MB_read/s    MB_wrtn/s    MB_dscd/s    MB_read    MB_wrtn    MB_dscd Device
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k fd0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop3
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop4
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop5
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop6
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop7
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k loop8
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     3.00         0.0k         1.0M         0.0k       0.0k       1.0M       0.0k nvme0n1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.00         0.0k         0.0k         0.0k       0.0k       0.0k       0.0k sda
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Furthermore, when add the &lt;code&gt;direct&lt;/code&gt; oflag which means directly write data to device without page cache, the write bandwidth is limited to &lt;code&gt;1.0 MB/s&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &amp;gt; sudo dd &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&#34;nv&#34;&gt;of&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/tmp/test/file1 &lt;span class=&#34;nv&#34;&gt;bs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1M &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;100&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;oflag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;d
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;irect
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;100+0 records in
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;100+0 records out
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;m&#34;&gt;104857600&lt;/span&gt; bytes &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;105&lt;/span&gt; MB, &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt; MiB&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; copied, 100.003 s, 1.0 MB/s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this article, we can learn about Cgroups in Linux, and how to use Cgroups to limit system I/O resources.&lt;/p&gt;
&lt;p&gt;In a subsequent article, I will introduce some system API related to Cgroups.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://facebookmicrosites.github.io/cgroup2/docs/overview.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://facebookmicrosites.github.io/cgroup2/docs/overview.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://kubernetes.io/docs/concepts/architecture/cgroups/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://kubernetes.io/docs/concepts/architecture/cgroups/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://andrestc.com/post/cgroups-io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://andrestc.com/post/cgroups-io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.kernel.org/admin-guide/cgroup-v2.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://docs.kernel.org/admin-guide/cgroup-v2.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://manpath.be/f35/7/cgroups#L557&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://manpath.be/f35/7/cgroups#L557&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zorrozou.github.io/docs/%E8%AF%A6%E8%A7%A3Cgroup%20V2.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zorrozou.github.io/docs/%E8%AF%A6%E8%A7%A3Cgroup%20V2.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>[OSDI&#39;22] XRP: In-Kernel Storage Functions with eBPF</title>
        <link>https://blog.ipandai.club/p/osdi22-xrp-in-kernel-storage-functions-with-ebpf/</link>
        <pubDate>Thu, 09 Feb 2023 14:51:48 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi22-xrp-in-kernel-storage-functions-with-ebpf/</guid>
        <description>&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;随着超低延迟SSD的发展，I/O过程中来自内核的延迟比重不断升高。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230224131538122.png&#34; alt=&#34;image-20230224131538122&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;观察一个请求的从发起到完成的整个过程，内核部分占据了48.6%的延迟。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230224132654967.png&#34; alt=&#34;image-20230224132654967&#34; style=&#34;zoom: 67%;&#34; /&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225105147344.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230225105147344&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;因此可以考虑跳过内核中一系列的数据传递，即Kernel Bypass。目前现有的研究大都对内核进行了激进的修改策略，或引入了新的硬件，Kernel Bypass主要使用SPDK，直接对硬件设备进行访问，而SPDK会强制开发者实现自己的文件系统，需要自行维护隔离性和安全性。&lt;/p&gt;
&lt;p&gt;在等待I/O完成时，往往会进行轮询，给CPU带来了一些性能损失。有研究表明当可调度的线程数量超过了CPU核心数量时，使用SPDK会提高平均延迟和尾延迟，严重降低了吞吐量。&lt;/p&gt;
&lt;p&gt;因此作者的核心思想为既可以完成Kernel Bypass，也不需要引入额外硬件并不对内核和文件系统进行很大的修改，作者借助BPF(Berkeley Packet Filter)来实现，BPF允许应用程序下放部分工作到内核中，同时还能保证隔离性，允许多线程共享一个CPU核心，提高利用率。&lt;/p&gt;
&lt;p&gt;许多I/O负载需要多次调用函数访问硬盘上的大型数据结构（B+ tree），作者称之为resubmission。&lt;/p&gt;
&lt;p&gt;作者提出了eXpress Resubmission Path，XRP在NVMe驱动中的中断处理器上添加了一个hook，使得XRP可以在I/O完成时直接从NVMe驱动层来发起BPF函数调用，快速启动I/O的重新提交。&lt;/p&gt;
&lt;p&gt;XRP的主要贡献为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首次使用BPF来卸载I/O任务到内核&lt;/li&gt;
&lt;li&gt;将B-tree的查找吞吐量提高了2.5倍&lt;/li&gt;
&lt;li&gt;XRP提供了接近Kernel Bypass的延迟，而且允许线程和进程高效地共享CPU核心&lt;/li&gt;
&lt;li&gt;XRP适用于多种不同用例，支持不同类型数据结构以及存储操作&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x01-bg--motivation&#34;&gt;0x01 Bg &amp;amp;&amp;amp; Motivation&lt;/h1&gt;
&lt;h2 id=&#34;io是目前的瓶颈&#34;&gt;I/O是目前的瓶颈&lt;/h2&gt;
&lt;h3 id=&#34;时间都去哪了&#34;&gt;时间都去哪了&lt;/h3&gt;
&lt;p&gt;作者通过实验发现延迟的部分来源是软件层面，即系统内核部分中block I/O的传递&lt;/p&gt;
&lt;h3 id=&#34;为什么不单纯的kernel-bypass&#34;&gt;为什么不单纯的kernel bypass&lt;/h3&gt;
&lt;p&gt;大部分bypass的方法都是直接对NVMe driver发起请求，此类方案不能实现细粒度的隔离或者再不同应用之间共享数据，同时不能有效的去接收I/O完成产生的中断，需要应用不断轮询，因此CPU就被某个应用独占，不能得到共享。而且当多个轮询线程共享一个CPU处理器时，他们之间对CPU的竞争同时缺少同步会导致尾延迟升高和吞吐量的降低。&lt;/p&gt;
&lt;h2 id=&#34;bpf&#34;&gt;BPF&lt;/h2&gt;
&lt;p&gt;Berkeley Packet Filter允许用户将一些简单函数下放到内核层来执行，起初是用于TCP的数据包过滤、负载均衡、数据包转发，目前推出了eBPF扩展。&lt;/p&gt;
&lt;p&gt;BPF可以验证函数的安全性，主要检查是否超出内存地址空间、是否有死循环、指令是否太多。&lt;/p&gt;
&lt;p&gt;BPF可以直接发起一系列I/O请求，来获取那些不被程序直接利用的中间数据，例如指针寻址过程，B-tree索引便利。&lt;/p&gt;
&lt;p&gt;一些在硬盘维护并且使用指针来查找的数据结构，例如LSM tree，可以用于加速查找的过程。&lt;/p&gt;
&lt;p&gt;有研究对比了分别从User Space、Syscall、NVMe Driver层发起I/O之间吞吐量和延迟的差异。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225134629890.png&#34; alt=&#34;image-20230225134629890&#34; style=&#34;zoom: 33%;&#34; /&gt;
&lt;p&gt;在NVMe Driver中发起I/O有效提高了吞吐量降低了延迟，原因是越靠近存储设备的一层，总体的延迟和性能越高。因此XRP选择在NVMe Driver层来实现。&lt;/p&gt;
&lt;h3 id=&#34;io_uring&#34;&gt;io_uring&lt;/h3&gt;
&lt;p&gt;io_uring是Linux的一个系统调用，可以批量提交异步I/O，并且相较于aio减少了系统调用的次数，作者通过实验验证了在NVMe Driver层提交I/O也可以提高io_uring的性能。&lt;/p&gt;
&lt;h1 id=&#34;0x02-challenges--principles&#34;&gt;0x02 Challenges &amp;amp;&amp;amp; Principles&lt;/h1&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;地址转换和安全问题：NVMe Driver不能访问文件系统的元数据，因此不能完成逻辑地址到物理地址的转换；BPF可以访问其他文件和用户的任何block。&lt;/li&gt;
&lt;li&gt;并发和缓存问题：对于从文件系统发起的并发读写很难维护。从文件系统发出的写请求将写入到page cache中，对于XRP不可见；同时对于硬盘上数据结构布局的修改，例如改变某一个指针，将会导致XRP获取到错误的数据，虽然可以进行加锁，但是从NVMe中断访问锁的开销太大。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;observation&#34;&gt;Observation&lt;/h2&gt;
&lt;p&gt;作者研究发现，大部分的存储引擎的硬盘数据结构都是稳定的，或不会进行就地更新。&lt;/p&gt;
&lt;p&gt;例如在LSM-tree中，进行索引的写入操作，写入到文件&lt;code&gt;SSTables&lt;/code&gt;中，这些文件为不可变，直至其被删除，不可变文件还降低了文件并发访问的成本。B-tree的索引虽然可以就地更新，但是在实际测试中（24小时YCSB读写改实验）发现，索引的修改次数很少，因此也不需要在NVMe Driver中频繁的更新文件系统的元数据。&lt;/p&gt;
&lt;p&gt;同时，索引一般存储在少量的大文件中，并且每个索引不会跨越多个文件。&lt;/p&gt;
&lt;h2 id=&#34;design-principles&#34;&gt;Design Principles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;一次只访问一个文件：可以简化地址转换和访问控制，还最小化了需要传递给NVMe driver的元数据&lt;/li&gt;
&lt;li&gt;面向于稳定的数据结构：XRP以不会频繁更新的数据结构为目标RocksDB、LevelDB、TokuDB、WiredTiger，不支持需要锁来访问的数据结构&lt;/li&gt;
&lt;li&gt;用户管理缓存：XRP无法直接访问Page Cache，因此如果block位于Page Cache中，XRP函数不能安全的并发执行，而大多数存储引擎都是在用户空间自行管理Cache。&lt;/li&gt;
&lt;li&gt;错误处理：如果访问失败（映射信息过期），需要程序在用户空间重试或回滚。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x03-design--implementation&#34;&gt;0x03 Design &amp;amp;&amp;amp; Implementation&lt;/h1&gt;
&lt;p&gt;XRP的架构图：&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225161425201.png&#34; alt=&#34;image-20230225161425201&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;resubmission-logic&#34;&gt;Resubmission Logic&lt;/h2&gt;
&lt;p&gt;XRP的Resubmission主要包含3个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BPF hook&lt;/li&gt;
&lt;li&gt;文件系统地址翻译&lt;/li&gt;
&lt;li&gt;NVMe请求的构建和提交&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当NVMe指令请求完成后，设备发起中断使内核发生上下文切换执行中断处理，对于在&lt;code&gt;中断上下文&lt;/code&gt;(即ISR中断处理程序保存的请求发起程序的上下文)每个NVMe请求，XRP调用相关的BPF函数，即上图的&lt;code&gt;bpf_func_0&lt;/code&gt;，该函数指针存储在内核I/O请求的结构体中（bio）。&lt;/p&gt;
&lt;p&gt;在调用BPF函数后，XRP调用元数据信息摘要，此文件系统摘要用于做地址转换。&lt;/p&gt;
&lt;p&gt;最终，XRP发起下一次NVMe请求，将请求append到相应CPU核心NVMe的SQ中完成resubmission（如上图所示）。&lt;/p&gt;
&lt;h3 id=&#34;bpf-hook&#34;&gt;BPF Hook&lt;/h3&gt;
&lt;p&gt;XRP引入&lt;code&gt;BPF_PROG_TYPE_XRP&lt;/code&gt;作为BPF函数的签名（函数签名一般包括函数的参数类型、个数等），符合该签名的程序可以被BPF Hook调用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225182708553.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230225182708553&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;参数解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;：存储从硬盘中读取的信息（例如B-tree Page），可以被BPF函数解析&lt;/li&gt;
&lt;li&gt;&lt;code&gt;done&lt;/code&gt;：在resubmission阶段通过此字段判断是否将数据上报给用户程序，或是进行后续I/O&lt;/li&gt;
&lt;li&gt;&lt;code&gt;next_addr&lt;/code&gt;：表示下一次resubmission要请求的逻辑地址，最大输入量fanout默认限制在16&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size&lt;/code&gt;：表示&lt;code&gt;next_addr&lt;/code&gt;逻辑地址的大小，某些情况下可以将其设置为0表示不发起I/O&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scratch&lt;/code&gt;：对于用户和BPF函数私有，用于向BPF函数传递用户的参数，BPF函数也可以在此存储两次I/O之间的中间变量，亦或是保存传递给用户的数据。相当于BPF可以利用的一个4KB的buffer。如果BPF函数需要更大的空间，还可以使用BPF maps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BPF上下文对于一个NVMe请求是私有的，不需要考虑加锁。为用户提供scratch buffer避免了调用&lt;code&gt;bpf_map_lookup_elem&lt;/code&gt;来访问buffer产生的开销。&lt;/p&gt;
&lt;h3 id=&#34;bpf-verifier&#34;&gt;BPF Verifier&lt;/h3&gt;
&lt;p&gt;BPF校验器通过追踪&lt;code&gt;寄存器中value的语义&lt;/code&gt;来保证内存安全。&lt;/p&gt;
&lt;p&gt;一个有效的value既可以是一个标量也可以是一个指针。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SCALAR_TYPE&lt;/code&gt;表示一个不能被指针取值(cannot be dereference)的value。&lt;/p&gt;
&lt;p&gt;verifier定义了多种指针类型。&lt;/p&gt;
&lt;p&gt;每个BPF函数都定义了一个回调函数&lt;code&gt;is_valid_access()&lt;/code&gt;来检查上下文访问是否正确并返回上下文中value type字段。&lt;/p&gt;
&lt;p&gt;PTR_TO_MEM表示一个指向固定内存大小的指针，支持直接通过offset取值（不超过边界的情况下），&lt;code&gt;BPF_PROG_TYPE_XRP&lt;/code&gt;的data和scratch字段通过&lt;code&gt;PTR_TO_MEM&lt;/code&gt;访问，其余部分则为SCALAR_TYPE。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;is_valid_access()&lt;/code&gt;还可以用来传递data/scratch buffer的大小，用于做边界检查。&lt;/p&gt;
&lt;h3 id=&#34;metadata-digest&#34;&gt;Metadata Digest&lt;/h3&gt;
&lt;p&gt;为了完成在NVMe Driver中的地址转换，XRP在文件系统和中断处理程序之间添加了一个接口。文件系统共享逻辑地址到物理地址的映射。&lt;/p&gt;
&lt;p&gt;元数据摘要共有两个函数，如下图所示。&lt;/p&gt;
&lt;p&gt;当地址映射被更新时，&lt;strong&gt;文件系统&lt;/strong&gt;将调用update函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中断处理程序&lt;/strong&gt;则负责调用lookup函数，通过接收到的inode地址，给出offset和长度获取地址映射；lookup中也会有越界检查来保证安全性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230225205022040.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230225205022040&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;当检测到不合法的逻辑地址，XRP将立即返回错误代码到用户空间。&lt;/p&gt;
&lt;p&gt;作者在ext4文件系统的实现中，元数据摘要来自缓存中的extent status tree，使用RCU机制进行并发控制。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;太长不看：&lt;/p&gt;
&lt;p&gt;RCU是一种类似于COW写时复制的机制，因为读取操作不需要加锁，在保证数据一致性的同时可以保证高性能，RCU主要用于Linux内核中对高并发访问数据结构的优化，而写时复制主要用于内存管理、文件系统等领域。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linux RCU：&lt;/p&gt;
&lt;p&gt;Linux RCU（Read-Copy-Update）是一种用于管理并发访问共享数据结构的技术。它是一种无锁并发机制，允许多个线程同时读取共享数据，而不需要互斥锁或读写锁等传统的同步机制，从而提高了并发性能和可伸缩性。&lt;/p&gt;
&lt;p&gt;RCU的基本思想是，当一个线程需要更新共享数据时，它将数据复制一份，并在副本上进行修改，而不是在原始数据上进行修改。其他线程仍然可以访问原始数据，直到更新完成。在更新完成后，新的数据副本将替换原始数据，并且RCU机制将确保在该更新之前任何已经读取的数据都不会丢失。&lt;/p&gt;
&lt;p&gt;RCU适用于高并发访问数据结构的场景，例如哈希表、链表和树等数据结构。它已经被广泛应用于Linux内核中，使得Linux内核在多处理器系统上的性能得到显著提升。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;ext4中，extents扩展块是文件系统中一段连续的块，一般用于存储大文件的数据信息。&lt;/p&gt;
&lt;p&gt;为了保证缓存数据与ext4中的extents一致，update函数会在extents发生修改或者删除时调用。同时为了防止竞态，为每个扩展块维护了一个版本号。&lt;/p&gt;
&lt;p&gt;在数据读取后，但还没有传递给BPF函数之前，会先调用元数据摘要的查找。如何相应的extents已经被删除或者当前版本号已经落后，XRP将中断此次操作。&lt;/p&gt;
&lt;p&gt;一般而言，为保障安全和正确性，在程序之间进行同步时就会对文件中一个部分的加锁，因此版本号的改变大多来自于恶意修改或程序出错。&lt;/p&gt;
&lt;p&gt;一种更简单的实现利用ext4文件系统现有的extent tree更新和访问函数来获取元数据摘要，这会保证ext4中的extent tree时刻是最新状态，但是在查找ext4中extent查找函数会获取自旋锁，对于中断处理程序带来了极大的开销，因此不太合适。&lt;/p&gt;
&lt;p&gt;目前XRP仅支持了ext4文件系统。对于F2FS可以直接利用存储了逻辑物理地址块映射的Node Address Table（NAT），通过保存一个NAT的副本来得到元数据摘要。每次更新NAT都调用&lt;code&gt;update_mapping&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;resubmitting-nvme-request&#34;&gt;Resubmitting NVMe Request&lt;/h3&gt;
&lt;p&gt;在查询到数据块的物理地址后，XRP将准备提交NVMe请求。&lt;/p&gt;
&lt;p&gt;XRP重复利用刚刚完成的NVMe请求结构，修改NVMe请求中的物理扇区和块地址为通过lookup函数查询到的偏移量，然后继续进行请求。&lt;/p&gt;
&lt;p&gt;由于bpf_xrp支持的最大地址个数为16，目前的resubmit只能获取到和初始NVMe请求一样多的物理段。也就是第一个NVMe请求只申请了一个物理段，那么后续的NVMe请求也只能去获取一个物理段。&lt;/p&gt;
&lt;p&gt;XRP会中断包含无效地址的BPF调用，可以在第一次I/O中next_addr中设置16个伪NVMe指令，以保证后续的请求可以有足够的访问空间。&lt;/p&gt;
&lt;h2 id=&#34;synchronization-limitations&#34;&gt;Synchronization Limitations&lt;/h2&gt;
&lt;p&gt;BPF目前只支持自旋锁进行同步。&lt;/p&gt;
&lt;p&gt;Verifier只允许BPF程序一次只获取一个锁，并且在函数退出前要将其释放。&lt;/p&gt;
&lt;p&gt;用户程序不需要直接去访问BPF的自旋锁，可以直接进行BPF的系统调用，该系统调用可以在持有锁时对保护的数据结构进行读写。&lt;/p&gt;
&lt;p&gt;需要在多个读写操作之间进行同步的复杂操作在用户空间无法完成。&lt;/p&gt;
&lt;p&gt;用户可以通过BPF的原子操作实现自定义的自旋锁，这允许BPF函数和用户程序直接获取任何自旋锁。然而BPF函数的行为会被限制，防止无限地等待某一个自旋锁。&lt;/p&gt;
&lt;p&gt;另一种同步方法就是RCU。XRP BPF都运行在NVMe中断处理程序上，而中断处理程序不能被抢占，可以认为他们已经进入了RCU读端临界区（a RCU read-side critical section）的状态，因为在中断处理程序执行时，其他任务无法访问共享数据结构，从而避免了并发冲突。因此，在这种情况下，BPF程序不需要获得锁来读取共享数据结构，从而避免了锁的开销，并提高了程序的执行效率。&lt;/p&gt;
&lt;h2 id=&#34;linux-schedulers&#34;&gt;Linux Schedulers&lt;/h2&gt;
&lt;h3 id=&#34;进程调度&#34;&gt;进程调度&lt;/h3&gt;
&lt;p&gt;作者研究发现根据Linux的CFS完全公平调度算法，当计算密集型的进程和I/O密集型的进程运行在一个核心时，由于超低延迟SSD的中断过于频繁，使得I/O密集型应用产生的I/O中断占用大量时间片，导致计算密集型进程的饥饿。而在普通SSD上，由于中断产生的频率较低，不会发生此现象。&lt;/p&gt;
&lt;p&gt;连续产生中断较多的XRP、网络中断较多的程序会加剧（exarcerbates）此问题，作者计划在之后考虑此问题。&lt;/p&gt;
&lt;h3 id=&#34;io调度&#34;&gt;I/O调度&lt;/h3&gt;
&lt;p&gt;NVMe默认使用noop调度器，如果需要公平调度，可以开启硬件队列总裁。&lt;/p&gt;
&lt;h1 id=&#34;0x04-case-studies&#34;&gt;0x04 Case Studies&lt;/h1&gt;
&lt;h2 id=&#34;use-xrp&#34;&gt;Use XRP&lt;/h2&gt;
&lt;p&gt;用户调用下图两个接口来加载BPF函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bpf_prog_load&lt;/code&gt;：加载&lt;code&gt;BPF_PROG_TYPE_XRP&lt;/code&gt;类型的BPF函数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;read_xrp&lt;/code&gt;：将调用BPF函数应用到一个具体的请求上&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230226143117688.png&#34; alt=&#34;image-20230226143117688&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;p&gt;具体的使用实例，作者给出了两个示例。&lt;/p&gt;
&lt;h2 id=&#34;bpf-kv&#34;&gt;BPF-KV&lt;/h2&gt;
&lt;p&gt;作者使用B+树在磁盘组织数据，同时自行维护DRAM Cache来对块和对象进行索引。&lt;/p&gt;
&lt;p&gt;最大请求数量限制为31，保证可以遍历一个节点的所有31个孩子。&lt;/p&gt;
&lt;p&gt;BPF函数用于查找KV对，首先获取在scratch buffer中的查询目标key，并线性地在node中查找。&lt;/p&gt;
&lt;p&gt;使用&lt;code&gt;read_xrp&lt;/code&gt;来实现read系统调用，BPF-KV首先为scratch buffer分配一块空间，并计算出offset来启动read。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230226152454999.png&#34; alt=&#34;image-20230226152454999&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;BPF-KV支持范围查询，查询过程中的数据保存在scratch buffer中。&lt;/p&gt;
&lt;p&gt;BPF-KV支持SUM、MAX、MIN此类操作&lt;/p&gt;
&lt;h2 id=&#34;wiredtiger&#34;&gt;WiredTiger&lt;/h2&gt;
&lt;p&gt;WiredTiger是MongoDB的默认存储引擎，可以使用LSM-tree作为数据结构，数据保存于LSM的不同level之间，每个level保存在一个文件中，没个文件使用B-tree索引。所有的文件为只读，更新和插入操作先写入如buffer中，buffer空间不足时保存到一个新的文件中。&lt;/p&gt;
&lt;p&gt;XRP只针对read操作做优化，update和insert在buffer中维护。&lt;/p&gt;
&lt;p&gt;WT的BPF函数和BPF-KV差不多，WT需要对B-tree的Page做解析操作。&lt;/p&gt;
&lt;h1 id=&#34;0x05-thinking&#34;&gt;0x05 Thinking&lt;/h1&gt;
&lt;p&gt;XRP是OSDI&#39;22的best paper，文章整体的工作量非常大，修改了ext4文件系统、BPF模块，并设计了优化后的KV存储引擎。文章从很新颖的角度优化了数据库引擎等I/O密集型应用，不需要频繁传递数据于用户空间和NVMe Driver时间，也减少了I/O过程中在轮询时的开销，提高了CPU利用率，通过BPF这一内核特性可以实现自定义I/O函数，同时保证一定的安全性和隔离。&lt;/p&gt;
&lt;h1 id=&#34;0x06-reference--more-information&#34;&gt;0x06 Reference &amp;amp;&amp;amp; More information&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LSM 不可修改文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kernel.org/doc/html/latest/RCU/whatisRCU.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org】 RCU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://draveness.me/mongodb-wiredtiger/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【draveness】MongoDB和Wiretiger存储引擎&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>[FAST&#39;22] MT^2: Memory Bandwidth Regulation on Hybrid NVM/DRAM Platforms</title>
        <link>https://blog.ipandai.club/p/fast22-mt2-memory-bandwidth-regulation-on-hybrid-nvm/dram-platforms/</link>
        <pubDate>Mon, 19 Dec 2022 16:50:00 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/fast22-mt2-memory-bandwidth-regulation-on-hybrid-nvm/dram-platforms/</guid>
        <description>&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;NVM和DRAM共享内存总线，二者之间的负载相互干扰，给混合NVM/DRAM平台的带宽分配带来了挑战&lt;/p&gt;
&lt;p&gt;本文提出了MT2，可以在混合NVM/DRAM平台中管理并发程序间的内存带宽。&lt;/p&gt;
&lt;p&gt;MT2首先检查内存流量间的干扰并通过硬件监视器和软件汇报从混合流量监控不同类型的内存带宽&lt;/p&gt;
&lt;p&gt;然后MT2利用一个动态的带宽流量调节算法基于多种方式来管理内存带宽&lt;/p&gt;
&lt;p&gt;为了能够更好的管理不同程序，MT2被集成到了cgroup中，添加了一个用于带宽分配的cgroup subsystem&lt;/p&gt;
&lt;p&gt;新兴的NVM存储器逐渐被用来做为可持久化内存来使用，基于NVM，提出了NVM文件系统，NVM编程库，NVM数据结构、NVM数据库，NVM作为大容量内存或者快速的字节寻址存储设备用于数据中心。&lt;/p&gt;
&lt;p&gt;然而NVM/DRAM混合平台加剧了吵闹邻居问题。在云存储环境中，多个用户常常共享一个服务器，不同用户的不同应用程序共享主机的一条总线，某些应用可能会过度使用内存带宽。在NVM/DRAM中，NVM和DRAM共享同一个总线，因此不同的应用程序将竞争有限的内存带宽，影响整体的性能&lt;/p&gt;
&lt;p&gt;在NVM/DRAM中调节带宽有如下几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;内存带宽不对称，在NVM/DRAM，不同的内存访问（如DRAM read，DRAM write，NVM read和NVM write）会产生不同的最大内存带宽。内存的实际可用带宽主要取决于负载中不同类型访问的占比。同时设置静态的内存带宽而不考虑实际的I/O访问占比是不正确的做法。同时NVM最大带宽通常小于DRAM。而且不同类型的内存访问的干扰程度不同，因此不能单纯将所有内存==访问延迟==视为相等&lt;/li&gt;
&lt;li&gt;NVM和DRAM共享内存总线，NVM流量和DRAM流量不可避免地会混合并且很难区分。对于混合的流量，监控不同种类的内存带宽。由于内存流量混合，几乎不可能在每个进程的基础上监控不同类型的内存带宽，这使为DRAM设计的现有硬件和软件调节方法无效。&lt;/li&gt;
&lt;li&gt;内存调节的硬件和软件机制不足。由于NVM和DRAM都可以通过CPU负载/存储指令直接访问，因此为了性能，对每个内存访问进行计算和限流是不切实际的。CPU供应商，如英特尔，支持硬件机制来调节内存带宽。然而，带宽限制是粗粒度和定性迭代的，这不足以精确的内存带宽调节。其他一些方法，如频率缩放和CPU调度，可能会提供相对细粒度的带宽调整。然而，它们也是定性的，并减慢了计算和内存访问的速度，因此对整个平台性能缺乏影响。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MT2作为Linux Cgroup中的一个Subsystem，用于减轻吵闹邻居问题。在内存带宽分配和云SLO保证方面提高效率。本文的主要贡献：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;发现了导致NVM/DRAM混合平台上内存密集型应用程序显著性能流失的内存带宽干扰问题&lt;/li&gt;
&lt;li&gt;首次对在NVM/DRAM平台现存的硬件和软件带宽分配机制进行研究&lt;/li&gt;
&lt;li&gt;高效有效地调节具有线程级粒度的NVM/DRAM混合平台上的内存带宽&lt;/li&gt;
&lt;li&gt;在英特尔Optane SSD上进行了测试和分析&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以下简称NVM/DRAM&lt;/p&gt;
&lt;h1 id=&#34;0x01-bg&#34;&gt;0x01 BG&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;MT^2&lt;/strong&gt; 持久内存（PM）/ 非易失性内存（NVM）的出现正改变着存储系统的金字塔层次结构。本文发现，由于 NVM 和 DRAM 共享同一条内存总线，带宽干扰问题变得更为严重和复杂，甚至会显著降低系统的总带宽。本工作介绍了对内存带宽干扰的分析，对现有软硬件技术进行了深入调研，并提出了一种在 NVM/DRAM 混合平台上监控调节并发应用的内存带宽的设计（MT^2）。MT^2 以线程为粒度准确监测来自混合流量的不同类型的内存带宽，使用软硬件结合技术控制内存带宽。在多个不同的用例中，MT^2 能够有效限制“吵闹邻居”（noisy neighbors），消除带宽干扰，保证高优先级应用的性能。&lt;/p&gt;
&lt;h2 id=&#34;noisy-neighbors&#34;&gt;Noisy Neighbors&lt;/h2&gt;
&lt;p&gt;在多租户的云环境中，内存带宽对应用程序的性能有很大影响。&lt;/p&gt;
&lt;p&gt;两种可以减轻吵闹邻居问题的方法是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prevention：主动为应用程序设置带宽限制（固定的）&lt;/li&gt;
&lt;li&gt;Remedy：系统检测吵闹邻居情况的出现并识别出吵闹的“邻居”对其进行限制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些方法需要去监控应用的带宽使用情况&lt;/p&gt;
&lt;h2 id=&#34;nvm&#34;&gt;NVM&lt;/h2&gt;
&lt;p&gt;NVM得益于其存储容量大，访问速度快的特性，正在逐步作为内存使用于商业服务器中。得益于PMDK（Persistent Memory Development Kit）同时还涌现出了大量基于NVM Memory的应用程序，如PmemKV，Pmem-RocksDB。&lt;/p&gt;
&lt;p&gt;NVM可以直接通过CPU的load/store指令进行访问&lt;/p&gt;
&lt;h2 id=&#34;memory-bandwidth-interference&#34;&gt;Memory Bandwidth Interference&lt;/h2&gt;
&lt;p&gt;由于NVM和DRAM共享内存总线，因此存在干扰问题。&lt;/p&gt;
&lt;p&gt;用两个Flexible I/O Tester来竞争总带宽进行测试，包括NVM/DRAM的读写，分别比较在不同的访问方式下的带宽干扰情况&lt;/p&gt;
&lt;p&gt;如下图所示，颜色越深表明干扰越明显，图中数据表示为在Task B运行的情况下Task A的吞吐量占Task Alone情况下的百分比（GB/s）&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223132031882.png&#34; alt=&#34;image-20230223132031882&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;从图中得出的两个结论为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;内存的干扰和内存的访问情况有关，占据更小带宽的内存访问可能会对其他访问造成更大的影响&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NVM的访问比DRAM访问对其他任务的影响更大，例如图中NVM Read列和DRAM Read列的对比，说明执行NVM write较多的应用更可能成为影响其他任务&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者还做了Task B进行NVM Write，Task A的延迟和吞吐量的变化，发现了&lt;strong&gt;随着带宽的逐渐降低，Task A的延迟逐渐增加。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;memory-bandwidth-monitoring&#34;&gt;Memory Bandwidth Monitoring&lt;/h2&gt;
&lt;p&gt;Intel的MBM技术可以用来监控从L3 Cache到下一级内存系统的带宽使用情况，即NVM或者DRAM，为每个逻辑核心都提供了硬件级别的内存带宽测量。&lt;/p&gt;
&lt;p&gt;每个逻辑核心分配了一个Resource Monitoring ID（RMID），一组逻辑核心可以分配相同的RMID，底层的硬件通过追踪具有相同RMID的内存带宽并将它们进行汇总。&lt;/p&gt;
&lt;h2 id=&#34;memory-bandwidth-allocation&#34;&gt;Memory Bandwidth Allocation&lt;/h2&gt;
&lt;p&gt;Intel MBA技术提供间接和粗略内存带宽控制，基本无性能损失。&lt;/p&gt;
&lt;p&gt;MBA在每个物理核心和共享L3 Cache之间引入了一个可编程的请求速率控制器，控制器通过在内存请求之间插入延迟来对内存带宽的使用进行限流。&lt;/p&gt;
&lt;p&gt;Intel要求使用Classes of Service（CLOS）为线程进行分组，然后为每个CLOS设置一个限流值。&lt;/p&gt;
&lt;h1 id=&#34;0x02-design&#34;&gt;0x02 Design&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221223002925874.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221223002925874&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，MT2工作在Kernel space，可以更方便高效地访问硬件信息。还能与内核其他组件高效的通信。&lt;/p&gt;
&lt;p&gt;MT2对user space暴露了相关管理接口，系统管理员可以将线程分配到不同的group并且制定各个group所使用的带宽，称为TGroup，Throttling Group&lt;/p&gt;
&lt;p&gt;MT2由两个部分组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor，负责从VFS，Performance Monitoring Unit（PMU），MBM收集性能数据，并将其分为四种类型DRAM/NVM R/W，将信息转发到Regulator&lt;/li&gt;
&lt;li&gt;Regulator，根据监控的数据和策略，执行两种机制：
&lt;ul&gt;
&lt;li&gt;调整MBA限流值&lt;/li&gt;
&lt;li&gt;调整CPU配额&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MT2使用动态带宽限流算法，根据实时带宽和干扰级别不断监控和调整限制。&lt;/p&gt;
&lt;p&gt;MT2提供两种缓解吵闹邻居的方案，来适应不同场景，如上文所示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prevention：系统管理员为每个TGroup设置带宽上限，MT2监控实时带宽并令所有group不会使用超过设定值的带宽。&lt;/li&gt;
&lt;li&gt;Remedy：对于没有超过带宽限制，但是仍然有很大影响的TGroup，可以由系统自动进行限制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;划分TGroup之后，管理员限制每个TGroup的最大带宽，监控器会保证每个TGroup的带宽不会超过阈值&lt;/p&gt;
&lt;p&gt;还要保证在大部分线程没有超过阈值时，产生的I/O干扰问题（空闲状态）&lt;/p&gt;
&lt;h2 id=&#34;monitor&#34;&gt;Monitor&lt;/h2&gt;
&lt;h3 id=&#34;带宽估计&#34;&gt;带宽估计&lt;/h3&gt;
&lt;p&gt;monitor需要获得不同种类I/O准确的带宽，例如DRAM的read/write带宽，NVM的read/write带宽。Regulator使用这些信息来决定是否要限制每个TGroup的内存带宽。&lt;/p&gt;
&lt;p&gt;对于NVM/DRAM的read带宽，可以直接通过PMU进行获取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVM Read Bandwidth：&lt;code&gt;ocr.all_data_rd.pmm_hit_local_pmm.any_snoop&lt;/code&gt; 需要通过PMU事件计数器来检索本地NVM read的数量并乘上cache line size 即64B来得出总带宽&lt;/li&gt;
&lt;li&gt;DRAM Read Bandwidth：&lt;code&gt;ocr.all_data_rd.l3_miss_local_dram.any_snoop&lt;/code&gt; DRAM PMU counter进行统计&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于Write的统计无法通过PMU，需要利用MBM来监控每个TGroup的总内存访问带宽，为DRAM read/write Bandwidth 和 NVM read/write Bandwidth的总和。二者的Read带宽可以很容易计算出来。&lt;/p&gt;
&lt;p&gt;只需通过周期性地收集NVM write的数量，就可以计算出一个TGroup总NVM write Bandwidth&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User-Space对于NVM的write只能通过文件系统API和通过MMap后的CPU store指令操作内存来实现，对于文件系统API可以hook内核VFS并监控每个TGroup NVM write 的数量。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于MMap的访问，作者基于app是否受信提出了两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于受信任的app（来自受信用户或者公司企业），使用他们来辅助收集使用mmap方式对NVM进行的write数量。作者提出了修改后的PMDK，调用PMDK的API刷新Cache lines 到NVM或执行永久地内存写入（movnt），通过计算并统计每个线程NVM write的数量到计数器中。为了要将计数器报告给MT2，每个进程都设置一个与内核共享的页面，进程中的每个线程将其每个线程计数器的值写入页面中的不同位置。内核中的MT2定期检查计数器，并计算每个TGroup的带宽。基于PMDK的应用无序进行修改，其他应用需要对源码进行少量修改。最终根据收集的NVM write信息，继续计算出DRAM的write 带宽&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
BW_{DW}=BW_{Total}-BW_{DR}-BW_{NR}-BW_{NW}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于不受信的app（可能不会按照规定上传NVM write带宽信息），只能进行粗略估计。利用PEBS（Processor Event Based Sampling 处理器事件采样），用于对每个TGroup带有目的地址的内存write操作进行采样，通过比较NVM的地址范围，可以计算出NVM和DRAM中采样的write操作比例，对NVM和DRAM write进行粗略估计。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;干扰检测&#34;&gt;干扰检测&lt;/h3&gt;
&lt;p&gt;MT2通过测量不同类型内存访问的延迟来检测干扰级别，因为内存访问延迟与带宽负相关&lt;/p&gt;
&lt;p&gt;MT2通过四个性能参数得到read延迟， &lt;code&gt;unc_m_pmm_rpq_occupancy.all &lt;/code&gt;($RPQ_O$), &lt;code&gt;unc_m_pmm_rpq_inserts&lt;/code&gt; ($RPQ_1$), &lt;code&gt;unc_m_rpq_occupancy&lt;/code&gt;, and &lt;code&gt;unc_m_rpq_inserts&lt;/code&gt;，通过$\frac{RPQ_O}{RPQ_1}$来计算NVM read延迟，同理得到DRAM read延迟&lt;/p&gt;
&lt;p&gt;MT2周期性地发起NVM和DRAM write请求，以此来测量完成时间，进而获得延迟&lt;/p&gt;
&lt;p&gt;通过设置阈值来判断带宽是否发生干扰&lt;/p&gt;
&lt;p&gt;干扰检测伪代码&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;detect_interference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bt&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bandwidth_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;latency&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;THRESHOLD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;作者使用吞吐量降低10%的延迟作为阈值&lt;/p&gt;
&lt;h2 id=&#34;regulator&#34;&gt;Regulator&lt;/h2&gt;
&lt;h3 id=&#34;内存调节机制&#34;&gt;内存调节机制&lt;/h3&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223164328178.png&#34; alt=&#34;image-20230223164328178&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;MBA&lt;/strong&gt;，通过设置限流值来对程序进行限制。MBA只支持throttling values，不能进行太精细的控制，MBA对DRAM敏感的负载的限制效果更好，对NVM-write的限制几乎没有效果，如上图b。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CPU调度&lt;/strong&gt;，可以进行更细粒度的内存带宽调整，利用Linux Cgroup CPU子系统来对应用使用的CPU核心进行限额。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有效性和比较&lt;/strong&gt;，作者通过对比测试后发现，CPU限流会影响应用程序的其他任务，限制其整体的效率，而MBA进对内存的访问进行限制，对程序整体的运行影响较小。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223170600388.png&#34; alt=&#34;image-20230223170600388&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;h3 id=&#34;动态带宽限流&#34;&gt;动态带宽限流&lt;/h3&gt;
&lt;h4 id=&#34;识别吵闹邻居&#34;&gt;识别吵闹邻居&lt;/h4&gt;
&lt;p&gt;Prevention策略，所有超过带宽上限的TGroup都被视为吵闹邻居&lt;/p&gt;
&lt;p&gt;Remedy策略，Regulator首先根据Monitor提供的信息检查是否存在严重的内存干扰，当内存干扰情况严重时，算法会根据每个TGroup的带宽使用情况识别吵闹邻居，根据上文的分析，包含NVM write操作最多的TGroup最有可能是吵闹邻居，因此算法要选取这类TGroup并进行限流。&lt;/p&gt;
&lt;h4 id=&#34;调节内存带宽&#34;&gt;调节内存带宽&lt;/h4&gt;
&lt;p&gt;算法会根据内存访问的类型来选取调节方法。&lt;/p&gt;
&lt;p&gt;对于NVM访问带宽的限制，算法采用CPU限制&lt;/p&gt;
&lt;p&gt;对于DRAM访问带宽的限制，算法首先削减其TGroup的MBA，如果MBA已经设定为最小值，算法将使用CPU限流来进一步调节。&lt;/p&gt;
&lt;h4 id=&#34;释放内存限制&#34;&gt;释放内存限制&lt;/h4&gt;
&lt;p&gt;一旦内存干扰消失，算法就会尝试放宽带宽限制，操作过程和施加内存限制的方式相反。&lt;/p&gt;
&lt;p&gt;当Regulator结束了一次调节/释放操作时，就会继续等待下一步操作，取决于Monitor提供的信息。&lt;/p&gt;
&lt;p&gt;整体的工作流程在作者的Slides上有一张图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230223191251147.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230223191251147&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x03-implementation&#34;&gt;0x03 Implementation&lt;/h1&gt;
&lt;h2 id=&#34;cgroup接口&#34;&gt;Cgroup接口&lt;/h2&gt;
&lt;p&gt;Cgroup是Linux内核中用于资源限制的工具，常用于容器化技术，目前正在整理有关内容计划分享一下。&lt;/p&gt;
&lt;p&gt;Cgroup通过暴露伪文件系统的文件实现其接口，Cgroupfs，用户通过修改文件的内容完成配置工作，MT2遵循Cgroup规范添加了TGroup，用于限流和管理的主要有以下几个文件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;priority&lt;/code&gt;：用于设置TGroup的优先级，包括&lt;code&gt;high&lt;/code&gt;和&lt;code&gt;low&lt;/code&gt;，&lt;code&gt;high&lt;/code&gt;优先级的TGroup不会被Regulator限制，&lt;code&gt;low&lt;/code&gt;优先级则会在内存干扰情况发生时被限制。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bandwidth&lt;/code&gt;：该文件只读，返回TGroup当前的带宽使用情况&lt;/li&gt;
&lt;li&gt;&lt;code&gt;limit&lt;/code&gt;用于获取和设置TGroup的绝对带宽，四个逗号分隔的数字来代表四种内存访问带宽上限，一旦带宽超过该值TGroup就会被限流。Zero Value表示无限制，该配置立即生效。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;线程创建问题&#34;&gt;线程创建问题&lt;/h2&gt;
&lt;p&gt;所有子进程都与父进程放在同一个TGroup中，除非管理员手动将其放入另一个TGroup。为此作者在进程/线程创建程序中添加了一个hook，即Linux内核中的&lt;code&gt;fork&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;mba&#34;&gt;MBA&lt;/h2&gt;
&lt;p&gt;MBA支持10个限流值，由于70、80、90的效果较差，作者只使用其中的8个值，并将每个值分配到一个CLOS中，将TGroup分配到相应的CLOS来实现不同等级的限流。在没有干扰的情况下，具有相同CLOS的TGroups有相同的请求速率。&lt;/p&gt;
&lt;h2 id=&#34;上下文切换&#34;&gt;上下文切换&lt;/h2&gt;
&lt;p&gt;为了给每个线程设置MT2上下文，包括设置PMU相关的上下文、写入与MBA相关的MSR寄存器以及设置CPU配额，作者给调度器添加了一个hook。每次发生上下文切换时，我们就为将要在这个CPU核上运行的新线程设置相应的MT2上下文。&lt;/p&gt;
&lt;h2 id=&#34;pmu&#34;&gt;PMU&lt;/h2&gt;
&lt;p&gt;用于计算没有命中Cache而进行NVM和DRAM访问的所有read指令。对于DRAM和NVM的read操作延迟也通过PMU获取。&lt;/p&gt;
&lt;h2 id=&#34;pebs&#34;&gt;PEBS&lt;/h2&gt;
&lt;p&gt;采样频率为10007，PEBS将每10007个事件记录一个线性地址，不会对系统产生太大的性能影响。&lt;/p&gt;
&lt;h2 id=&#34;专有内核线程&#34;&gt;专有内核线程&lt;/h2&gt;
&lt;p&gt;在MT2内核模块的初始化阶段创建一个内核线程，定期检测内存干扰，监控带宽并执行动态带宽限流。&lt;/p&gt;
&lt;p&gt;内核线程的伪代码如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;kthread_main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;current_time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;interference&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;detect_interference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TGroups&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;aggregate_bandwidths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;adjust_bandwidths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;interference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;sleep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;INTERVAL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;current_time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;0x04-thinking&#34;&gt;0x04 Thinking&lt;/h1&gt;
&lt;p&gt;作者在内核中实现了线程粒度的内存带宽监控，同时利用了多种方法来实现内存带宽的限流。&lt;/p&gt;
&lt;p&gt;同时利用了分析DRAM/NVM读写带宽之间的相互影响，从而确定了优化的方向和方法。&lt;/p&gt;
&lt;p&gt;本文章来自SJTU的IPADS实验室，文章中的论述非常有条理，采取的每一项措施都有实验验证，有例可循，例如选取NVM write操作较多的TGroup作为干扰项，以及MBA和CPU限额综合调整带宽，都有相应的实验分析。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>[ACM Trans. On Storage] HintStor: A Framework to Study I/O Hints in Heterogeneous Storage</title>
        <link>https://blog.ipandai.club/p/acm-trans.-on-storage-hintstor-a-framework-to-study-i/o-hints-in-heterogeneous-storage/</link>
        <pubDate>Wed, 30 Nov 2022 10:58:22 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/acm-trans.-on-storage-hintstor-a-framework-to-study-i/o-hints-in-heterogeneous-storage/</guid>
        <description>&lt;h1 id=&#34;0x00-everyday-english&#34;&gt;0x00 Everyday English&lt;/h1&gt;
&lt;p&gt;Heterogeneous 各种各样的&lt;/p&gt;
&lt;p&gt;semantic 语义上的&lt;/p&gt;
&lt;p&gt;conservative 保守的、传统的 &amp;mdash;aggressive 激进的&lt;/p&gt;
&lt;p&gt;aggregate 合计&lt;/p&gt;
&lt;p&gt;orchestrate 精心策划&lt;/p&gt;
&lt;p&gt;proactive 积极主动的&lt;/p&gt;
&lt;p&gt;by means of 通过，借助于&lt;/p&gt;
&lt;p&gt;off-the-shelf 现成的&lt;/p&gt;
&lt;p&gt;elaborate 详细阐述&lt;/p&gt;
&lt;h1 id=&#34;0x01-intro&#34;&gt;0x01 Intro&lt;/h1&gt;
&lt;p&gt;随着存储技术的发展，由SCM、SSD、HDD以及一系列云存储构成了目前异构存储系统&lt;/p&gt;
&lt;p&gt;异构存储系统将冷数据放在更慢的层次，热数据放在更快的层次，存储机制的激进和保守都会带来一些性能损失，而且异构存储系统由不同速度和特点的存储设备组成&lt;/p&gt;
&lt;p&gt;在异构存储系统中，还存在前台和后台I/O的干扰问题，带来明显的延迟，硬盘的管理层往往不能分辨出I/O请求的优先级高低，并决定将其数据存储到哪一层次&lt;/p&gt;
&lt;p&gt;这种情况被称为I/O栈高层次与底层存储系统之间的语义鸿沟，其中一个解决方案为使用I/O access hints，当app读取一个文件时，文件块可能散落在不同设备中，上层可以告知存储系统，使得存储系统提前将该文件的数据汇总到读取速度更高的层次，本文提出了一个分类器，允许存储后端控制器针对不同的I/O指令实施不同的I/O策略，例如SSD可以优先处理元数据和小文件，使其先缓存至文件系统中&lt;/p&gt;
&lt;p&gt;本文提出了一个通用灵活的框架，HintStor，为异构存储系统提供access hints&lt;/p&gt;
&lt;p&gt;设计和实现了一套新的用户层面的接口，文件系统插件，块存储数据管理器，在存储管理方面，HintStror通过在block级别进行统计（热力图等）来触发数据迁移，并通过FIFO队列处理I/O；HintStor还提供了access hint的评估，有以下几个要点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新的app/user层面的接口允许用户定义和配置新的hint&lt;/li&gt;
&lt;li&gt;VFS中的文件系统插件提取文件布局信息并建立文件层面的数据分类器，用于下层各种文件系统&lt;/li&gt;
&lt;li&gt;在基于DM的Linux中，块存储数据管理器实现了四个原子access hint操作，触发数据迁移和I/O调度，因此可以执行和分析与上层hint有关的不同策略&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者基于SSD、HDD、SCM以及云存储进行了评估，以体现系统的灵活性，实现并分析了以下四个hints：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用文件系统内部的数据进行分类（元数据，文件数据，文件大小）&lt;/li&gt;
&lt;li&gt;Stream ID，该ID用于对不同数据进行分类，并将相关数据存储在一起或紧密地存储在同一个设备上&lt;/li&gt;
&lt;li&gt;云预取，cloud prefetch，调研了在access hints情况下，如何帮助有效地集成本地存储和云存储。&lt;/li&gt;
&lt;li&gt;I/O任务调度，用户可以在应用层面向块存储设备发起hints，来对前台和后台任务进行区分&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x02-bg&#34;&gt;0x02 Bg&lt;/h1&gt;
&lt;p&gt;在目前的hints机制中，主要考虑的是宿主机的page cache和预取机制，目前system中的系统调用可以通过指定一个随机访问flag来告知内核选取正确的预取和page cache策略以优化对某个文件的访问。&lt;/p&gt;
&lt;p&gt;目前很少有对异构存储系统的优化，需要解决的问题是在不同的存储设备上的智能数据移动，&lt;code&gt;fadvise()&lt;/code&gt;和&lt;code&gt;inoice()&lt;/code&gt;系统调用可以改善prefetching，但是不能解决数据移动问题。&lt;/p&gt;
&lt;p&gt;MPI-I/O hints在高性能计算系统中优化文件系统，目前这些研究着眼于优化存储系统中的buffer/cache部分，red hat通过在用户空间限制I/O来对不同供应商的存储设备进行block对齐。&lt;/p&gt;
&lt;p&gt;目前有些文件系统支持自定义类别，btrfs可以在同一文件系统中支持不同的卷，同时需要用户为存储设备静态配置卷，用户可能让多个应用运行在一个逻辑卷中，为了实现高效的数据管理，作者考虑在卷上支持动态的access hints。&lt;/p&gt;
&lt;h1 id=&#34;0x03-hintstor-design&#34;&gt;0x03 HintStor Design&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221202204914922.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221202204914922&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;HintStor的架构图如上图所示&lt;/p&gt;
&lt;p&gt;Device Mapper是一个开源框架，为由多种块设备组成的卷提供映射&lt;/p&gt;
&lt;p&gt;整体包含三层，在应用层、文件系统层、块层提供了新的接口和插件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在用户层通过接口连接Block Storage Data Manager和文件系统，使得应用可以接受数据和发送access hints&lt;/li&gt;
&lt;li&gt;为了提取chunk的文件系统语义，HintStor将一个插件绑定到文件系统层，来利用内部文件的文件数据结构和文件数据布局&lt;/li&gt;
&lt;li&gt;为了控制对大chunk的数据管理，HintStor实现了新的block storage manager，可以实现对存储设备的access hints策略&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在Device Mapper中实现了两个功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redirector 将I/O请求转发到相应的设备&lt;/li&gt;
&lt;li&gt;Migrator 提供块设备间的异步数据拷贝，为了防止拷贝过程中对带宽的占用，会对流量进行限制，迁移后会通过一个flag来决定是否删除拷贝的原始数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interface&#34;&gt;Interface&lt;/h2&gt;
&lt;p&gt;HintStor使用sysfs接口为I/O请求设置一些属性，与块设备和文件系统通信，类似于一个伪文件系统，可以从其他内核的子系统中提取信息。&lt;/p&gt;
&lt;p&gt;在内核实现了部分系统调用，使用JSON格式的请求进行通信，JSON的编解码可能有性能损失？&lt;/p&gt;
&lt;p&gt;一些应用程序可能要对代码进行少量修改&lt;/p&gt;
&lt;h2 id=&#34;file-system-plugin&#34;&gt;File System Plugin&lt;/h2&gt;
&lt;p&gt;HintStor可以获取与请求相关的data block信息，提前触发数据迁移，优化后续的访问&lt;/p&gt;
&lt;p&gt;HintStor可以区分前台和后台I/O ，优先对前台I/O进行满足&lt;/p&gt;
&lt;p&gt;HintStor在VFS提供了一个FS_HINT调用，用于获取文件的数据布局信息，获取文件系统元数据，该接口供用户空间和应用进行调用，同时支持查询多个文件的映射。&lt;/p&gt;
&lt;p&gt;该功能提供在ext2、ext4和btrfs等Linux中主流的文件系统中&lt;/p&gt;
&lt;p&gt;FS_HINT会维护ext4预分配的block group，btrfs可以将一个块设备挂载为sub-Volume，用于跨设备管理数据&lt;/p&gt;
&lt;p&gt;通过ioctl接口来获取文件的边界，来触发数据迁移或者I/O调度&lt;/p&gt;
&lt;h2 id=&#34;block-storage-data-manager&#34;&gt;Block Storage Data Manager&lt;/h2&gt;
&lt;p&gt;主要包含：&lt;/p&gt;
&lt;p&gt;chunk的映射表：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维护LBA到Physical Block Address的映射&lt;/li&gt;
&lt;li&gt;chunk size 可自定义&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;chunk-level I/O 分析器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于监控每个chunk上的I/O访问统计信息&lt;/li&gt;
&lt;li&gt;估算每个chunk的访问频率，生成热力图，该信息用于周期性的进行数据迁移&lt;/li&gt;
&lt;li&gt;虽然这些信息可能不准确，仍然可以用来作为数据移动的正确性评估指标&lt;/li&gt;
&lt;li&gt;当chunk被迁移到更低速的层次，访问频率可能会升高，进而将数据转移到更快的层次&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;chunk 调度器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于评估不同任务的优先级进行调度&lt;/li&gt;
&lt;li&gt;目前支持一个工作队列和一个等待队列，默认使用FIFO机制来维护原有的请求顺序&lt;/li&gt;
&lt;li&gt;当任务优先级相关的Hint发起时，会将低优先级的请求放到等待队列尾部&lt;/li&gt;
&lt;li&gt;应用可以设置一个定时器来调用一个请求，用于将接近deadline的请求从等待队列移动到工作队列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;access hinit原子操作的分类模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HintStor将数据的布局和移动视作改善异构存储系统的基本功能&lt;/li&gt;
&lt;li&gt;每一个access hint指令是一个四元组&lt;code&gt;(op,chunk_id,src_addr,dest_addr)&lt;/code&gt;，包括操作类型、chunk ID、在逻辑卷中的源地址和目的地址&lt;/li&gt;
&lt;li&gt;共有四个原子操作：
&lt;ul&gt;
&lt;li&gt;Redirect：当最初请求的原地址被重定向到其他地址时触发，例如在一个由多个SSD和HDD组成的逻辑卷中，可以将存储到HDD上的较小的文件重定向到SSD中。该处理函数会调用DM中的驱动并管理被重定向的请求，当bio到达DM时，将对data chunk重新赋值为目的地址&lt;/li&gt;
&lt;li&gt;Migrate：作为HintStor中关键的部分，在后台会保留一个migration daemon后台驻留程序，起初当数据块存放于不同的存储设备时，数据的访问频率会时不时发生变化，Migration操作通过调用DM中的migrator target driver，将data chunk从原有的位置转移到目的位置。为了保证一致性，迁移过程中该chunk将加锁进行保护，忽略对chunk的访问。HintStor提供主动和策略两种迁移方式，主动即用户主动触发，同时实现了基于时间的启发式迁移策略，可以配置为每两个小时迁移top-k访问最频繁的chunk到更快的存储设备中&lt;/li&gt;
&lt;li&gt;Replicate：用于将数据的副本进行保留，使用DM中的migrator target driver，保留原来数据的指针，使得热点数据分布在多个存储设备上，提高数据访问的平均时间，同时可以利用Replicate创建数据的多个副本，需要加锁&lt;/li&gt;
&lt;li&gt;Prefetch：类似于Buffering， HintStor预留一个buffer space空间用于预取，将data chunk加载到buffer space中，实现类似于Migrate和Replicate，主要区别是Prefetch在拷贝时不需要加锁进行保护&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;classification&#34;&gt;Classification&lt;/h2&gt;
&lt;p&gt;I/O access hints可以分为两种，分别为从操作系统动态地进行捕获和从文件系统静态地获取&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221209005726034.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221209005726034&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;动态的access hints主要帮助块存储设备在运行时管理数据的布局，对于冷数据，便于文件系统进行预取。&lt;/p&gt;
&lt;p&gt;HintStor在chunk级别处理I/O请求，并且绑定一些额外的信息，例如文件结构和用户的提示，任务调度则在进程level实现&lt;/p&gt;
&lt;h1 id=&#34;conclusion--thinking&#34;&gt;Conclusion &amp;amp;&amp;amp; Thinking&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;对比Flash Share，HintStor主要针对Linux I/O栈和文件系统层面，Flash Share 还考虑到了NVMe协议方面和SSD层面&lt;/li&gt;
&lt;li&gt;在处理前台/后台I/O部分二者比较类似&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Linux Block I/O Stack简介</title>
        <link>https://blog.ipandai.club/p/linux-block-i/o-stack%E7%AE%80%E4%BB%8B/</link>
        <pubDate>Sun, 09 Oct 2022 13:44:18 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/linux-block-i/o-stack%E7%AE%80%E4%BB%8B/</guid>
        <description>&lt;p&gt;本文将从操作系统内核层面去介绍Linux I/O 栈&lt;/p&gt;
&lt;p&gt;==涉及Linux内核的部分主要为Linux 2.6==&lt;/p&gt;
&lt;h1 id=&#34;linux-io-栈概述&#34;&gt;Linux I/O 栈概述&lt;/h1&gt;
&lt;p&gt;Linux I/O 栈的简图如下&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20221106012036118.png&#34; alt=&#34;image-20221106012036118&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;下面分别简述各个层次的功能&lt;/p&gt;
&lt;h2 id=&#34;应用程序层&#34;&gt;应用程序层&lt;/h2&gt;
&lt;p&gt;应用程序层通过系统调用向VFS发起I/O请求&lt;/p&gt;
&lt;h2 id=&#34;vfs层&#34;&gt;VFS层&lt;/h2&gt;
&lt;p&gt;VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。&lt;/p&gt;
&lt;p&gt;来自应用程序的请求到达VFS后，VFS会创建包含I/O请求信息的结构体提交给块I/O层。&lt;/p&gt;
&lt;p&gt;Linux在文件系统层提供三种IO访问的形式：Buffered IO、MMap 、Direct I/O。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Buffered I/O：这种访问情况下文件数据需要经过设备、Page Cache、用户态缓存，需要进行两次的拷贝动作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MMap：内存映射技术，将内存中的一部分空间与设备进行映射，使得应用程序能够向访问普通内存一样对文件进行访问。使文件数据仅经过设备、Page Cache即可直接传递到应用程序。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Direct I/O：采用Direct IO的方式可以让用户态直接与块设备进行对接，跳过了Page Cache，从硬盘和用户态中拷贝数据，提高文件第一次读写的效率，若之后需要重复访问同一数据，需要消耗比利用Page Cache更多的时间，一般用于数据库系统。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;块io层&#34;&gt;块I/O层&lt;/h2&gt;
&lt;p&gt;系统能够随机访问固定大小的数据片的设备被称为块设备，最常见的块设备为硬盘（机械硬盘，固态盘），对块设备的访问通常是在其内部安装文件系统。操作系统为了对其访问和保证性能，需要一个子系统来对块设备和对块设备的请求进行管理。&lt;/p&gt;
&lt;h2 id=&#34;scsi底层--设备驱动层&#34;&gt;SCSI底层 &amp;amp;&amp;amp; 设备驱动层&lt;/h2&gt;
&lt;p&gt;块I/O层将请求发往SCSI层，SCSI就开始真实处理这些IO请求，但是SCSI层又对其内部按照功能划分了不同层次：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SCSI高层：高层驱动负责管理disk，接收块I/O层发出的IO请求，打包成SCSI层可识别的命令格式，继续往下发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SCSI中层：中层负责通用功能，如错误处理，超时重试等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SCSI低层：底层负责识别物理设备，将其抽象提供给高层，同时接收高层派发的SCSI命令，交给物理设备处理&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更正：对于传统的块设备而言，服务器通过SCSI协议、SAS接口连接，个人电脑通过AHCI协议/SATA接口连接，目前主流的发展方向为NVMe协议/M.2接口。&lt;/p&gt;
&lt;h2 id=&#34;物理外设层&#34;&gt;物理外设层&lt;/h2&gt;
&lt;p&gt;在I/O中一般为磁盘、固态硬盘等存储设备&lt;/p&gt;
&lt;h1 id=&#34;vfs简介&#34;&gt;VFS简介&lt;/h1&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。&lt;/p&gt;
&lt;p&gt;VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。&lt;/p&gt;
&lt;p&gt;VFS中有四个最重要的数据结构，分别是dentry、file、inode、superblock，下文对他们分别进行介绍。&lt;/p&gt;
&lt;h2 id=&#34;directory-entry-cache-dcache&#34;&gt;Directory Entry Cache (dcache)&lt;/h2&gt;
&lt;p&gt;VFS提供了open，stat，chmod等系统调用，需要向他们提供文件的路径名，VFS会在dcache中去查询。&lt;/p&gt;
&lt;p&gt;dcache提供了非常快的查找机制将路径名转换到一个具体的dentry目录项，缓存的目录项存储在RAM中并且不会持久化到硬盘。&lt;/p&gt;
&lt;p&gt;在没有命中缓存时，VFS会通过查找和加载inode来创建dentry缓存，最终实现可以通过path name查找到对应的dentry目录项。&lt;/p&gt;
&lt;h2 id=&#34;the-inode-object&#34;&gt;The Inode Object&lt;/h2&gt;
&lt;p&gt;每个独立的dentry都有一个指向一个inode的指针，inode一般保存在disc（块设备文件系统）中或者内存中（虚拟文件系统）。disc中的inode在被请求或修改时会复制到内存中，并在修改后回写到disc。多个dentry可以指向同一个inode（硬链接）&lt;/p&gt;
&lt;p&gt;对于查找inode的请求，VFS在父目录的inode调用lookup函数。&lt;/p&gt;
&lt;h2 id=&#34;the-file-object&#34;&gt;The File Object&lt;/h2&gt;
&lt;p&gt;在打开一个文件时，需要分配一个文件结构体（内核层面对file descriptor的实现），结构体内容的创建通过dentry指针来获取，文件结构体存储在进程的文件描述符表中。&lt;/p&gt;
&lt;p&gt;对文件的读写、关闭通过用户空间的fd来操作正确的文件结构。&lt;/p&gt;
&lt;p&gt;只要文件打开，它就会保持对dentry的使用状态，这同时意味着inode仍在使用。&lt;/p&gt;
&lt;h2 id=&#34;the-dentry-object&#34;&gt;The Dentry Object&lt;/h2&gt;
&lt;p&gt;目录项纪录子文件和子目录的名称&lt;/p&gt;
&lt;p&gt;每个目录项的结构&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;大小&lt;/th&gt;
&lt;th&gt;字段&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;__le32&lt;/td&gt;
&lt;td&gt;4bytes&lt;/td&gt;
&lt;td&gt;Inode&lt;/td&gt;
&lt;td&gt;inode编号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;__le16&lt;/td&gt;
&lt;td&gt;2bytes&lt;/td&gt;
&lt;td&gt;Rec_len&lt;/td&gt;
&lt;td&gt;目录项的长度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;__u8&lt;/td&gt;
&lt;td&gt;2bytes&lt;/td&gt;
&lt;td&gt;Name_len&lt;/td&gt;
&lt;td&gt;文件名长度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;__u8&lt;/td&gt;
&lt;td&gt;2bytes&lt;/td&gt;
&lt;td&gt;File_type&lt;/td&gt;
&lt;td&gt;文件类型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;char[EXT2_NAME_LEN]&lt;/td&gt;
&lt;td&gt;最大255个字符&lt;/td&gt;
&lt;td&gt;Name&lt;/td&gt;
&lt;td&gt;文件名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所有目录都包含的目录项包括当前目录和上级目录:&lt;code&gt;.&lt;/code&gt;,&lt;code&gt;..&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;要对文件名进行4字节对齐，后面补&lt;code&gt;\0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;所有目录项顺序拼接组成目录信息，在进行查询时可以使用rec_len来计算偏移量，便于查找目录中的文件（顺序遍历）&lt;/p&gt;
&lt;h2 id=&#34;the-superblock-object&#34;&gt;The SuperBlock Object&lt;/h2&gt;
&lt;p&gt;superblock对象表示一个挂载的文件系统，存储有关文件系统的相关信息&lt;/p&gt;
&lt;h3 id=&#34;注册和挂载一个文件系统&#34;&gt;注册和挂载一个文件系统&lt;/h3&gt;
&lt;p&gt;使用如下的API&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;linux/fs.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;extern&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;register_filesystem&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;extern&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;unregister_filesystem&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;文件系统的挂载可以通过&lt;code&gt;-&amp;gt;mount()&lt;/code&gt;方法将新的文件系统挂载到挂载点，当pathname解析解析到挂载点时，会跳转到被挂在文件系统的root。&lt;/p&gt;
&lt;p&gt;内核所注册的文件系统在&lt;code&gt;/proc/filesystems&lt;/code&gt;文件中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;file_system_type&lt;/code&gt;结构的定义如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fs_flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dentry&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mount&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                 &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kill_sb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;super_block&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;module&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;list_head&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fs_supers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lock_class_key&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_lock_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lock_class_key&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_umount_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;mount()&lt;/code&gt;方法有以下几个参数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fs_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;page-cache&#34;&gt;Page Cache&lt;/h1&gt;
&lt;p&gt;​	页高速缓存(Page Cache)是Linux内核所使用的主要硬盘高速缓存。在绝大多数情况下，内核在读写硬盘时都引用页高速缓存。新页被追加到页高速缓存以满足用户态进程的读请求。如果页不在高速缓存中，新页就被加到高速缓存中，然后用从硬盘读出的数据填充它 。 如 果内存有足够的空闲空间 ，就让该页在高速缓存中长期保留 ，使其他进程再使用该页时不再访问硬盘。&lt;/p&gt;
&lt;p&gt;​	同时， 在把一页数据写到块设备之前，内核首先检查对应的页是否已经在高速缓存中， 如果不在，就要先在其中增加 一个新项，并用要写到硬盘中的数据填充该项。I/O数据的传送并不是马 上开始，而是要延迟几秒之后才对硬盘进行更新，从而使进程有机会对要写人硬盘的数据做进 一步的修改 (内核执行延迟的写操作)。&lt;/p&gt;
&lt;p&gt;​	几乎所有的读写都依赖Page Cache，除非指定了O_DIRECT标志位，此时I/O将使用进程用户态地址空间的缓冲区，一般数据库应用会使用此方式。&lt;/p&gt;
&lt;p&gt;​	对Page Cache中页的识别通过索引节点和在相应文件中的偏移量来实现。&lt;/p&gt;
&lt;h2 id=&#34;主要数据结构&#34;&gt;主要数据结构&lt;/h2&gt;
&lt;p&gt;​	Page Cache中的核心数据结构是&lt;code&gt;address_space&lt;/code&gt;，该对象在页面所属的inode中，由于多个页面可能属于同一个所有者，因此多个页面可能被连接到同一个address space，该对象同时还将所有者的页面和对页面的操作建立连接。&lt;/p&gt;
&lt;p&gt;有关页面的操作主要有以下几个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;==writepage== 写操作，将页写入到硬盘映像&lt;/li&gt;
&lt;li&gt;==readpage== 读操作，将数据从硬盘映像加载到页&lt;/li&gt;
&lt;li&gt;sync_page 在所有者的页面已经准备就绪后，启动I/O数据的传输&lt;/li&gt;
&lt;li&gt;wirtepages 把指定数量的所有者脏页写回硬盘&lt;/li&gt;
&lt;li&gt;==prepare_write== 为写操作做准备（硬盘文件系统使用）&lt;/li&gt;
&lt;li&gt;==commit_write== 完成写操作（硬盘文件系统使用）&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;基数树-radix-tree&#34;&gt;基数树 Radix Tree&lt;/h2&gt;
&lt;p&gt;基数树类似于字典树，通过要查找的ID的二进制序列，来一级一级的进行查找，为了减少树的高度和叶子非结点个树，可以使用2 bit或者4 bit作为树的结点。&lt;/p&gt;
&lt;p&gt;在内核中，Radix Tree的叶子结点保存了指向所有者页描述符的指针。&lt;/p&gt;
&lt;p&gt;Page Cache 中的每个文件都是一棵基数树（radix tree，本质上是多叉搜索树），树的每个节点都是一个页。&lt;/p&gt;
&lt;p&gt;内核通过把页索引转换为Radix Tree中的路径，使用Radix Tree来快速搜索页描述符所在的位置。内核通过页描述符可以确定页面是否为等待刷新到硬盘的脏页或其中数据的I/O传输是否正在进行。&lt;/p&gt;
&lt;p&gt;Radix Tree中的每一个叶子节点指向文件内相应偏移所对应的Cache项&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20221101143123364.png&#34; alt=&#34;image-20221101143123364&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;page-cache处理函数&#34;&gt;Page Cache处理函数&lt;/h2&gt;
&lt;h3 id=&#34;find_get_page&#34;&gt;find_get_page()&lt;/h3&gt;
&lt;p&gt;传入 address space对象指针和便宜了，获取自旋锁，在Radix Tree中搜索具有指定偏移量的叶子结点，若成功找到，会增加页面使用计数器，释放自旋锁，并返回页描述符的指针，否则将释放自旋锁并返回NULL。&lt;/p&gt;
&lt;h3 id=&#34;add_to_page_cache&#34;&gt;add_to_page_cache()&lt;/h3&gt;
&lt;p&gt;传入页描述符地址，address space对象的address mapping字段，页索引在地址空间的偏移量，为Radix Tree分配新节点所使用的内存分配标志gfp_mask，插入成功则返回0&lt;/p&gt;
&lt;h3 id=&#34;remove_from_page_cache&#34;&gt;remove_from_page_cache()&lt;/h3&gt;
&lt;p&gt;获取自旋锁、关闭中断后在Radix Tree中删除节点，返回删除页的描述符指针，page-&amp;gt;mapping字段设置为NULL，将所缓存页的page-&amp;gt;mapping-&amp;gt;nrpages计数器值减1，最后释放自旋锁，打开中断。&lt;/p&gt;
&lt;h3 id=&#34;read_cache_page&#34;&gt;read_cache_page()&lt;/h3&gt;
&lt;p&gt;确保Page Cache中包括指定页的最新版本，首先检查页面是否存在，若不存在则新申请分配空间，调用add_to_page_cache插入相应的页描述符，调用lru cache add插入到该地址空间的非活跃LRU链表中。&lt;/p&gt;
&lt;p&gt;在保证页面存在于Page Cache后，使用mark page accessed记录页面已经被访问过，若PG_uptodate标志位0，则页面不是最新的，需要从硬盘重新读取该页&lt;/p&gt;
&lt;h2 id=&#34;buffer-cache&#34;&gt;Buffer Cache&lt;/h2&gt;
&lt;p&gt;在Linux内核2.4版本之前，Buffer Cache和Page Cache是独立的，因为操作系统对硬盘等块设备的读写是基于块的，而非页，文件的数据在Page Cache中缓存，硬盘的块数据在Buffer Cache中缓存。但是这种表示方式效率非常低，自2.4版本的内核开始，二者开始统一表示。&lt;/p&gt;
&lt;p&gt;因为VFS和文件系统都是通过块Block来组织硬盘上的数据，因此Buffer Cache必须存在，目前它作为Buffer Page存储在特定的页中。&lt;/p&gt;
&lt;p&gt;对于文件这种通过Page Cache来表示的数据，通过Page Cache来表示Buffer Cache，而对于文件的metadata，直接使用Buffer Cache来表示。&lt;/p&gt;
&lt;h2 id=&#34;脏页的处理&#34;&gt;脏页的处理&lt;/h2&gt;
&lt;p&gt;在进程修改了页面数据后，该页面就成为了脏页，并将脏页刷新到块设备上的操作延迟，这样可以显著提高系统的性能。&lt;/p&gt;
&lt;p&gt;为了防止硬件错误、掉电等情况导致RAM内容的丢失，以及RAM容量过大的需求，在以下情况主动刷新脏页到硬盘中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Page Cache太满，或脏页数量太多&lt;/li&gt;
&lt;li&gt;脏页已经存在了较长的时间&lt;/li&gt;
&lt;li&gt;进程请求对块设备或文件的变化进行刷新（sync、fsync和fdatasync系统调用）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;文件的访问&#34;&gt;文件的访问&lt;/h1&gt;
&lt;p&gt;文件的访问一般有五种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;==规范模式==：由系统调用read和write来读写，read将阻塞调用进程，直到数据被拷贝到用户地址空间，write则在数据拷贝到Page Cache后立即结束&lt;/li&gt;
&lt;li&gt;==同步模式==：&lt;code&gt;O_SYNC&lt;/code&gt;标志为1，影响写操作（读操作总是阻塞的），将阻塞写操作，直到数据被有效的写入硬盘。&lt;/li&gt;
&lt;li&gt;==内存映射模式==：文件打开后，通过系统调用&lt;code&gt;mmap&lt;/code&gt;将文件映射到内存中，此后文件成为完全保存在RAM中的字节数组。&lt;/li&gt;
&lt;li&gt;==直接I/O模式==：O_DIRECT标志为1，任何读写操作都在用户地址空间和硬盘间进行传输，直接跳过Page Cache。&lt;/li&gt;
&lt;li&gt;==异步模式==：该模式下，文件可以通过一组POSIX API或者Linux系统调用来进行访问，异步模式下数据传输请求不阻塞调用的进程，而是在后台执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;读文件&#34;&gt;读文件&lt;/h2&gt;
&lt;p&gt;内核实现了&lt;code&gt;read()&lt;/code&gt;系统调用为进程提供文件的读取功能，入口为&lt;code&gt;sys_read()&lt;/code&gt;需要提供三个参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文件描述符&lt;/li&gt;
&lt;li&gt;保存数据的缓冲区&lt;/li&gt;
&lt;li&gt;读取字符数目的长度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于VFS而言，首先根据文件描述符编号，内核(使用&lt;code&gt;fs/file_table.c&lt;/code&gt;中的&lt;code&gt;fget_light()&lt;/code&gt;函数)从进程的 &lt;code&gt;task_struct&lt;/code&gt;中找到与之相关的文件实例，&lt;code&gt;file_pos_read()&lt;/code&gt;函数确定读取当前文件的位置，并返回一个参数&lt;code&gt;file-&amp;gt;file_pos&lt;/code&gt;，实际的读取操作通过&lt;code&gt;vfs_read()&lt;/code&gt;函数来执行，该函数会判断文件是否有&lt;code&gt;file-&amp;gt;f_op-&amp;gt;read&lt;/code&gt;方法，从而确定后续使用的函数，若不存在则会调用&lt;code&gt;do_sync_read&lt;/code&gt;函数。最后通过&lt;code&gt;file_pos_write&lt;/code&gt;函数来记录文件新的读写位置，通过修改&lt;code&gt;file_ops&lt;/code&gt;的值来实现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221022151840698.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221022151840698&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;为了提高在文件读取时的性能，Linux kernel提供了缓冲区和缓冲系统。&lt;/p&gt;
&lt;p&gt;Linux对文件的读是基于页的，内核总是一次传送几个完整的数据页。若发起read调用时数据不在RAM中，内核则分配一个新的page frame，并调取文件的相应部分填入其中，将其加入到Page Cache中，最后把请求的数据拷贝到进程的地址空间。&lt;/p&gt;
&lt;p&gt;大多数硬盘文件系统，使用通用函数generic_file_read来实现read系统调用。&lt;/p&gt;
&lt;h3 id=&#34;generic_file_read&#34;&gt;generic_file_read()&lt;/h3&gt;
&lt;p&gt;该函数有如下几个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filp 文件对象的地址&lt;/li&gt;
&lt;li&gt;buf 用户态内存区域的线性地址，用于存放读取出的文件数据&lt;/li&gt;
&lt;li&gt;count 读取字符的个数&lt;/li&gt;
&lt;li&gt;ppos 存放读操作开始处的文件偏移量，一般为&lt;code&gt;filp-&amp;gt;f_pos&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;首先，函数初始化两个描述符：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;局部变量iovec类型的local_iov，包含buf和count字段&lt;/li&gt;
&lt;li&gt;局部变量kiocb类型的kiocb，跟踪正在进行的同步和异步I/O操作的完成状态&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后调用&lt;code&gt;__generic_file_aio_read()&lt;/code&gt;获取上面两个描述符的地址，返回值时文件有效读入的字节数，在该函数返回后，&lt;code&gt;generic_file_read()&lt;/code&gt;也将终止。&lt;/p&gt;
&lt;h3 id=&#34;__generic_file_aio_read&#34;&gt;__generic_file_aio_read()&lt;/h3&gt;
&lt;p&gt;接收四个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kiocb描述符地址：iocb&lt;/li&gt;
&lt;li&gt;iovec描述符数组的地址：iov（描述等待接收数据的用户态缓冲区）&lt;/li&gt;
&lt;li&gt;iovec描述符数组的长度&lt;/li&gt;
&lt;li&gt;存放文件当前指针的变量地址：ppos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该函数在对Page Cach触发read时的执行过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调用access_ok()来检查iovec描述的用户态缓冲区是否合法&lt;/li&gt;
&lt;li&gt;建立读操作描述符，存放与用户缓冲区有关的、正在进行的文件操作的状态&lt;/li&gt;
&lt;li&gt;调用&lt;code&gt;do_generic_file_read()&lt;/code&gt;，给定参数文件对象指针filp，文件偏移量指针ppos，读操作描述符地址、函数file_read_actor()的地址&lt;/li&gt;
&lt;li&gt;返回拷贝到用户态缓冲区的字节数，对应读操作描述符的written字段&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;do_generic_file_read()&lt;/p&gt;
&lt;h2 id=&#34;写流程&#34;&gt;写流程&lt;/h2&gt;
&lt;p&gt;​	很多硬盘文件系统通过通用函数generic_file_write实现write方法。write系统调用负责将用户地址空间中的数据移动到内核数据结构中，然后将数据拷贝到Page Cache中，将这些页标记为脏，然后持久化到硬盘中。&lt;/p&gt;
&lt;p&gt;​	write系统调用的结构与read同样简单。除了用f_op-&amp;gt;write和do_sync_write替换了read中对应的例程之外，二者的代码流程图几乎完全相同。&lt;/p&gt;
&lt;p&gt;​	从形式上看来，sys_write与sys_read的参数相同:一个文件描述符、一个指针变量、一个长度指示(表示为整数)。显然，其语义稍有不同。指针并非指向存储读取数据的缓冲区，而是指向需要 写入文件的数据。长度参数指定了数据的字节长度。&lt;/p&gt;
&lt;h2 id=&#34;预取机制&#34;&gt;预取机制&lt;/h2&gt;
&lt;p&gt;​	很多硬盘的访问都是顺序的。普通文件以相邻扇区成组存放在硬盘上，因此很少移动磁头就可以快速检索到文件。当程序读或拷贝一个文件时，它通常从第一个字节到最后一个字节顺序地访问文件。因此，在处理进程对同一文件的一系列读请求时，可以从硬盘上很多相邻的扇区读取。&lt;/p&gt;
&lt;p&gt;​	预读(Read-ahead)是一种技术，这种技术在于在实际请求前读普通文件或块设备文件的几个相邻的数据页。在大多数情况下，预读能极大地提高硬盘的性能，因为预读使硬盘控制器处理较少的命令，其中的每条命令都涉及一大组相邻的扇区。此外，预读还能提高系统的晌应能力。顺序读取文件的进程通常不需要等待请求的数据，因为请求的数据已经在RAM中了。&lt;/p&gt;
&lt;h2 id=&#34;缓存回写机制&#34;&gt;缓存回写机制&lt;/h2&gt;
&lt;p&gt;缓存回写机制也就是Page Cache对脏页进行回写，将对文件的I/O修改永久持久化到硬盘上的过程，此处不再赘述。后续计划深入了解Linux内核对回写的控制。&lt;/p&gt;
&lt;h1 id=&#34;文件系统&#34;&gt;文件系统&lt;/h1&gt;
&lt;p&gt;文件系统作为操作系统中一个不可或缺的部分，负责在存储设备上管理、存储、获取数据、维护信息。&lt;/p&gt;
&lt;p&gt;在Linux中，内核通过VFS虚拟文件系统向用户提供通用的接口，向下对接各种不同的文件系统，可以有效避免不同文件系统的实现差异对上层带来影响。&lt;/p&gt;
&lt;h2 id=&#34;ext4&#34;&gt;ext4&lt;/h2&gt;
&lt;p&gt;本部分大量内容来自kernel.org的文档&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kernel.org/doc/html/latest/filesystems/ext4/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ext4 Data Structures and Algorithms — The Linux Kernel documentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;ext-文件系统发展历史&#34;&gt;ext 文件系统发展历史&lt;/h3&gt;
&lt;h4 id=&#34;ext1&#34;&gt;ext1&lt;/h4&gt;
&lt;p&gt;Extended file system，扩展文件系统。&lt;/p&gt;
&lt;p&gt;Linux最早使用的文件系统为Minix的文件系统，但是该系统存在文件大小限制的问题，同时性能不佳。在1992年4月，Rémy Card开发了扩展文件系统，首个版本作为Linux中的文件系统一起发行，最大支持2GB空间，同时它还是Linux中第一个使用VFS实现出的文件系统。&lt;/p&gt;
&lt;h4 id=&#34;ext2&#34;&gt;ext2&lt;/h4&gt;
&lt;p&gt;在首个ext文件系统中，文件访问、存在inode修改以及文件内容修改没有使用独立时间戳的问题，同时最大仅支持255个字符的文件名以及2GB的空间，ext2系统除了修复了这个问题外，还在硬盘存储数据结构中预留了很多空间供未来开发使用，具有良好的可拓展性。&lt;/p&gt;
&lt;p&gt;由于块驱动的限制，ext2文件系统最大支持2TB的单个文件。&lt;/p&gt;
&lt;h4 id=&#34;ext3&#34;&gt;ext3&lt;/h4&gt;
&lt;p&gt;ext3在当时性能不是很出众，但是支持从当时最流行的ext2文件系统升级，无需备份和数据恢复，同时有较少的资源占用（CPU开销）。&lt;/p&gt;
&lt;p&gt;ext3相较于ext2增加了&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;日志支持&lt;/li&gt;
&lt;li&gt;文件系统在线增长&lt;/li&gt;
&lt;li&gt;对大目录提供HTree哈希树索引&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ext3提供了三个日志级别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;日志（最低风险）：metadata和文件内容一起写入日志中，由于还要写入文件系统，因此带来额外的性能开销。&lt;/li&gt;
&lt;li&gt;有序（中等风险）：只将metadata写入日志，但是保证metadata提交前，文件内容会被写入。大部份发行版默认的方式。写入过程中崩溃时，文件系统会清除还没有被提交的文件修改/创建，但是对于文件的覆盖写入，可能会导致文件处于新旧文件的中间态。&lt;/li&gt;
&lt;li&gt;回写（最高风险）：只记录metadata到日志，文件内容在日志提交前或者提交后写入。如果在日志提交前写入失败，会导致硬盘出现垃圾。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ext3存在的一些缺陷：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;没有在线的碎片整理工具&lt;/li&gt;
&lt;li&gt;官方缺少压缩工具&lt;/li&gt;
&lt;li&gt;不支持恢复已经删除的文件&lt;/li&gt;
&lt;li&gt;缺少快照支持&lt;/li&gt;
&lt;li&gt;日志中不支持校验和&lt;/li&gt;
&lt;li&gt;使用四个字节存储Unix时间，因此在2038年1月18日之后将无法继续处理文件&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ext4-1&#34;&gt;ext4&lt;/h4&gt;
&lt;p&gt;ext4在Linux 2.6.28中作为功能完整稳定的文件系统发布，ext4文件系统首先兼容了原有的ext3系统，提供安全可靠的快速迁移。同时还有如下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;支持更大的文件系统和文件大小&lt;/p&gt;
&lt;p&gt;以4KB块，目前支持最大&lt;code&gt;1EiB(1024*1024*1024GB)&lt;/code&gt;的文件系统大小和16TiB的文件大小，使用48bit来编址，未来会支持64bit&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;子目录可扩展性&lt;/p&gt;
&lt;p&gt;ext4文件系统支持最多64000个子目录，是ext3文件系统的两倍&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;扩展数据块&lt;/p&gt;
&lt;p&gt;鼓励对大文件划分多个连续数据块，在硬盘上进行连续布局，从而减少维护大量的间接映射，有助于提高性能，减少硬盘碎片&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多块同时分配，&lt;/p&gt;
&lt;p&gt;在ext3文件系统中，文件系统在分配空闲块时，只能一次处理一个块申请，ext4支持多块分配机制，可以一次申请多个块&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;块的延时分配&lt;/p&gt;
&lt;p&gt;借鉴了XFS、ZFS、btrfs等现代文件系统，在执行写入过程时，在以往的文件系统中，都是对写入到cache的数据立即分配相应的block，消耗了大量的时间，延时分配后，只有当数据需要被刷新到硬盘时，才会执行块的分配，可以和上面两个feat进行配合，提高了文件系统的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;快速fsck&lt;/p&gt;
&lt;p&gt;fsck是极其耗时的过程，主要是需要检查文件系统所有的inode。ext4中，在每个group&amp;rsquo;s inode table中都保存了一个空闲inode链表（同时保存了checksum），因此fsck就不必再检查这些inode&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;日志校验和&lt;/p&gt;
&lt;p&gt;日志在硬盘中是很容易出错的部分，添加校验和可以避免文件系统恢复错误的日志带来更大的损失。使用校验和还使得在ext3中日志的两步提交简化为一步&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;无日志模式&lt;/p&gt;
&lt;p&gt;ext4中可以关闭日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在线碎片整理&lt;/p&gt;
&lt;p&gt;使用e4defrag工具手动进行碎片整理&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inode相关feat&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;更大的inode&lt;/p&gt;
&lt;p&gt;从128bytes提高到256bytes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inode预留&lt;/p&gt;
&lt;p&gt;创建目录时预留一些inode提高性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;纳秒级时间戳&lt;/p&gt;
&lt;p&gt;提高系统的time resolution&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;硬盘空间预分配&lt;/p&gt;
&lt;p&gt;应用可以让文件系统预先分配一部分硬盘上的空间，文件系统会提前创建好相关的数据结构并分配相应的块，后续可以直接写入数据，类似P2P下载时预分配，1.可以防止类似功能的低效率应用级实现；2.减少硬盘碎片；3.对于实时应用来说（延迟敏感，航空工业等），这个特点改善了延迟。此feat通过&lt;code&gt;libc posix_fallocate()&lt;/code&gt;实现&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;默认启用写屏障&lt;/p&gt;
&lt;p&gt;通过消耗一些性能来改善文件系统的完整性，它确保文件系统元数据在硬盘上被正确地写入和排序，即使在掉电时也是如此，对于一些创建大量小文件或者操作元数据的程序，会带来很大的性能影响。如果硬盘是电池供电，通过&lt;code&gt;barrier=0&lt;/code&gt;可以关闭写屏障（会保证安全性）。写屏障保护保证在数据写入&lt;code&gt;缓存（硬盘缓存）&lt;/code&gt;后，先写入日志中的元数据刷新到硬盘，防止出现因为调度策略导致的数据先于日志写入硬盘&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;discard/TRIM&lt;/p&gt;
&lt;p&gt;为SSD提供的TRIM支持。&lt;/p&gt;
&lt;p&gt;对于SSD而言，由于文件系统在删除时只对块标记为空闲，而SSD却并不知道哪些数据块可用，再次写入时会先清除闪存中的数据，再进行写入，要擦除无效页，要先移走有效页，然后再对一整行进行擦除，最后才能执行写入过程，这个现象也被称为&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Write_amplification&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;写入放大(Write Amplification)&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;TRIM主要使得文件系统告知SSD哪些页不再包含有效的数据，有助于提高SSD的寿命和磨损均衡&lt;/p&gt;
&lt;p&gt;随着使用的block数量接近SSD的容量上限，会导致SSD的性能下降，文件系统通过discard指令告知SSD哪些范围内的block已经不再使用，SSD可以将其回收或者用来实现磨损均衡&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据组织&#34;&gt;数据组织&lt;/h3&gt;
&lt;p&gt;ext4文件系统将部分block组织为一个group，默认block大小采用4KB，因此一个group中的block数量为&lt;code&gt;8*block_size_in_bytes=32768 blocks&lt;/code&gt;，空间为128MB。&lt;/p&gt;
&lt;p&gt;ext4部分在硬盘中按小端写入，jdb2日志部分在硬盘中按大端写入。&lt;/p&gt;
&lt;p&gt;ext4在分配硬盘空间时只能分配若干个block，一个block包含硬盘上的若干个扇区，扇区的数量必须是2的幂次。&lt;/p&gt;
&lt;p&gt;不使用扩展布局的文件（通过block映射维护）必须存放在文件系统前$2^{32}$个block中，通过扩展布局来保存的文件，必须存放在前$2^{48}$个block中。&lt;/p&gt;
&lt;p&gt;对于32位和64位的文件系统，各个数据结构的大小限制如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221115165058013.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221115165058013&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221115165128885.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221115165128885&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在一个Block Group中，数据的组织如表格所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Group 0 Padding&lt;/th&gt;
&lt;th&gt;ext4 Super Block&lt;/th&gt;
&lt;th&gt;Group Descriptors&lt;/th&gt;
&lt;th&gt;Reserved GDT Blocks&lt;/th&gt;
&lt;th&gt;Data Block Bitmap&lt;/th&gt;
&lt;th&gt;inode Bitmap&lt;/th&gt;
&lt;th&gt;inode Table&lt;/th&gt;
&lt;th&gt;Data Blocks&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1024 bytes&lt;/td&gt;
&lt;td&gt;1 block&lt;/td&gt;
&lt;td&gt;many blocks&lt;/td&gt;
&lt;td&gt;many blocks&lt;/td&gt;
&lt;td&gt;1 block&lt;/td&gt;
&lt;td&gt;1 block&lt;/td&gt;
&lt;td&gt;many blocks&lt;/td&gt;
&lt;td&gt;many more blocks&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;硬盘中会预留1024 bytes，用于操作系统的引导模块安装，该padding分区只有group 0含有，为了避免supper block损坏导致整个文件系统不可用，在其他block group中还会有备份，如果一个block group不含有冗余备份，那么它将以data block bitmap开头。&lt;/p&gt;
&lt;p&gt;在执行mkfs时，还会在group descriptor描述符分区和data block bitmap分区之间分配一个保留的GDT blocks分区，用于文件系统日后的扩展。&lt;/p&gt;
&lt;p&gt;ext4文件系统中还引入了Flexible Block Groups的概念(flex_bg)，主要思想是将若干个block group组合成一个大的Group，将所有block group中的元数据（inode，bitmap）都集中到第一个block group中，提高对元数据加载和查询的效率，并且使得文件数据在硬盘上连续，更紧凑。一般通过&lt;code&gt;2^sb.s_log_groups_per_flex&lt;/code&gt;个block group来组成一个大的Group。&lt;/p&gt;
&lt;p&gt;自ext3起，ext文件系统就开始使用Meta block groups(META_BG)，是一组只用一个group descriptor来描述的block groups，首个block group不再存储每个block group的描述符，转由meta block group来存储，并且会在其中存储几个冗余副本。&lt;/p&gt;
&lt;p&gt;对于ext4文件系统的读写流程计划在此后单独写一篇文章进行总结。&lt;/p&gt;
&lt;h2 id=&#34;f2fs&#34;&gt;F2FS&lt;/h2&gt;
&lt;p&gt;F2FS文件系统提出于&lt;code&gt;USENIX FAST&#39;15&lt;/code&gt;，&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast15/technical-sessions/presentation/lee&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;F2FS: A New File System for Flash Storage | USENIX&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;全称为Flash-Friendly File System，基于日志结构文件系统(Log-structured File System, LFS)，针对LFS中wandering tree和gc开销大的问题进行了优化。&lt;/p&gt;
&lt;h3 id=&#34;主要特点&#34;&gt;主要特点&lt;/h3&gt;
&lt;p&gt;针对闪存进行了优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;==扩大随机写入区域来提高性能==（此处存在疑问），但带来了空间局部性&lt;/li&gt;
&lt;li&gt;尽可能使得文件系统的处理单元与FTL中保持一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;针对wandering tree问题，该问题是因为LFS的脏数据通过追加更新，如果一个数据块变成脏数据，那么其索引块，以及间接索引块都会变成脏块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用node来代替inode和指针块&lt;/li&gt;
&lt;li&gt;通过包含所有node块地址的Node Address Table(NAT)来切断更新递归的传播&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;垃圾回收问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持后台清理&lt;/li&gt;
&lt;li&gt;支持贪心和cost-benefit算法&lt;/li&gt;
&lt;li&gt;为动态/静态冷热数据分离提供multi-head logs&lt;/li&gt;
&lt;li&gt;引入自适应日志来实现高效的块分配&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;btrfs&#34;&gt;Btrfs&lt;/h2&gt;
&lt;p&gt;Btrfs是一个写时复制的文件系统，基于B-tree实现，专注于容错，修复和易于管理，发布于2014年，发展目标是为了取代ext3文件系统，解决ext3的限制。在2021年，Fedora 33宣布将使用Btrfs作为安装时默认的文件系统。而Fedora受到redhat的直接赞助，说明Btrfs在不断发展的过程中，得到了社区中部分用户以及企业的认可，目前还处在测试和不断完善的阶段。&lt;/p&gt;
&lt;p&gt;主要特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于B-Tree维护元数据，插入查询操作高效&lt;/li&gt;
&lt;li&gt;基于COW，提高硬盘寿命&lt;/li&gt;
&lt;li&gt;支持只读/可读快照&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;块io层-与-io调度器&#34;&gt;块I/O层 与 I/O调度器&lt;/h1&gt;
&lt;h2 id=&#34;块设备&#34;&gt;块设备&lt;/h2&gt;
&lt;p&gt;对于Unix系统来说，有着一切皆文件的设计哲学，因此外部设备在操作系统看来是一个设备文件。设备文件共分为两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;块设备：数据可以被随机访问&lt;/li&gt;
&lt;li&gt;字符设备：数据不可以被随机访问，或者包含受限制的随机访问（设备内部构成）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;块设备的一个主要特点就是CPU和总线读写数据的时间开销与硬盘硬件速度不匹配。&lt;/p&gt;
&lt;p&gt;内核再对块设备发出I/O请求时，内核利用&lt;code&gt;通用块层&lt;/code&gt;发起I/O，每次I/O请求都是通过一个&lt;code&gt;bio&lt;/code&gt;结构体来描述。通用块层下的I/O调度程序会根据策略将待处理的I/O请求进行归类，将相邻的请求聚集在一起，减少硬盘磁头的移动。&lt;/p&gt;
&lt;p&gt;块是操作系统和硬件设备在传输数据时的基本单位。&lt;/p&gt;
&lt;p&gt;本文主要介绍mq-deadline，kyber，bfq三个多队列设计的调度器&lt;/p&gt;
&lt;h2 id=&#34;mq-deadline&#34;&gt;mq-deadline&lt;/h2&gt;
&lt;p&gt;mq-deadline调度器主要根据deadline调度器来设计，是deadline调度器的多队列版本，适配了block层的多队列&lt;/p&gt;
&lt;p&gt;mq-deadline调度器将IO分为read和write两种类型，对于这每种类型的IO有一棵红黑树和一个FIFO的队列，红黑树用于将I/O按照其访问的LBA排列方便查找合并，FIFO队列则记录了I/O进入mq-deadline调度器的顺序，以提供超时期限的保障&lt;/p&gt;
&lt;p&gt;read请求的I/O可以抢占write的分发机会，但不能一直占有，维护了一个计数保证read请求不会导致write请求饥饿&lt;/p&gt;
&lt;p&gt;mq-deadline调度器会优先去批量式地分发I/O而不去管I/O的到期时间，当批量分发到一定的个数再关心到期时间，然后去分发即将到期的I/O&lt;/p&gt;
&lt;p&gt;最后mq-deadline针对sync穿透性I/O这种需要尽快发送到设备的I/O设置另外一个dispatch队列，然后每次派发的时候都优先派发dispatch队列上的I/O&lt;/p&gt;
&lt;h2 id=&#34;bfq&#34;&gt;bfq&lt;/h2&gt;
&lt;p&gt;全称为Budget Fair Queueing，bfq是一种比例共享的I/O调度器，有一些低延迟能力。支持cgroup。&lt;/p&gt;
&lt;p&gt;BFQ的主要特点有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BFQ保证了系统和应用程序的响应能力，以及对时间敏感程序保证低延迟（音视频播放）&lt;/li&gt;
&lt;li&gt;BFQ在进程或者组之间分配带宽、时间，在保持吞吐量时进行时间分配&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在默认配置下，BFQ优先考虑延迟而非吞吐量，可以通过设置&lt;code&gt;low_latency=0&lt;/code&gt;来关闭低延迟启发式算法，来提高设备吞吐量。BFQ通过锁来保护每个I/O请求，会增加一些额外开销。&lt;/p&gt;
&lt;p&gt;BFQ是一种比例份额调度器，主要部分借鉴了CFQ，是一种公平的调度器。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个在设备上执行I/O的进程都与一个权重和一个bfq 队列相关联&lt;/li&gt;
&lt;li&gt;BFQ授权一个队列（进程）可以在一段时间内对设备的独占访问，并通过将每个队列与预算（请求扇区数量）相关联来实现。&lt;/li&gt;
&lt;li&gt;每个队列获得设备访问授权后，队列的预算会在每次请求发送时根据请求大小递减&lt;/li&gt;
&lt;li&gt;当发生：队列完成其预算、队列清空、触发预算超时时，队列才会过期，预算超时机制可以防止随机I/O进程对设备长时间的占用降低吞吐量&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kyber&#34;&gt;kyber&lt;/h2&gt;
&lt;p&gt;Kyber I/O调度器是Linux上面针对高速存储设备（NVMe闪存）设计的一个新的I/O调度器，配和多队列的Block层使用。在Linux 4.12的时候和BFQ调度器一起成为内核中的一个可选项&lt;/p&gt;
&lt;p&gt;Kyber调度器的基本思路是会为每一个的硬件的队列维护一个不同类型I/O请求的队列，这些请求主要根据I/O操作的方式来进行区分。Kyber按照读、同步写以及其它的(异步写等)将I/O请求分为了3类。在Kyber的设计中，更加倾向于让读优先，这个策略也和其它的一些调度器的设计类似&lt;/p&gt;
&lt;p&gt;Kyber在一个Kyber上下文中维护了关于这几类请求的队列。它通过限制每一个队列的长度来对在这里产生的请求的延迟进行控制。Kyber只有在这些队列里面的请求被处理了之后才会收集新的请求。这里限制的方式采用了基于Token的方式&lt;/p&gt;
&lt;p&gt;由于Kyber面向的是高速存储，这类设备一般是NVMe SSD、NVM。采用类似CFQ中的一些对请求排序的方法可能有损于性能，所以在Kyber中没有对请求排序的逻辑。Kyber会对一些I/O请求进行合并操作，以及会尝试批量处理这些请求来提高性能。批量处理的大小根据请求类型来决定&lt;/p&gt;
&lt;h1 id=&#34;nvme接口协议&#34;&gt;NVMe接口协议&lt;/h1&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;NVMe协议的产生是为了取代固态硬盘原有的AHCI协议+SATA接口，随着固态硬盘技术的发展，使得性能瓶颈从存储设备上的转移到了协议和接口中，于是固态硬盘的几大生产商一起制定了该协议。&lt;/p&gt;
&lt;p&gt;NVMe实际上是非易失性存储器标准，不限于闪存SSD，使用PCIe接口。&lt;/p&gt;
&lt;p&gt;相比于AHCI协议，NVMe的主要特点为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;低时延：存储介质方面，存储介质得到了巨大提升；控制器方面，PCIe主控直接与CPU相连，SATA接口需要南桥控制器中转再连接CPU；软件接口方面，简化了指令路径，提高了并发能力&lt;/li&gt;
&lt;li&gt;高性能：相较于AHCI做出了大量优化&lt;/li&gt;
&lt;li&gt;低功耗：自动功耗状态切换，动态能耗管理&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;nvme工作原理&#34;&gt;NVMe工作原理&lt;/h2&gt;
&lt;p&gt;NVMe作为高层次的协议，原则上可以用于任何接口，一般使用PCIe&lt;/p&gt;
&lt;p&gt;NVMe制定了主机和SSD之间的通信命令，NVMe共有两种命令：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Admin命令：用于主机管理、SSD控制&lt;/li&gt;
&lt;li&gt;I/O 命令：用于主机和SSD之间的数据传输&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Admin命令集有：&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20221105160942779.png&#34; alt=&#34;image-20221105160942779&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;NVM I/O指令集有：&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20221105161030669.png&#34; alt=&#34;image-20221105161030669&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;从主机io请求到ssd&#34;&gt;从主机I/O请求到SSD&lt;/h3&gt;
&lt;p&gt;NVMe有三个重要的机制，Submission Queue、Completion Queue、Doorbell Register。&lt;/p&gt;
&lt;p&gt;SQ和CQ位于主机的内存中，DB位于SSD控制器内部，下图直观的展示了他们之间的位置关系。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20221105161458148.png&#34; alt=&#34;image-20221105161458148&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在主机发起I/O指令时，现将指令放在SQ中，主机会通过修改DB寄存器来通知SSD从SQ中取出需要执行的指令。CQ记录了指令执行的状态（成功/失败），SSD负责向CQ中写入命令的状态。&lt;/p&gt;
&lt;p&gt;NVMe对指令的处理流程可以用下图来概括：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221105162016851.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221105162016851&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主机写请求到SQ&lt;/li&gt;
&lt;li&gt;主机写SQ尾DB寄存器，通知SSD需要执行I/O请求&lt;/li&gt;
&lt;li&gt;SSD控制器取出SQ中的请求&lt;/li&gt;
&lt;li&gt;SSD控制器执行命令&lt;/li&gt;
&lt;li&gt;SSD控制器将请求的状态写入CQ&lt;/li&gt;
&lt;li&gt;SSD控制器发出MSI-X中断，通知主机指令完成&lt;/li&gt;
&lt;li&gt;主机收到中断后，处理CQ，看查请求的完成状态&lt;/li&gt;
&lt;li&gt;主机写入CQ头DB寄存器，告知SSD该指令的结果已经收到并处理&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;submission-queue和completion-queue简介&#34;&gt;Submission Queue和Completion Queue简介&lt;/h2&gt;
&lt;p&gt;由于NVMe的SQ、CQ机制，这两个数据结构必然是成对存在的，也存在多对一的关系。&lt;/p&gt;
&lt;p&gt;对于I/O指令和Admin指令，分别由专有的SQ和CQ进行管理，即Admin SQ/CQ和I/O SQ/CQ。&lt;/p&gt;
&lt;p&gt;I/O SQ和SQ是通过Admin的相关指令来进行创建的。&lt;/p&gt;
&lt;p&gt;主机方面每个CPU核心可以有一个或者多个SQ，但是只能有一个CQ。一个核心创建多个SQ主要是为了提高多线程的并发能力，同时可以对不同SQ设置优先级来提高服务质量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221105165937311.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221105165937311&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于Admin SQ/CQ队列，深度在$2-4096(4K)$&lt;/p&gt;
&lt;p&gt;对于I/O SQ/CQ队列，深度在$2-65536(64K)$，一个SQ命令条目大小为$64Byte$，一个CQ条目大小为$16Byte$，队列深度可以自行配置。&lt;/p&gt;
&lt;p&gt;一个PCIe接口也支持多个lane。&lt;/p&gt;
&lt;p&gt;整个NVMe的工作流程像是如下图所示的两个生产者消费者模型&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20221105234231636.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221105234231636&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;db-doorbell-register&#34;&gt;DB: Doorbell Register&lt;/h2&gt;
&lt;p&gt;在SQ和CQ队列中都有Head和Tail，并且分别有对应的Doorbell，即Head DB和Tail DB，DB在SSD一端，记录SQ和CQ队列头、尾的位置。&lt;/p&gt;
&lt;p&gt;对于SSD而言，要频繁从SQ头取出数据，可以轻松的获取SQ队头的位置，所以SQ Head DB由SSD来维护，而对于SQ的队尾对于主机而言更容易维护，因为要频繁的向队尾插入数据，因此SQ Tail DB由主机维护，SSD根据SQ的头尾可以获取当前队列中&lt;code&gt;有多少请求在等待执行&lt;/code&gt;。同理，CQ Head DB由主机来维护，CQ Tail DB由SSD来维护，SSD根据CQ队列的头尾来判断CQ队列是否还能接受新的&lt;code&gt;请求完成信息&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;从Doorbell的名字来看，还有通知的作用，当Doorbell Register的值改变时，SSD就知道有新的请求需要处理；主机就知道有新的请求已经完成了。&lt;/p&gt;
&lt;p&gt;注意：对于主机来说，DB是可以写不可读的。&lt;/p&gt;
&lt;h2 id=&#34;其他有关nvme的内容&#34;&gt;其他有关NVMe的内容&lt;/h2&gt;
&lt;p&gt;寻址问题（PRP、SGL）&lt;/p&gt;
&lt;p&gt;Namespace&lt;/p&gt;
&lt;p&gt;数据保护问题&lt;/p&gt;
&lt;p&gt;后续有时间再对这些内容进行学习&lt;/p&gt;
&lt;h1 id=&#34;基于闪存的固态盘&#34;&gt;基于闪存的固态盘&lt;/h1&gt;
&lt;p&gt;此前一篇博客整理了有关闪存和固态盘的内容，下文为转载过来的内容&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.ipandai.club/p/%e5%ad%98%e5%82%a8%e6%8a%80%e6%9c%af%e5%9f%ba%e7%a1%80%e5%9b%ba%e6%80%81%e7%a1%ac%e7%9b%98/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Flash Memory &amp;amp;&amp;amp; 固态硬盘&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要由Flash Memory 和FTL组成&lt;/p&gt;
&lt;p&gt;Non-Volatile Memory 提供低延迟持久性的内存/存储，也可以用来做内存&lt;/p&gt;
&lt;p&gt;根据延迟数量级，一般用PCM做内存，Flash Memory做外存&lt;/p&gt;
&lt;h2 id=&#34;flash-memory&#34;&gt;Flash Memory&lt;/h2&gt;
&lt;h3 id=&#34;闪存原理&#34;&gt;闪存原理&lt;/h3&gt;
&lt;h4 id=&#34;类型&#34;&gt;类型&lt;/h4&gt;
&lt;p&gt;NOR闪存&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储密度低&lt;/li&gt;
&lt;li&gt;可字节改写&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NAND闪存（主流）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储密度高&lt;/li&gt;
&lt;li&gt;不可覆盖写&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用于外存需要较高的存储量级，一般用NAND&lt;/p&gt;
&lt;h4 id=&#34;闪存单元&#34;&gt;闪存单元&lt;/h4&gt;
&lt;p&gt;读：电压代表不同数值&lt;/p&gt;
&lt;p&gt;写：电子注入&lt;/p&gt;
&lt;p&gt;相比晶体管添加了浮栅门，保存电子&lt;/p&gt;
&lt;p&gt;原理其实比较简单，非电子系就不做太详细的研究了&lt;/p&gt;
&lt;p&gt;闪存页(4KB,8KB,16KB，读写单元)，阵列中的每一行&lt;/p&gt;
&lt;p&gt;闪存块(擦除单元)，由多个页组成的单元&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922124359358.png&#34; alt=&#34;image-20220922124359358&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;选中行和列，然后将数据加载到Sense Amplifiers&lt;/p&gt;
&lt;p&gt;存储单元有两个阈值的电压，可以根据两个电压的中点作为读电压，2.5V读电压时左边通电，数据为1，右边则不通电，数据为0&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922124755447.png&#34; alt=&#34;image-20220922124755447&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;Pass Through&lt;/p&gt;
&lt;p&gt;选取一个较大的电压，使得所有的单元都接通，数据为1，不影响其他行的状态&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922131649578.png&#34; alt=&#34;image-20220922131649578&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;如图所示的存储结构，在第二行施加2.5V电压，其他行施加5V，最终读取数据为0011&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922134857889.png&#34; alt=&#34;image-20220922134857889&#34; style=&#34;zoom:33%;&#34;/&gt;
&lt;p&gt;上述为SLC，Single Level Cell，单存储单元&lt;/p&gt;
&lt;h4 id=&#34;多比特闪存&#34;&gt;多比特闪存&lt;/h4&gt;
&lt;p&gt;多比特闪存单元MLC，包含2Bits 4个Level的数据&lt;/p&gt;
&lt;p&gt;TLC 3 Bits 8个Level&lt;/p&gt;
&lt;p&gt;QLC 4 Bits 16个Level&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多比特使用格雷码来编码&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;使用格雷码使得相邻单元只有一位差异，方便纠错&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;多比特提高了存储密度，但是提高了错误率，因为施加的电压差距很小。可靠性会降低。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922135516463.png&#34; alt=&#34;image-20220922135516463&#34; style=&#34;zoom:45%;&#34; /&gt;
&lt;p&gt;对于多比特的写，MLC分为高比特和低比特，对于低比特的状态加偏移电压确定高比特，在低比特时需要加的电压较大，操作难度低，运行速度快，在高比特时需要加的电压小，波形的间距小，操作难度高，运行的速度较慢。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922140421405.png&#34; alt=&#34;image-20220922140421405&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;对于多比特的读，先看lower bit，加一次电压，即可筛选出低位的0，1，再加两次电压确定upper bit。因为upper bit为0的在中间部分，为1的在两侧，因此需要在两个分界线分别加一次电压来确定upper bit为多少。&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922141455826.png&#34; alt=&#34;image-20220922141455826&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h4 id=&#34;闪存&#34;&gt;闪存&lt;/h4&gt;
&lt;p&gt;Block的大小的一种配置：&lt;/p&gt;
&lt;p&gt;一行有两个Page，Upper Page和Lower Page，每个单元中，低位构成Lower Page，高位构成Upper Page，有128个单元，128K/8=16KB&lt;/p&gt;
&lt;p&gt;有64列bitlines，一个block的大小即为&lt;code&gt;16KB*64*2=2MB&lt;/code&gt;，一般按照此比例配置Block&lt;/p&gt;
&lt;p&gt;写入时按照固定顺序，写入高低页面相互独立，不能同时写，在写入加压时容易使相邻单元发生数据偏移，要降低错误率&lt;/p&gt;
&lt;p&gt;写入是需要先擦除再写入，擦除整个块&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922143608667.png&#34; alt=&#34;image-20220922143608667&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;闪存特性&#34;&gt;闪存特性&lt;/h4&gt;
&lt;h5 id=&#34;读写粒度&#34;&gt;读写粒度&lt;/h5&gt;
&lt;p&gt;闪存页读写粒度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4KB，8KB，16KB必须全部读取或者写入&lt;/li&gt;
&lt;li&gt;us延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;闪存块擦除力度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2MB擦除&lt;/li&gt;
&lt;li&gt;ms延迟，可以通过FTL来优化&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;不可覆盖写&#34;&gt;不可覆盖写&lt;/h5&gt;
&lt;p&gt;写前需要擦除，读写粒度与擦除粒度不同&lt;/p&gt;
&lt;p&gt;存在64bytes的OOB（out of bound area），保存ECC，用于纠错，容忍写入时部分比特出错&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922162139217.png&#34; alt=&#34;image-20220922162139217&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;有限次擦除&#34;&gt;有限次擦除&lt;/h5&gt;
&lt;p&gt;随着擦除次数的增加，存储单元不能可靠的保持状态（存储数据）。&lt;/p&gt;
&lt;p&gt;氧化层老化变薄，束缚电子能力变弱&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;耐久性 变薄地次数&lt;/li&gt;
&lt;li&gt;保持力 不通电可以放置的时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SLC：10w次&lt;/p&gt;
&lt;p&gt;MLC：1w次&lt;/p&gt;
&lt;p&gt;TLC：1k次&lt;/p&gt;
&lt;p&gt;根据特性来设计FTL固件&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922164523965.png&#34; alt=&#34;image-20220922164523965&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;ftl&#34;&gt;FTL&lt;/h2&gt;
&lt;p&gt;固态硬盘整体构成&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922164728094.png&#34; alt=&#34;image-20220922164728094&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;SSD中的通道可以并行，通道中也可以并行读取，每个Plane中有寄存器，暂时存储准备好的数据。不同单元并行，因此内部带宽大&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922165354311.png&#34; alt=&#34;image-20220922165354311&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;垃圾回收&#34;&gt;垃圾回收&lt;/h3&gt;
&lt;p&gt;page对于OS而言，是写入时的block&lt;/p&gt;
&lt;p&gt;Page三种状态&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;空闲 free page&lt;/li&gt;
&lt;li&gt;有效页 live/valid page&lt;/li&gt;
&lt;li&gt;无效页 dead/invalid page&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922184349456.png&#34; alt=&#34;image-20220922184349456&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;需要擦除无效页，先移走有效页，然后再对一整行进行擦除，转为空闲&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220922184406162.png&#34; alt=&#34;image-20220922184406162&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;时间开销：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复制有效数据到$(R_{Latency}+W_{Latency})*N$，N是移动page的数量&lt;/li&gt;
&lt;li&gt;擦除产生的开销 ms级延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;gc策略&#34;&gt;GC策略&lt;/h4&gt;
&lt;p&gt;要解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;何时启动GC&lt;/li&gt;
&lt;li&gt;选中那些/多少Block进行GC&lt;/li&gt;
&lt;li&gt;有效的页如何被转写&lt;/li&gt;
&lt;li&gt;新数据写到哪里&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GC的时间开销：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;块擦除的时间 ms&lt;/li&gt;
&lt;li&gt;有效页的复制时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;贪心策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找到脏页最多的block来进行擦除&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Hot/Cold 数据隔离，分组问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;磨损均衡&#34;&gt;磨损均衡&lt;/h3&gt;
&lt;p&gt;优化寿命，有静态和动态策略&lt;/p&gt;
&lt;p&gt;静态：周期性的调整冷热数据存储的位置&lt;/p&gt;
&lt;p&gt;冷热数据的分区：将冷数据放在一起，热数据放在一起&lt;/p&gt;
&lt;h3 id=&#34;ftl简介&#34;&gt;FTL简介&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;维护映射，虚拟地址到物理地址&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用SRAM存储映射&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向上层隐藏擦除操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;避免原地更新-&amp;gt;异地更新&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新一个新页面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高性能的垃圾回收和擦除&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OOB有物理地址到虚拟地址的映射，用于掉电恢复，这里引用一段&lt;a class=&#34;link&#34; href=&#34;https://pages.cs.wisc.edu/~remzi/OSTEP/file-ssd.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;wisc的OSTEP中的一段解释（44 Flash- based SSD）&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;OOB保存的在每个页中映射信息，当掉电或者重启时用它在内存中重建映射&lt;/p&gt;
&lt;p&gt;为了防止在重建时扫码整个SSD，可以使用日志或者检查点的方式来加速这个过程&lt;/p&gt;
&lt;p&gt;大致看了一下OSTEP，是有关操作系统的一本非常好的书，希望以后有时间读一下&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220924141132556.png&#34; alt=&#34;image-20220924141132556&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;page-level-ftl&#34;&gt;Page-Level FTL&lt;/h3&gt;
&lt;p&gt;原理类似OS中的页表，由Logical Page Number查询页表得到Physical Page Number&lt;/p&gt;
&lt;p&gt;缺点是页表占用很大的空间&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220924152322358.png&#34; alt=&#34;image-20220924152322358&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;block-level-ftl&#34;&gt;Block-Level FTL&lt;/h3&gt;
&lt;p&gt;保持Block 到Block的映射&lt;/p&gt;
&lt;p&gt;先查找到对应的Block，在根据offset得到page，块内的页码偏移offset是固定的&lt;/p&gt;
&lt;img src=&#34;https://blog.ipandai.club/img/image-20220924152259043.png&#34; alt=&#34;image-20220924152259043&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;优点是占用空间很小，缺点是GC负载增加&lt;/p&gt;
&lt;p&gt;原因：offset在不同Block中保持不变，在异地更新时，要选择其他block中相同的offset进行写入，如果选中的block已经存在数据，需要把数据迁移。【TODO 这里讲得不是非常的清楚】&lt;/p&gt;
&lt;h3 id=&#34;hybrid-ftl&#34;&gt;Hybrid FTL&lt;/h3&gt;
&lt;p&gt;对写入分为新/旧数据，新写入的数据用Page-Level Mapping效率高，写入Log Blocks作为缓冲，之后再更新到Data Blocks&lt;/p&gt;
&lt;p&gt;旧数据因为写入后更新相对不频繁，使用Block-Level Mapping，写入Data Blocks&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;NNSS实验室暑期实习文档&lt;/li&gt;
&lt;li&gt;《Linux内核设计与实现》&lt;/li&gt;
&lt;li&gt;《深入理解Linux内核》&lt;/li&gt;
&lt;li&gt;《Linux设备驱动程序》&lt;/li&gt;
&lt;li&gt;《深入浅出SSD：固态存储核心技术、原理与实战》&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.enterprisestorageforum.com/hardware/nand-dram-sas-scsi-and-sata-ahci-not-dead-yet/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;NAND, DRAM, SAS/SCSI and SATA/AHCI: Not Dead, Yet | Enterprise Storage Forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.quora.com/Linux-Kernel/What-is-the-major-difference-between-the-buffer-cache-and-the-page-cache-Why-were-they-separate-entities-in-older-kernels-Why-were-they-merged-later-on/answer/Robert-Love-1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【Quora】Robert Love对缓冲区缓存和页面缓存的主要区别是什么的回答？为什么它们在较旧的内核中是独立的实体？为什么后来他们被合并了？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://pages.cs.wisc.edu/~remzi/OSTEP/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【OSTEP】Operating Systems: Three Easy Pieces (wisc.edu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1xE411T7Dy/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【bilibili 清华大学】存储技术基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ext4.wiki.kernel.org/index.php/Ext4_Howto&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org Ext4文件系统介绍】Ext4 Howto - Ext4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://btrfs.wiki.kernel.org/index.php/Status&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org Btrfs文件系统现状】Status - btrfs Wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kernel.org/doc/html/latest/filesystems/ext4/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org Ext4文件系统设计】ext4 Data Structures and Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast15/technical-sessions/presentation/lee&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【USENIX FAST&#39;21】F2FS: A New File System for Flash Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://wiki.archlinux.org/title/Ext4_%28%e7%ae%80%e4%bd%93%e4%b8%ad%e6%96%87%29#%e5%85%b3%e9%97%ad%e5%b1%8f%e9%9a%9c&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【archlinux.org ArchWiki】Ext4 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-ssd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【Red Hat Customer Portal | SSD discard】Chapter 21. Solid-State Disk Deployment Guidelines Red Hat Enterprise Linux 7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.51cto.com/u_15061941/3859244&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【mq-deadline】mq-deadline调度器原理及源码分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://nan01ab.github.io/2019/02/Kyber.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【Kyber I/O】Kyber IO Scheduler of Linux · Columba M71&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
