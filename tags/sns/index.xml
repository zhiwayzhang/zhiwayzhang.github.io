<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>SNS on Coding_Panda&#39;s Blog</title>
        <link>https://blog.ipandai.club/tags/sns/</link>
        <description>Recent content in SNS on Coding_Panda&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 17 Mar 2023 23:30:48 +0800</lastBuildDate><atom:link href="https://blog.ipandai.club/tags/sns/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[FAST&#39;23] HadaFS: A File System Bridging the Local and Shared Burst Buffer for Exascale Supercomputers</title>
        <link>https://blog.ipandai.club/p/fast23-hadafs-a-file-system-bridging-the-local-and-shared-burst-buffer-for-exascale-supercomputers/</link>
        <pubDate>Fri, 17 Mar 2023 23:30:48 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/fast23-hadafs-a-file-system-bridging-the-local-and-shared-burst-buffer-for-exascale-supercomputers/</guid>
        <description>&lt;p&gt;HadaFS，一个为超算提供本地和共享Burst Buffer的文件系统&lt;/p&gt;
&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;现代的超算通过SSD来实现Burst Buffer（BB）layer。&lt;/p&gt;
&lt;p&gt;根据部署位置，BB可以分为两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local BB，作为本地硬盘部署在每个计算节点上，有高伸缩性和性能&lt;/li&gt;
&lt;li&gt;Shared BB，部署在专用节点上，可以被多个计算节点访问，可以共享数据，部署成本低&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HadaFS已经部署在了神威新一代超算（Sunway New- generation Supercomputer，SNS）中。支持最大600000用户，最大I/O带宽3.1TB/s。&lt;/p&gt;
&lt;p&gt;Burst Buffer作为数据加速层，一般使用SSD。自2016年起，越来越多的超算开始使用Burst Buffer。&lt;/p&gt;
&lt;p&gt;对于local BB，有一些局限性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由于难以进行数据共享，Local BB不适用于所有场景，例如N-1 I/O mode，所有进程共享一个文件、workflow&lt;/li&gt;
&lt;li&gt;由于超算程序之间的I/O负载差异较大，而数据密集型应用的比例较低，造成了大量的资源浪费&lt;/li&gt;
&lt;li&gt;随着超算规模的扩大，local BB的部署成本未来会急剧升高&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相比于Local BB，Share BB便于数据共享，部署成本低，但是在超算上部署也面临许多问题。LPCC将SSD集成到Lustre FS Client，提高read/write性能的一种缓存技术，LPCC存储在Lustre client SSD中的数据必须先刷新到Lustre server才能进行共享。BeeOND类似于LPCC，继承了BeeGFS的可伸缩性和缓存共享限制。&lt;/p&gt;
&lt;p&gt;超算的发展使得并行I/O需求增加，BB架构相较于传统GFS有着同样的高性能，但是容量较小，因此BB要与GFS进行整合。目前的BB架构数据迁移的效率很低，浪费了很多计算资源，因此高伸缩性的BB数据管理和迁移也是目前要解决的问题。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，作者提出了BB File System&amp;ndash;HadaFS。&lt;/p&gt;
&lt;p&gt;基于share BB，结合了local BB的伸缩性、性能优势和share BB的数据共享、部署成本低的优点。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HadaFS提供Localized Triage Architecture（LTA）局部分类架构，解决了shared BB伸缩性不足的问题，实现了超大规模的扩展和数据共享，LTA将所有HadaFS server构建为一个共享存储池，可以在client和server之间灵活的控制并发问题，来保证数据共享。&lt;/li&gt;
&lt;li&gt;HadaFS提出了一个运行时的user-level接口，来保证来自client的I/O请求可以被最近的server处理，类似local BB。&lt;/li&gt;
&lt;li&gt;为了解决由POSIX接口强一致性导致的性能问题，HadaFS提出了一种包含三种元数据同步机制的全路径索引方法，来解决传统文件系统在复杂元数据的管理上的问题，以及文件系统和应用I/O行为不匹配的问题。使用KV的方法来代替传统的目录树结构。&lt;/li&gt;
&lt;li&gt;HadaFS集成了数据管理工具，帮助用户管理BB中的数据，完成BB和GFS之间的高效数据迁移。&lt;/li&gt;
&lt;li&gt;提出Hadash，在BB中提供高效的数据查询，加速BB和传统超算存储的数据迁移。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;0x01-motivation--bg&#34;&gt;0x01 Motivation &amp;amp;&amp;amp; Bg&lt;/h1&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;h3 id=&#34;bb可伸缩性和应用行为的矛盾&#34;&gt;BB可伸缩性和应用行为的矛盾&lt;/h3&gt;
&lt;p&gt;随着超算百亿亿次计算记录的打破，尖端超算中的I/O并行可以达到数十万，加大了BB在伸缩性上的压力。&lt;/p&gt;
&lt;p&gt;目前的一些顶尖超算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frontier使用独立的硬件来分别构造local BB和shared BB，需要使用大量的SSD和高昂的建设维护成本。&lt;/li&gt;
&lt;li&gt;Fugaku使用shared BB，使用软件来提供存储服务，local BB和shared BB使用不同的name space。这种方式是静态的，很难控制在高并发情况下的I/O竞态问题。&lt;/li&gt;
&lt;li&gt;Summit部署local BB，支持数据在应用之间的共享，基于GFS，效率较低。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于shared BB既可以用于计算节点，也可以用于数据转发节点，作者相信shared BB更适合超级计算机。&lt;/p&gt;
&lt;h3 id=&#34;复杂元数据管理与应用行为的不匹配&#34;&gt;复杂元数据管理与应用行为的不匹配&lt;/h3&gt;
&lt;p&gt;传统的文件系统需要考虑兼容性，因此严格遵循POSIX协议。超算中，计算节点普遍使用read/write，执行目录树访问的次数较少。&lt;/p&gt;
&lt;p&gt;因此减轻对POSIX的实现成为很多文件系统的优化方向。由于应用程序的种类很多，如何减少POSIX接口面临巨大挑战。&lt;/p&gt;
&lt;p&gt;Wang等人分析了一些HPC应用的行为，整理了几种一致性语义，如下表Table 1&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322160638470.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322160638470&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong consistency&lt;/li&gt;
&lt;li&gt;Commit consistency&lt;/li&gt;
&lt;li&gt;Session consistency&lt;/li&gt;
&lt;li&gt;Eventual consistency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;系统支持的一致性等级越高，系统的适应性越强。也就是说，当一个系统能够保证数据在不同节点之间保持一致，那么它就能更好地适应不同的应用场景和需求。&lt;/p&gt;
&lt;p&gt;需要灵活地选择合适的一致性语义来平衡需求和BB系统的性能。&lt;/p&gt;
&lt;h3 id=&#34;低效的数据管理&#34;&gt;低效的数据管理&lt;/h3&gt;
&lt;p&gt;虽然使用BB加速了I/O，但是BB的利用率很低。&lt;/p&gt;
&lt;p&gt;BB仅用于I/O的加速，并不永久的存储数据，同时BB的容量相较于GFS很小，因此要考虑BB和GFS之间高效的数据传输。&lt;/p&gt;
&lt;p&gt;BB和GFS之间的数据迁移/传输有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;透明迁移，软件自动以blocks或者file的形式将BB中的数据迁移到GFS，造成了大量不必要的数据传输&lt;/li&gt;
&lt;li&gt;非透明迁移，计算节点来实现数据迁移，导致计算资源在数据迁移时产生空闲。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两种数据传输方式都会异步地提前将数据从GFS加载到BB，以达到预取的目的。&lt;/p&gt;
&lt;p&gt;然而他们都不能支持用户在程序运行时动态的管理BB数据迁移，不利于提高BB的利用率。&lt;/p&gt;
&lt;h2 id=&#34;bg-神威超算架构&#34;&gt;Bg-神威超算架构&lt;/h2&gt;
&lt;p&gt;SNS神威超算的架构图如下&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322164156234.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322164156234&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每个计算节点包含一个神威多核处理器SW26010P（自研），采用多种架构，有6个Core Groups，390个计算核心。通过环网相连接。&lt;/p&gt;
&lt;p&gt;整个系统有超过100000个SW26010P处理器，通过一种Fat-tree 网络 SWnet连接。&lt;/p&gt;
&lt;p&gt;计算节点与I/O转发节点连接，I/O转发节点提供I/O请求的转发或存储介质。&lt;/p&gt;
&lt;p&gt;SNS使用类似太湖之光的软件架构（LWFS+Lustre），通过独立的存储网络来链接存储节点，以提供I/O请求转发。&lt;/p&gt;
&lt;p&gt;当提供存储服务时，SNS使用HadaFS，I/O转发节点作为HadaFS的server，使用NVMe SSD来处理用户的I/O。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我个人感觉，Burst Buffer就类似于I/O栈中的Page Cache，HadaFS处理的就是Buffer I/O。&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x02-design&#34;&gt;0x02 Design&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;下图为HadaFS的架构，包括HadaFS client，HadaFS server和数据管理工具。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322170237986.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322170237986&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;HadaFS作为shared BB文件系统，提供所有client的global view。&lt;/p&gt;
&lt;p&gt;HadaFS Client运行于计算节点，并提供静态/动态链接库，用于拦截并转发应用程序的POSIX I/O请求到HadaFS Server，HadaFS client的生命周期取决于应用程序。&lt;/p&gt;
&lt;p&gt;HadaFS不支持move，rename，link操作，这些操作很少在并行系统中使用。&lt;/p&gt;
&lt;p&gt;HadaFS Server运行于Burst Buffer节点，包含NVMe SSD，提供全局的数据和元数据存储服务。&lt;/p&gt;
&lt;p&gt;在HadaFS中，每个文件与两种服务器有关。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据存储服务器：通过NVMe SSD上的基本文件系统存储HadaFS文件数据。&lt;/li&gt;
&lt;li&gt;元数据存储服务器：通过高性能的数据库存储HadaFS文件的元数据（使用RocksDB）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hadash是数据管理工具，运行于用户登录节点，用于管理HadaFS和GFS之间的数据迁移。&lt;/p&gt;
&lt;h2 id=&#34;lta-localized-triage-architecture&#34;&gt;LTA (Localized Triage Architecture)&lt;/h2&gt;
&lt;p&gt;kernel在实现完整POSIX接口和处理I/O时有较大的开销，kernel bypass是一种常用手段。&lt;/p&gt;
&lt;p&gt;在高并发的情况下，kernel bypass可能会导致服务不稳定。&lt;/p&gt;
&lt;p&gt;例如，计算节点有24CPU核心，所有的进程只需要进行一次挂载就可以访问运行于kernel mode的文件系统client。相反，每个进程需要在用户层挂载一个文件系统client。两种方法各有优点和局限性，HadaFS通过LTA将两种优点进行整合。&lt;/p&gt;
&lt;p&gt;HadaFS直接挂载Client到应用，实现kernel bypass。&lt;/p&gt;
&lt;p&gt;为了防止单个server处理的FS client过多，HadaFS采用每个client只连接一个server的方法。&lt;/p&gt;
&lt;p&gt;对于一个HadaFS client，与之相连的HadaFS server被称为bridge server，bridge server负责处理client的所有I/O请求，并根据client I/O request的offset和size写入数据到文件。每个文件对应在bridge server ext4 文件系统中的一个独立文件。当client需要访问其他服务器的数据时，文件必须通过bridge server转发。&lt;/p&gt;
&lt;p&gt;如果一个bridge server的空间已满，所有的client将自动切换到其他server。&lt;/p&gt;
&lt;p&gt;为了保证大部分I/O请求在bridge server中处理并减少转发的次数，HadaFS提出了&lt;code&gt;mount(mount_point, Seq)&lt;/code&gt;接口来允许应用在必要时（&lt;strong&gt;自己选择bridge server有优势时&lt;/strong&gt;）控制bridge server的选择。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mount_point表示挂载点，在HadaFS中这是一个文件路径的前缀&lt;/li&gt;
&lt;li&gt;Seq可以根据&lt;strong&gt;应用数据共享模式&lt;/strong&gt;、&lt;strong&gt;网络拓扑&lt;/strong&gt;、&lt;strong&gt;适应应用程序数据和系统架构并行性的其他因素&lt;/strong&gt;来灵活设置。Seq有三种类型：
&lt;ul&gt;
&lt;li&gt;应用程序&lt;code&gt;MPI_RANK&lt;/code&gt;，client将会按照轮询的方式连接server。适用于应用被多次提交到不同的计算节点的情况，来保证每个应用可以准确连接到原先的bridge server，减少数据转发。&lt;/li&gt;
&lt;li&gt;计算节点ID，用于匹配计算节点和BB节点的拓扑结构，保证计算节点可以将数据存储到网络中最近的BB节点。&lt;/li&gt;
&lt;li&gt;根据应用数据分发和共享需要设置，每个client可以显示指定要连接的server，应用可以通过灵活的控制client到server的映射改善数据访问的效率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LTA通过bridge server作为local BB，还实现了shared BB的功能（数据共享）。&lt;/p&gt;
&lt;p&gt;mount_point，Seq可以通过环境变量设置，因此，HadaFS可以通过在应用程序开始之前加载HadaFS lib以提前读取环境变量来支持用户的透明挂载。&lt;/p&gt;
&lt;p&gt;为了充分利用HadaFS的高性能，尤其是read性能，建议用户执行实现client-to-server的映射关系，减少数据转发。在挂载HadaFS之后，应用可以直接调用POSIX文件接口来实现I/O。&lt;/p&gt;
&lt;h2 id=&#34;namespace--metadata-handing&#34;&gt;Namespace &amp;amp;&amp;amp; metadata handing&lt;/h2&gt;
&lt;p&gt;为了改善伸缩性和性能，HadaFS不使用目录树的索引机制，使用类似CHFS和Vesta的full-path索引。&lt;/p&gt;
&lt;p&gt;对一个HadaFS文件，其数据保存在client中的bridge server，元数据存储位置由路径的hash决定。&lt;/p&gt;
&lt;p&gt;文件的元数据以KV的方式存储，每个文件路径都有全局唯一的ID作为key。&lt;/p&gt;
&lt;p&gt;HadaFS client直接通过文件绝对路径的前缀来检查文件是否符合要求。&lt;/p&gt;
&lt;p&gt;当多个文件需要访问，负载可以分发到多个服务器，改善元数据访问的性能。&lt;/p&gt;
&lt;p&gt;HadaFS的元数据兼容Linux的权限结构，包括name，ino，owner，mode，timestamp等。HadaFS将这些信息分成四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;维护文件的创建信息，name，owner，mode等&lt;/li&gt;
&lt;li&gt;维护文件的访问信息，file size，modification time，access time等&lt;/li&gt;
&lt;li&gt;❌因为使用全局唯一ID，HadaFS不需要维护，ino，stdev等信息&lt;/li&gt;
&lt;li&gt;文件分段的位置信息，是一个有序链表，链表通过offset排序，每个元素包括server name，fragment offset，size，writing time等信息&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HadaFS server维护两种元数据数据库，数据结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322200851411.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322200851411&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;一个数据库为Local Metadata Database LMDB，保存第一和第四种元数据类别，文件的local id（LIB）是文件的本地路径。&lt;/p&gt;
&lt;p&gt;另一种为Global Metadata Database，保存第一、二、四种元数据（第三种不需要维护），HadaFS文件的元数据存储在唯一的GMDB，通过文件的全路径hash索引。&lt;/p&gt;
&lt;p&gt;两种数据库基于RocksDB，该数据库不支持多线程写，但这并不是性能瓶颈。&lt;/p&gt;
&lt;p&gt;两种元数据库的Key构造包含：用户的UID、GID、PATH，GID和UID用于前缀字符串检索。&lt;/p&gt;
&lt;p&gt;对于多对多N-N I/O模式，每个client写入到独立的file，并且存储在LMDB的元数据和存储在GMDB的第一、四类元数据匹配。&lt;/p&gt;
&lt;p&gt;对于多对一N-1I/O模式，多个client需要共享一个文件，可能使用不同的HadaFS bridge server，这时，GMDB负责从多个LMDB合并文件的元数据。&lt;/p&gt;
&lt;p&gt;在文件read/write时，LMDB记录文件元数据的改变，维护一个包含本地数据段位置信息的有序链表，并将数据发送到相应的GMDB。GMDB负责维护全局的文件数据段链表，来保证全局的数据共享。&lt;/p&gt;
&lt;h2 id=&#34;hadafs-io-control--data-flow&#34;&gt;HadaFS I/O Control &amp;amp;&amp;amp; Data Flow&lt;/h2&gt;
&lt;p&gt;介绍HadaFS种的控制流和数据流。下图展示了3个HadaFS Client和3个server的关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230322204336470.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230322204336470&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Client A发起I/O请求，创建文件F1并写入100MB数据。数据将会直接写入Client A的Bridge server：X&lt;/li&gt;
&lt;li&gt;Server X写入F1文件的元数据信息和位置信息到LMDB&lt;/li&gt;
&lt;li&gt;基于F1的文件路径，处理好F1的元数据信息。Server X写入F1文件的元数据信息和位置信息到Server Y上的GMDB。&lt;/li&gt;
&lt;li&gt;Client C发起读取文件F1的I/O请求&lt;/li&gt;
&lt;li&gt;Client C的Bridge Server Z接受I/O请求，从Server Y根据F1的路径获取F1的元数据和位置信息&lt;/li&gt;
&lt;li&gt;Server Z从Server X读取数据，并转发到Client C&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于局部写和全局读是有利的，尤其是对于需要频繁输出检查点的应用。&lt;/p&gt;
&lt;p&gt;对于读敏感的应用可以自行维护client到bridge server的映射表，减少数据转发，通过mount接口实现高性能。&lt;/p&gt;
&lt;p&gt;HadaFS除了限制了每个server连接的client数量，减少了性能抖动，还为存储系统充分支持应用程序的并行性奠定了基础。&lt;/p&gt;
&lt;h2 id=&#34;data-management-tool&#34;&gt;Data Management Tool&lt;/h2&gt;
&lt;p&gt;现有的BB方案，例如LPCC，可能会导致迁移大量的临时数据。Datawarp要求应用程序在其源代码或脚本中指定BB和GFS之间的迁移，这通常是一种静态迁移方法，并要求计算节点参与迁移。&lt;/p&gt;
&lt;p&gt;HadaFS中，用户使用Hadash以目录树的视图来获取和管理文件，视图根据功能分为两类，元数据信息查询和数据迁移。&lt;/p&gt;
&lt;p&gt;元数据信息查询提供如下指令：ls，du，find，grep等。ls和find可以在目录树视图查询文件信息。Hadash从metadata-数据库获取信息，并以Linux shell中常用的命令的形式展示这些信息。&lt;/p&gt;
&lt;p&gt;其他触发数据迁移的指令，例如rm、get、put等，Hadash通过Redis pipeline发送指令到HadaFS server中的数据管理模块，然后每个HadaFS server的数据管理模块使用LMDB处理本地数据请求，并行地执行这些指令。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Redis Pipeline就是一组Redis命令的组装，避免频繁的执行命令的Request/Response，类似事务的概念，还可以使用Lua脚本实现类似的功能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是从HadaFS到GFS进行数据迁移的控制流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323103315198.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323103315198&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户通过UI发起数据管理指令到Hadash server&lt;/li&gt;
&lt;li&gt;Hadash Server接收并转发指令到BB节点的所有Hadash代理&lt;/li&gt;
&lt;li&gt;Hadash代理解析指令，通过LMDB获取文件相应的链表信息，在本地SSD读取文件&lt;/li&gt;
&lt;li&gt;最终，Hadash代理写入这些文件到GFS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当所有的文件写入完成时，Hadash代理通过另一个Redis Pipeline返回success，Hadash将通知用户迁移已经完成。&lt;/p&gt;
&lt;p&gt;如果将要被迁移文件正持续追加写入，Hadash将持续拷贝新写入的数据，类似于Linux默认的数据拷贝机制cp。&lt;/p&gt;
&lt;p&gt;Hadash使用前缀匹配方法来展示目录树，可以直接通过LMDB本地执行，减少了对GMDB的影响。&lt;/p&gt;
&lt;p&gt;Hadash使用分布式管理方法来实现数据的本地化管理。该方式主要的瓶颈在GFS，性能根据BB节点的数量增长。&lt;/p&gt;
&lt;h2 id=&#34;hadafs优化&#34;&gt;HadaFS优化&lt;/h2&gt;
&lt;h3 id=&#34;持久化语义和元数据优化&#34;&gt;持久化语义和元数据优化&lt;/h3&gt;
&lt;p&gt;HadaFS采用宽松的持久化语义，不支持在client和server中缓存数据，而是基于ext4文件系统缓存机制，持久化语义取决于元数据的同步。&lt;/p&gt;
&lt;p&gt;HadaFS为不同的应用场景提出了三种元数据的同步机制，避免出现传统文件系统复杂的设计。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;mode 1:异步地更新所有元数据（eventual consistency），在文件的打开、删除、读写期间，所有的操作都先在bridge server本地执行，然后元数据将被异步从LMDB更新到GMDB。该模式的性能最高，相当于为计算节点提供本地存储，适用于没有数据依赖性的场景&lt;/li&gt;
&lt;li&gt;mode 2:同步地更新部分元数据，异步地更新另一部分元数据（session/commit consistency semantics）。第一类元数据（name，owner）将在文件被创建时同步地更新。第二类元数据（file size，modify time）将在文件读写时被异步地进行更新，或通过flush操作同步地更新。该模式为默认方式，可以平衡读写文件读写的性能&lt;/li&gt;
&lt;li&gt;mode 3:在所有的open、read、write操作时同步元数据（strong consistency semantics，HadaFS不支持覆盖写）。所有的server要先获取文件位置来保证第一类元数据的同步，诸如open、write、read、flush的操作需要同步第二类元数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HadaFS使用全路径索引和bridge server，不需要分布式锁，对于使用N-N I/O的应用，不会产生数据冲突，然而对于N-1写模式，HadaFS需要先将数据写入到每个Bridge Server，因此不支持覆盖写，原子写只在mode 3中支持。&lt;/p&gt;
&lt;h3 id=&#34;优化共享文件&#34;&gt;优化共享文件&lt;/h3&gt;
&lt;p&gt;LTA架构适合N-N I/O模型，对于N-1 I/O模型，HadaFS使用类似ADIOS BP的文件布局。先将数据写入到独立的bridge server，再由GMDB维护文件的元数据（BP group index）。一个共享的文件可以存储在多个服务器中，read/write可以被转换成并发的read/write。&lt;/p&gt;
&lt;p&gt;HadaFS以文件的形式存储数据，因此可能会产生碎片。例如，100000个进程并发地写同一个文件，每个进程写入6次，在完全随机的情况下可能产生600000个文件碎片。HadaFS使用一个根据offset排序的链表，合并相同bridge server相邻的段位置信息。读写时的段插入和获取操作时间复杂度为LogN，N为文件段的数量。&lt;/p&gt;
&lt;p&gt;所有的元数据同步机制支持N-1模式，元数据只存储在GMDB中。&lt;/p&gt;
&lt;h3 id=&#34;避免干扰&#34;&gt;避免干扰&lt;/h3&gt;
&lt;p&gt;超算中不同的任务之间存在资源竞争，造成I/O干扰。HadaFS中主要是由于不同的client共享一个server，可以通过用户自行决定连接的server，改善该问题。&lt;/p&gt;
&lt;p&gt;HadaFS还提供了自动调节工具，通过监控系统来优化I/O，自动指定连接的BB资源，设置环境变量，选择合适的元数据同步机制，达到隔离BB资源的目的。&lt;/p&gt;
&lt;h2 id=&#34;超算中的hadafs&#34;&gt;超算中的HadaFS&lt;/h2&gt;
&lt;p&gt;HadaFS已经在SNS中部署了一年多，下图为HadaFS的部署架构图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323151800866.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323151800866&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每个I/O转发节点有两个HadaFS server，每个HadaFS server使用一个NVMe SSD来支持文件数据的存储和元数据（LMDB、GMDB）。&lt;/p&gt;
&lt;p&gt;容错的代价是很高的，应用会周期性写入检查点来减少数据恢复的开销，如果一个BB节点失效，只要SSD没有损坏，HadaFS就可以恢复，为了减少数据恢复的成本，HadaFS周期性地备份关键数据到GFS，在HadaFS部署的一年多时间里，总共产生了15次BB节点的失效，其中没有SSD的故障。&lt;/p&gt;
&lt;h1 id=&#34;0x03-evaluation&#34;&gt;0x03 Evaluation&lt;/h1&gt;
&lt;h2 id=&#34;数据转发性能&#34;&gt;数据转发性能&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323223311503.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323223311503&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于大气科学研究模型NEMO，HadaFS性能如下，说明了对于需要共享文件的程序，HadaFS可以提升其吞吐量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323223349489.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323223349489&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;元数据访问性能&#34;&gt;元数据访问性能&lt;/h2&gt;
&lt;p&gt;MDTest&lt;/p&gt;
&lt;p&gt;Client-Server比例为256:1&lt;/p&gt;
&lt;p&gt;竞品为BeeGFS，配置与Hada相同，GFS使用132 OSS和4 MDS&lt;/p&gt;
&lt;p&gt;测试了Create/Stat/Remove操作的OPS&lt;/p&gt;
&lt;p&gt;BeeGFS由于集群管理服务的限制不能达到65536进程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323224828257.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323224828257&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据访问性能&#34;&gt;数据访问性能&lt;/h2&gt;
&lt;p&gt;IOR测试I/O带宽&lt;/p&gt;
&lt;p&gt;竞品为BeeGFS和GFS，配置同metadata测试&lt;/p&gt;
&lt;p&gt;BeeGFS和mode1、mode2性能相当，但是不能达到65536进程数&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323225137305.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323225137305&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用VPIC-I/O和BD-CATS-I/O测试共享文件的读写访问性能&lt;/p&gt;
&lt;p&gt;HadaFS和BeeGFS使用16个服务器，可以看出低进程情况下二者性能相当，但是BeeGFS的伸缩性较差，HadaFS通过LTA来避免I/O冲突。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323225545079.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323225545079&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;数据迁移性能&#34;&gt;数据迁移性能&lt;/h2&gt;
&lt;p&gt;HadaFS使用256台服务器&lt;/p&gt;
&lt;p&gt;Datawarp使用4096进程&lt;/p&gt;
&lt;p&gt;随着文件大小和数量的增加，HadaFS的性能逐渐优于Datawarp&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323230150281.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323230150281&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;io干扰&#34;&gt;I/O干扰&lt;/h2&gt;
&lt;p&gt;使用五种程序进行测试：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DNDC：农业生态系统的生物地球化学程序&lt;/li&gt;
&lt;li&gt;CAM：气候模拟、全球大气模型&lt;/li&gt;
&lt;li&gt;Shentu：高伸缩性图引擎&lt;/li&gt;
&lt;li&gt;WRF：天气预测系统&lt;/li&gt;
&lt;li&gt;APT：动态粒子仿真程序&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图深色色块代表程序速度降低的指数，每一行代表左侧的应用受到下放程序的影响&lt;/p&gt;
&lt;p&gt;由于可以灵活的选择与client连接的server，可以避免一定的I/O干扰&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.ipandai.club/img/image-20230323230453622.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20230323230453622&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;0x04-conclusion&#34;&gt;0x04 Conclusion&lt;/h1&gt;
&lt;p&gt;HadaFS已经用于新一代的神威超算中，基于Shared Burst Buffer，使应用既可以单点访问BB，也可以通过HadaFS实现对文件同时进行读写，并实现了高性能。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
