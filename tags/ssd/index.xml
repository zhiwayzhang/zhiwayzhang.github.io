<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>SSD - Tag - Coding Panda&#39;s Blog</title>
    <link>https://zhiwayzhang.github.io/tags/ssd/</link>
    <description>SSD - Tag - Coding Panda&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</managingEditor>
      <webMaster>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</webMaster><lastBuildDate>Sun, 09 Oct 2022 13:44:18 &#43;0800</lastBuildDate><atom:link href="https://zhiwayzhang.github.io/tags/ssd/" rel="self" type="application/rss+xml" /><item>
  <title>Linux Block I/O Stack简介</title>
  <link>https://zhiwayzhang.github.io/posts/linuxblockiostack/</link>
  <pubDate>Sun, 09 Oct 2022 13:44:18 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/linuxblockiostack/</guid>
  <description><![CDATA[本文将从操作系统内核层面去介绍Linux I/O 栈
==涉及Linux内核的部分主要为Linux 2.6==
Linux I/O 栈概述 Linux I/O 栈的简图如下
下面分别简述各个层次的功能
应用程序层 应用程序层通过系统调用向VFS发起I/O请求
VFS层 VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。
来自应用程序的请求到达VFS后，VFS会创建包含I/O请求信息的结构体提交给块I/O层。
Linux在文件系统层提供三种IO访问的形式：Buffered IO、MMap 、Direct I/O。
Buffered I/O：这种访问情况下文件数据需要经过设备、Page Cache、用户态缓存，需要进行两次的拷贝动作。
MMap：内存映射技术，将内存中的一部分空间与设备进行映射，使得应用程序能够向访问普通内存一样对文件进行访问。使文件数据仅经过设备、Page Cache即可直接传递到应用程序。
Direct I/O：采用Direct IO的方式可以让用户态直接与块设备进行对接，跳过了Page Cache，从硬盘和用户态中拷贝数据，提高文件第一次读写的效率，若之后需要重复访问同一数据，需要消耗比利用Page Cache更多的时间，一般用于数据库系统。
块I/O层 系统能够随机访问固定大小的数据片的设备被称为块设备，最常见的块设备为硬盘（机械硬盘，固态盘），对块设备的访问通常是在其内部安装文件系统。操作系统为了对其访问和保证性能，需要一个子系统来对块设备和对块设备的请求进行管理。
SCSI底层 &amp;&amp; 设备驱动层 块I/O层将请求发往SCSI层，SCSI就开始真实处理这些IO请求，但是SCSI层又对其内部按照功能划分了不同层次：
SCSI高层：高层驱动负责管理disk，接收块I/O层发出的IO请求，打包成SCSI层可识别的命令格式，继续往下发
SCSI中层：中层负责通用功能，如错误处理，超时重试等
SCSI低层：底层负责识别物理设备，将其抽象提供给高层，同时接收高层派发的SCSI命令，交给物理设备处理
更正：对于传统的块设备而言，服务器通过SCSI协议、SAS接口连接，个人电脑通过AHCI协议/SATA接口连接，目前主流的发展方向为NVMe协议/M.2接口。
物理外设层 在I/O中一般为磁盘、固态硬盘等存储设备
VFS简介 Intro VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。
VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。
VFS中有四个最重要的数据结构，分别是dentry、file、inode、superblock，下文对他们分别进行介绍。
Directory Entry Cache (dcache) VFS提供了open，stat，chmod等系统调用，需要向他们提供文件的路径名，VFS会在dcache中去查询。
dcache提供了非常快的查找机制将路径名转换到一个具体的dentry目录项，缓存的目录项存储在RAM中并且不会持久化到硬盘。
在没有命中缓存时，VFS会通过查找和加载inode来创建dentry缓存，最终实现可以通过path name查找到对应的dentry目录项。
The Inode Object 每个独立的dentry都有一个指向一个inode的指针，inode一般保存在disc（块设备文件系统）中或者内存中（虚拟文件系统）。disc中的inode在被请求或修改时会复制到内存中，并在修改后回写到disc。多个dentry可以指向同一个inode（硬链接）
对于查找inode的请求，VFS在父目录的inode调用lookup函数。
The File Object 在打开一个文件时，需要分配一个文件结构体（内核层面对file descriptor的实现），结构体内容的创建通过dentry指针来获取，文件结构体存储在进程的文件描述符表中。
对文件的读写、关闭通过用户空间的fd来操作正确的文件结构。
只要文件打开，它就会保持对dentry的使用状态，这同时意味着inode仍在使用。
The Dentry Object 目录项纪录子文件和子目录的名称]]></description>
</item>
<item>
  <title>【存储技术基础】固态硬盘</title>
  <link>https://zhiwayzhang.github.io/posts/storagessd/</link>
  <pubDate>Thu, 22 Sep 2022 09:36:42 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/storagessd/</guid>
  <description><![CDATA[主要由Flash Memory 和FTL组成
Non-Volatile Memory 提供低延迟持久性的内存/存储，也可以用来做内存
根据延迟数量级，一般用PCM做内存，Flash Memory做外存
0x00 Flash Memory 闪存原理 类型 NOR闪存
存储密度低 可字节改写 NAND闪存（主流）
存储密度高 不可覆盖写 用于外存需要较高的存储量级，一般用NAND
闪存单元 读：电压代表不同数值
写：电子注入
相比晶体管添加了浮栅门，保存电子
原理其实比较简单，非电子系就不做太详细的研究了
闪存页(4KB,8KB,16KB，读写单元)，阵列中的每一行
闪存块(擦除单元)，由多个页组成的单元
选中行和列，然后将数据加载到Sense Amplifiers
存储单元有两个阈值的电压，可以根据两个电压的中点作为读电压，2.5V读电压时左边通电，数据为1，右边则不通电，数据为0
Pass Through
选取一个较大的电压，使得所有的单元都接通，数据为1，不影响其他行的状态
如图所示的存储结构，在第二行施加2.5V电压，其他行施加5V，最终读取数据为0011
上述为SLC，Single Level Cell，单存储单元
多比特闪存 多比特闪存单元MLC，包含2Bits 4个Level的数据
TLC 3 Bits 8个Level
QLC 4 Bits 16个Level
多比特使用格雷码来编码
使用格雷码使得相邻单元只有一位差异，方便纠错
多比特提高了存储密度，但是提高了错误率，因为施加的电压差距很小。可靠性会降低。
对于多比特的写，MLC分为高比特和低比特，对于低比特的状态加偏移电压确定高比特，在低比特时需要加的电压较大，操作难度低，运行速度快，在高比特时需要加的电压小，波形的间距小，操作难度高，运行的速度较慢。
对于多比特的读，先看lower bit，加一次电压，即可筛选出低位的0，1，再加两次电压确定upper bit。因为upper bit为0的在中间部分，为1的在两侧，因此需要在两个分界线分别加一次电压来确定upper bit为多少。
闪存 Block的大小的一种配置：
一行有两个Page，Upper Page和Lower Page，每个单元中，低位构成Lower Page，高位构成Upper Page，有128个单元，128K/8=16KB
有64列bitlines，一个block的大小即为16KB*64*2=2MB，一般按照此比例配置Block
写入时按照固定顺序，写入高低页面相互独立，不能同时写，在写入加压时容易使相邻单元发生数据偏移，要降低错误率
写入是需要先擦除再写入，擦除整个块
特性 读写粒度 闪存页读写粒度：]]></description>
</item>
<item>
  <title>[ATC&#39;19] Asynchronous I/O Stack: A Low-latency Kernel I/O Stack for Ultra-Low Latency SSDs</title>
  <link>https://zhiwayzhang.github.io/posts/asyncio/</link>
  <pubDate>Wed, 17 Aug 2022 10:30:41 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/asyncio/</guid>
  <description><![CDATA[Intro 优化I/O的方法
用户层面直接调用外部存储设备，需要应用包含文件系统的调用，臃肿，同时不同应用和用户间的冲突问题。 优化操作系统内核的I/O 栈 使用轮询来减少上下文切换的开销 在底层减少一半的中断处理 分散I/O指令 I/O block调度机制 Related Work 减少内核的开销
减少中断处理的后半部分 是用轮询技术而非中断，减少上下文切换 混合轮询 基于SSD的闪存随机读写简化调度策略 在NVMe固件中进行调度 对高优先级的任务，提供不同的IO path支持，最小化IO path的开销 修改存储接口
分散/分散 IO合并多个IO到一个指令，减少往返次数 移除doorbell机制和完成信号 改善fsync
冲fsync请求发出到收到response，延长数据持久化的时间 在日志提交记录中使用校验和，有效的重叠日志写入和块写入 提出写守序系统调用，重叠的fsync效果相同，当应用需要使用fsync时，关于IO的操作将同步进行 用户层直接访问外设，存在隔离、保护等安全问题
Motivation 背景 现状：
I/O 请求过程中太多的步骤 页面缓存分配和索引 DMA，一系列数据结构的创建 目前的 ULL SDD实现了低于10微妙的IO延迟，然而操作系统内核产生的延迟没有明显变化
本文专注于：
Linux内核中的read()和write()+fsync() 基于NVMe SSD的Ext4文件系统 For Read Path 研究发现许多剩余的操作不必在设备I/O之前或之后执行
此类操作可以在设备I/O操作进行时执行
因为此类操作大多独立于设备I/O操作，因此考虑让这些操作与IO重叠
For Write Path 缓冲写write()，并不发起IO请求，不能异步处理
由于fsync的回写机制和文件系统崩溃一致性（日志系统），包含部分IO请求
由于文件系统带来的三次IO操作
数据块写 jbd2发起写入日志Block I/O 提交Block I/O 这些IO的创建，涉及众多过程（block分配，请求缓冲页，创建和提交bio，设置DMA地址），因此可以让CPU将这些前置操作在IO请求发起前预执行。
For Lightweight Block Layer 传统Block Layer涉及过多过程，推迟了IO指令提交给设备的时间
因为ULL SSD的高速随机IO性能和低速的顺序IO，请求重排的效果很低]]></description>
</item>
<item>
  <title>[OSDI&#39;19] Flashshare: Punching Through Server Storage Stack from Kernel to Firmware for Ultra-Low Latency SSDs</title>
  <link>https://zhiwayzhang.github.io/posts/flashshare/</link>
  <pubDate>Wed, 17 Aug 2022 10:30:30 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/flashshare/</guid>
  <description><![CDATA[超低延迟固态硬盘从内核到固件的服务器存储堆栈
个别名词解释 the 99^th percentile 超过统计数据99%的数是多少
blk-mq Linux Multiqueue block layer 内核对ssd随机I/O的优化
message signaled interrupt (MSI)
1.摘要 flash share
在内核中，扩展了系统堆栈的数据结构，传递应用程序的属性（？），包括内核层到SSD固件。
对于给定的属性，FlashShare的块层管理IO调度并处理NVMe中断。
评估结果表明，FLASHSHARE可以将共同运行应用程序的平均周转响应时间分别缩短22%和31%。
1.0 Intro 1.1 现状 网络服务提供商，满足服务级别协议SLA，延迟敏感
某个段时间短可能有大量请求涌入，供应商会超额配置机器以满足SLA
现状：该场景并不常见，因此大部分情况下服务器的资源占用率非常低，能耗比低。
为了解决利用率低，服务器会运行离线的数据分析应用，延迟不敏感，以吞吐量为导向。
因此，在运行了多个进程的服务器上，I/O延迟增高，满足SLA非常困难。
现有的ULL SSD相较于NVMe SSD可以减少10倍的延迟
但是这些ULL SSD在同时运行多个进程下高强度压榨服务器的时候，不能充分利用ULL SSD的优势/表现一般。
the 99th percentile 是0.8ms（apache）
但是当服务器同时运行pagerank的时候，延迟会增加228.5%。
原因：略
从固件到内核优化堆栈的存储。
内核级别的增强：
两个挑战
Linux的blk-mq导致I/O请求队列化，引入延迟 NVMe的队列机制没有对I/O优先级的策略，因此，来自离线应用的IO请求容易阻塞在线应用的紧急请求，造成延迟。 对于latency critical的请求，绕过NVMe的请求队列。同时令NVMe的驱动通过知晓每个应用的延迟临界匹配NVMe的提交和请求队列。
固件层设计：
​	即使内核级的优化保证了延迟敏感的请求可以获得高优先级，但如果基础固件不了解延迟临界值，ULL特性（类似内存的性能）无法完全暴露给用户。本文中重新设计了I/O调度和缓存的固件，以直接向用户暴露ULL特性。将ULL SSD的集成缓存进行分区，并根据工作负载的属性对每个I/O服务独立的分配缓存。固件动态的更新分区大小并以精细粒度调整预取I/O粒度。
ULL SSD的新中断处理服务： ​	当前的NVMe中断机制没有对ULL I/O服务优化。轮询方法（Linux 4.9.30）消耗了大量的CPU资源去检查I/O服务的完成情况。当轮询在线交互服务的IO请求完成状态时，flashShare使用一个仅对离线应用程序使用消息信号中断的选择性中断服务程序Select-ISR。
​	通过将NVMe队列和ISR卸载到硬件加速器中来进一步优化NVMe completion routine。
​	各种仿真实验后效果不错，效率提高了22%和31%。
2.0 Background 2.1 存储内核栈 Linux文件系统IO]]></description>
</item>
</channel>
</rss>
