<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>File System - Tag - Coding Panda&#39;s Blog</title>
    <link>https://zhiwayzhang.github.io/tags/file-system/</link>
    <description>File System - Tag - Coding Panda&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</managingEditor>
      <webMaster>zhiweizhang@hust.edu.cn (Zhiwei Zhang)</webMaster><lastBuildDate>Thu, 14 Dec 2023 21:48:26 &#43;0800</lastBuildDate><atom:link href="https://zhiwayzhang.github.io/tags/file-system/" rel="self" type="application/rss+xml" /><item>
  <title>使用Filebench测试ZNS&#43;F2FS</title>
  <link>https://zhiwayzhang.github.io/posts/filebench/</link>
  <pubDate>Thu, 14 Dec 2023 21:48:26 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/filebench/</guid>
  <description><![CDATA[<p>本文的内容主要参考了filebench在<code>USENIX ;login:'16</code>上的文章（正值filebench 1.5发布）和官方仓库的wiki，filebench从2005年开源到今天已经过去了18年了，github最后一次commit截止在了3年前，之后也没有维护过了，不过filebench目前已经比较完善了，也得到了学术界比较广泛的认可，到今天依然还能拿来跑一些benchmark。本篇文章主要目的是解释一些filebench中的参数，几个预定义工作负载，最后来介绍如何使用filebench来测试ZNS的性能。</p>]]></description>
</item>
<item>
  <title>[FAST&#39;23] More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba</title>
  <link>https://zhiwayzhang.github.io/posts/pangu/</link>
  <pubDate>Thu, 02 Mar 2023 15:11:40 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/pangu/</guid>
  <description><![CDATA[[FAST'23] More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba
FAST'23 会议论文翻译，《不仅仅是容量:盘古面向性能的演变》
本论文讲述了Pangu存储系统是如何随着硬件及商业需求，去演变提供更高的性能的，存储服务的I/O延迟达到了100-us。盘古的演变主要有两个部分：
Phase 1: 盘古通过优化文件系统并设计了用户端的存储操作系统，积极引入高速SSD和Remote Direct Memory Access(RDMA)网络技术。因此，盘古在有效降低了I/O延迟的同时，还提高了吞吐量和IOPS。 Phase 2: 盘古从面向卷的存储供应商转变为面向性能。为了适应这一商业模式的改变，盘古使用足够多的SSD和25Gbps-100Gbps的RDMA带宽更新了基础设施。这引入了一些列的关键设计，包括减少流量放大，远程直接缓存访问，和CPU计算卸载，来保证盘古完全获得基于硬件升级所带来的性能提升。 除了技术上的创新，作者还分享了盘古发展过程中的运营经验，并讨论了其中的重要教训。
0x00 Intro 盘古的开发始于2009年，目前已经是阿里巴巴集团和阿里云统一存储平台。盘古为阿里的核心业务提供了可伸缩性、高性能和可靠性。
Elastic Block Storage(EBS), Object Storage Service(OSS), Network-Attached Storage(NAS), PolarDB, MaxCompute这些云服务基于盘古建立。经过十几年的发展，盘古已经成为了一个拥有ExaBytes并管理万亿文件的全局存储系统。
盘古 1.0: 提供存储容量 Pangu 1.0设计于2009-2015年，通过高性能的CPU和HDD组成，可以提供ms毫秒级别的I/O延迟和Gbps级别的数据中心带宽。
Pangu 1.0基于Linux Ext4设计了一个分布式的内核文件系统和内核TCP，并给予不同种类的存储服务提供多种文件类型支持（Tempfile，LogFile，Random Access file）。
此时正处于云计算的初始阶段，性能受限于HDD性能和网络带宽，相较于更快的访问速度，用户更关注存储容量。
新的硬件，新的设计 自2015年起，为了引入新兴的SSD和RDMA技术，盘古2.0开始设计和开发。盘古2.0的目标是提供100us级别I/O延迟的高性能的存储服务。尽管SSD和RDMA在存储和网络中实现低延迟、高性能的I/O，团队发现：
盘古1.0中使用的多种文件类型，特别是允许随机访问的文件类型，在固态硬盘上的表现很差，而固态硬盘在顺序操作上可以实现高吞吐量和IOPS。 由于数据复制和频繁的中断，内核空间的软件栈无法跟上SSD和RDMA的高IOPS和低I/O延迟。 从以服务器为中心的数据中心架构向资源分散的数据中心架构的范式转变，对实现低I/O延迟提出了额外的挑战。 盘古2.0 Phase 1: 通过重构文件系统与用户空间的存储操作系统来拥抱SSD和RDMA 为了实现高性能和低I/O延迟，盘古2.0首先在其文件系统中的关键组件提出了新的设计。为了简化整个系统的开发和管理，盘古设计了一个统一、追加写入的持久化层。它还引入了一个独立的分块布局，以减少文件写入操作的I/O延迟。
盘古2.0设计了一个用户空间的存储操作系统（USSOS），USSOSS使用一个RTC(Run to completion)线程模型来实现用户空间存储栈和用户空间网络栈的高效协作。它还为高效的CPU和内存资源分配提出了一个用户空间的调度机制。
盘古2.0部署了在动态环境下提供SLA保证的机制。通过这些创新，盘古2.0实现了毫秒级别的P999 I/O延迟。
盘古2.0 Phase 2: 通过基础设施更新和突破网络/内存/CPU瓶颈，适应以性能为导向的业务模式 2018年起，盘古逐渐从容量导向的商业模式转变为性能导向。这是因为越来越多的企业用户将他们的业务转移到了阿里云并且他们对存储性能的延迟和性能有很严格的要求。这在COVID-19疫情爆发之后变得越来越快，为了适应这一商业模式转变和日益增长的用户，盘古2.0需要继续升级基础设施。
用原有的服务器和交换机沿着基于CLOS架构的拓扑结构来对基础设施进行扩容是不经济的，包括高昂的总成本（机架空间、电力、散热、人力成本）和更高的碳排放/环境问题。因此，盘古开发来室内高容量存储服务器（每个服务器96TB SSD）并且升级到了25Gbps-100Gbps的网络带宽。
为了完全获得这些升级带来的性能提升，盘古2.0提出了一系列的技术来处理在{网络/内存/CPU}的性能瓶颈并充分利用其强大的硬件资源。具体来说，盘古2.0通过减少网络流量放大率和动态调整不同流量的优先级来优化网络带宽；通过提出Remote Direct Cache Access(RDCA)来处理内存瓶颈；通过消除数据序列化/反序列化的开销并引入CPU等待指令来同步超线程，以此来解决CPU瓶颈问题。]]></description>
</item>
<item>
  <title>[ATC&#39;20] PinK: High-speed In-storage Key-value Store with Bounded Tails</title>
  <link>https://zhiwayzhang.github.io/posts/pinkkv/</link>
  <pubDate>Sat, 26 Nov 2022 13:40:59 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/pinkkv/</guid>
  <description><![CDATA[0x00 Everyday English preferable 更可取的
consequently 因此
inevitably 必然的
conventional 常规的、传统的
exacerbates 加剧
deteriorates 恶化
deterministic 确定性
0x01 Intro 本文主要提出了Pink，基于LSM tree的KV-SSD，主要通过避免使用布隆过滤器，而是使用少量DRAM来进行优化
传统的KV-SSD主要通过在控制器的DRAM中维护一个hash table来寻址，受到DRAM的容量限制，超出容量的部分会暂时存放在闪存中，因此在寻址时带来了访问闪存的额外开销，同时当发生哈希碰撞时，可能会操作多个闪存，造成了大量不可预测的开销
单纯使用LSM做为替换的话，可以减少DRAM的存储，但是存在一些性能问题：
尾延迟：很多LSM-tree使用布隆过滤器来快速检索，可靠性差，存在尾延迟问题 写放大：由于LSM对KV索引的排序和合并操作，带来了很多对闪存额外的访问，还增大了FTL进行GC的负担 布隆过滤器的重建和KV排序：消耗了大量的CPU资源，带来I/O性能的大幅下降 本文提出了一种基于LSM-tree的KV存储引擎-PinK，解决了上述三个问题。PinK使用了四种技术。
PinK的核心是level pinning，将固定在LSM-tree最顶层的KV索引固定到DRAM中
由于部分level固定在DRAM中，因此可以直接进行compaction，无需占用闪存的I/O，被固定的索引通过电容进行保护，因此也不需要定期进行刷新，这里要注意作者使用DRAM容量很小，使用电容足以保证持久化
GC的主要I/O开销来自对LSM-tree索引的更新，Pink通过延迟LSM-tree中索引更新到compaction阶段来减少GC的I/O开销
在SSD控制器和NAND芯片之间添加比较器，PinK在读取KV对象时执行KV排序，完全消除了compaction的CPU成本
0x02 Bg 了解一些有关NAND闪存SSD、KV-SSD、Hash-based KV-SSD、LSM-Tree-based KV-SSD的现状，并对Hash和LSM-tree进行比较
作者对LSM-tree的设计，L0位于DRAM中，并作为write buffer，其余各层存储在闪存中
0x03 Design Pink有四个主要的数据结构
位于DRAM中的level lists和Skiplist
位于闪存中的meta segments和data segments
在LSM-tree KV 文件系统方面，作者主要参考了WiscKey FAST'16，取代FTL进行GC、索引和磨损均衡
Skiplist Skiplist作为LSM-tree中的L0，起到类似write buffer的作用，暂时保存KV对象，Skiplist中的对象为：&lt;key size, key, value size, value&gt;
当Skiplist满时，会将缓存的对象以meta segment和data segment的形式刷新到L1中，在L1和更低的level中，Key和Value将被分离存储到meta/data segment中，data segment中存储value、key和key size用于GC
Level list 用于跟踪闪存中每个level的meta segment]]></description>
</item>
<item>
  <title>[OSDI&#39;21] Modernizing File System through In-Storage Indexing</title>
  <link>https://zhiwayzhang.github.io/posts/modernfilesystem/</link>
  <pubDate>Sat, 29 Oct 2022 16:48:32 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/modernfilesystem/</guid>
  <description><![CDATA[Questions 核心思想部分需要再思考和梳理下，
为什么采用 KV SSD 替代传统 block SSD 能够解决上述问题？ 其中，利用了 KV SSD 的什么特性？ 对文件系统哪些地方做了什么以优化？ 论文试图解决的问题：
随着存储设备的不断发展，存储设备的性能越来越高，但当前操作系统内核的文件系统在一些操作上并不能够充分利用如今存储设备的性能。
文件系统在执行数据写入时，需要执行大量操作维护元数据、进行硬盘的空间管理、维护文件系统的一致性，工作量大。
核心思想：
使用Key-Value存储接口取代传统的快设备接口。
具体实现：
提出Kevin，分为Kevin=KevinFS + KevinSSD
KevinFS
维护文件和目录到KV对象的映射关系 将POSIX系统调用转译成KV操作指令 保证KV-SSD的一致性 KevinSSD
在存储设备中索引KV对象 对多个KV对象提供事务操作 0x00 Intro Kevin避免了大量元数据的维护带来的I/O放大
不需要日志即可完成崩溃一致性的维护
可以抵御文件分段后造成的性能下降
存储的文件块通过LSM进行分部排序和索引
0x01 BG &amp;&amp; Related Work 传统块设备 提供块粒度（512B or 4KB）的访问
HDD通过维护一个间接表来处理坏块
基于闪存的SSD通过FTL来维护逻辑块到物理地址的映射索引表，以便在异地更新的NAND上模拟可重写介质并且排除坏块。
现有研究缓解了I/O调度、文件碎片化、日志相关问题，没能消除元数据的修改带来的I/O流量
DevFS实现了在存储设备内部的文件系统，直接将接口暴露给应用，调用时不用发生Trap，对于元数据的维护都在存储设备端执行，移除了I/O 栈减少了通信接口的开销。缺点是需要大量的DRAM和多核心的CPU、能提供的功能有限，还限制了快照、重复数据删除等文件系统高级功能的实现。
KV存储可以高效处理元数据和小文件的写入
LSM-Tree LSM Tree有多级，包括$L_1,L_2,&hellip;,L_{h-1},L_{h}$，h是树的高度，对于各级的大小有如下关系，$Size(L_{i+1})&gt;=T*Size(L_i)$
每层都有按Key排序的唯一KV对象。各层之间的Key范围可能会出现重叠。
KV对象首先写入DRAM中的Memtable，当Memtable不为空时，缓存的KV对象将刷新到L1中进行持久化，当L1不为空时，KV对象刷新到L2中，依此类推。在刷新过程中会执行压缩操作来对两层之间的KV对象进行合并和排序，在排序后写回磁盘中。
为了改善压缩操作的I/O开销，可以将所有的value存储在value log中，在LSM tree中只保存value指针，存储&lt;key, value pointer&gt;形式。对于失效的value要考虑垃圾回收问题。
在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。
0x02 Architecture KEVIN分为两个部分
KEVINFS：维护文件、目录到KV对象映射的文件系统
KEVINSSD：索引KV对象到闪存的LSM-tree
KV Command 支持多种KV指令，同时支持事务]]></description>
</item>
<item>
  <title>【存储系统】文件系统</title>
  <link>https://zhiwayzhang.github.io/posts/filesystem/</link>
  <pubDate>Tue, 11 Oct 2022 13:44:49 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/filesystem/</guid>
  <description><![CDATA[0x00 基本概念 文件的基本概念， 包含信息的比特流/块，计算机往往需要处理远高于主存容量的数据，需要使用文件来保存；文件的生命周期更长，不能存储在堆栈中；对文件的数据/信息对外进行分享的媒介。可以用于进程间的信息共享，自由性和容量上都优于共享内存的方式。
文件系统的功能 Namespace 命名空间
文件目录的建立、维护和检索（目录树） 存储管理
空间管理：存储空间的分配和管理（管理空闲的块） 文件块管理：逻辑地址与存储物理地址的映射（logical offset—Physical Block number） 数据保护
文件出现坏块 灾备 可靠性/一致性
0x01 用户视角 文件目录操作 文件操作：
Create/Delete Open/Close 返回File Handle Read/Write Append 追加写入 Seek 移动文件指针 offset Getattr/Setattr 读取/修改inode中保存的文件信息 Rename 目录操作：
Createdir/rmdir Link 软链接、硬链接 讲目录项目指向文件的inode Unlink Readdir 读取子目录以及目录项 Rename 文件目录的构成 文件的组成：
文件内容，字节序列 文件的元数据metadata，文件控制块，Unix系统中称为inode(index node) 目录的组成：
本质是一种特殊的文件，内容是子目录、文件等目录项目 目录也有inode File Handle 创建原文件的一个实例对象，对其进行读写，Offset可以共享给两个File Handle，也可以每个File Handle独有各自的Offset。
Linux VFS中为文件描述符fd，对应一个File Object，包含文件的信息，文件系统管理dentry、inode的缓存，用于查找文件在磁盘上的位置。关于Linux VFS的内容，计划再单独开个博客记录一下。
File Control Block (Inode) 每个文件唯一的id，包含以下结构
Block的信息，文件在磁盘上的位置 数据大小 Ctime创建时间，utime更新时间，access time最后一次访问时间 所有者信息，ACL(access control lists)访问控制 链接情况统计 Dirent - Directory Entry 目录项纪录子文件和子目录的名称]]></description>
</item>
<item>
  <title>Linux Block I/O Stack简介</title>
  <link>https://zhiwayzhang.github.io/posts/linuxblockiostack/</link>
  <pubDate>Sun, 09 Oct 2022 13:44:18 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/linuxblockiostack/</guid>
  <description><![CDATA[本文将从操作系统内核层面去介绍Linux I/O 栈
==涉及Linux内核的部分主要为Linux 2.6==
Linux I/O 栈概述 Linux I/O 栈的简图如下
下面分别简述各个层次的功能
应用程序层 应用程序层通过系统调用向VFS发起I/O请求
VFS层 VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。
来自应用程序的请求到达VFS后，VFS会创建包含I/O请求信息的结构体提交给块I/O层。
Linux在文件系统层提供三种IO访问的形式：Buffered IO、MMap 、Direct I/O。
Buffered I/O：这种访问情况下文件数据需要经过设备、Page Cache、用户态缓存，需要进行两次的拷贝动作。
MMap：内存映射技术，将内存中的一部分空间与设备进行映射，使得应用程序能够向访问普通内存一样对文件进行访问。使文件数据仅经过设备、Page Cache即可直接传递到应用程序。
Direct I/O：采用Direct IO的方式可以让用户态直接与块设备进行对接，跳过了Page Cache，从硬盘和用户态中拷贝数据，提高文件第一次读写的效率，若之后需要重复访问同一数据，需要消耗比利用Page Cache更多的时间，一般用于数据库系统。
块I/O层 系统能够随机访问固定大小的数据片的设备被称为块设备，最常见的块设备为硬盘（机械硬盘，固态盘），对块设备的访问通常是在其内部安装文件系统。操作系统为了对其访问和保证性能，需要一个子系统来对块设备和对块设备的请求进行管理。
SCSI底层 &amp;&amp; 设备驱动层 块I/O层将请求发往SCSI层，SCSI就开始真实处理这些IO请求，但是SCSI层又对其内部按照功能划分了不同层次：
SCSI高层：高层驱动负责管理disk，接收块I/O层发出的IO请求，打包成SCSI层可识别的命令格式，继续往下发
SCSI中层：中层负责通用功能，如错误处理，超时重试等
SCSI低层：底层负责识别物理设备，将其抽象提供给高层，同时接收高层派发的SCSI命令，交给物理设备处理
更正：对于传统的块设备而言，服务器通过SCSI协议、SAS接口连接，个人电脑通过AHCI协议/SATA接口连接，目前主流的发展方向为NVMe协议/M.2接口。
物理外设层 在I/O中一般为磁盘、固态硬盘等存储设备
VFS简介 Intro VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。
VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。
VFS中有四个最重要的数据结构，分别是dentry、file、inode、superblock，下文对他们分别进行介绍。
Directory Entry Cache (dcache) VFS提供了open，stat，chmod等系统调用，需要向他们提供文件的路径名，VFS会在dcache中去查询。
dcache提供了非常快的查找机制将路径名转换到一个具体的dentry目录项，缓存的目录项存储在RAM中并且不会持久化到硬盘。
在没有命中缓存时，VFS会通过查找和加载inode来创建dentry缓存，最终实现可以通过path name查找到对应的dentry目录项。
The Inode Object 每个独立的dentry都有一个指向一个inode的指针，inode一般保存在disc（块设备文件系统）中或者内存中（虚拟文件系统）。disc中的inode在被请求或修改时会复制到内存中，并在修改后回写到disc。多个dentry可以指向同一个inode（硬链接）
对于查找inode的请求，VFS在父目录的inode调用lookup函数。
The File Object 在打开一个文件时，需要分配一个文件结构体（内核层面对file descriptor的实现），结构体内容的创建通过dentry指针来获取，文件结构体存储在进程的文件描述符表中。
对文件的读写、关闭通过用户空间的fd来操作正确的文件结构。
只要文件打开，它就会保持对dentry的使用状态，这同时意味着inode仍在使用。
The Dentry Object 目录项纪录子文件和子目录的名称]]></description>
</item>
<item>
  <title>[ATC&#39;19] Asynchronous I/O Stack: A Low-latency Kernel I/O Stack for Ultra-Low Latency SSDs</title>
  <link>https://zhiwayzhang.github.io/posts/asyncio/</link>
  <pubDate>Wed, 17 Aug 2022 10:30:41 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/asyncio/</guid>
  <description><![CDATA[Intro 优化I/O的方法
用户层面直接调用外部存储设备，需要应用包含文件系统的调用，臃肿，同时不同应用和用户间的冲突问题。 优化操作系统内核的I/O 栈 使用轮询来减少上下文切换的开销 在底层减少一半的中断处理 分散I/O指令 I/O block调度机制 Related Work 减少内核的开销
减少中断处理的后半部分 是用轮询技术而非中断，减少上下文切换 混合轮询 基于SSD的闪存随机读写简化调度策略 在NVMe固件中进行调度 对高优先级的任务，提供不同的IO path支持，最小化IO path的开销 修改存储接口
分散/分散 IO合并多个IO到一个指令，减少往返次数 移除doorbell机制和完成信号 改善fsync
冲fsync请求发出到收到response，延长数据持久化的时间 在日志提交记录中使用校验和，有效的重叠日志写入和块写入 提出写守序系统调用，重叠的fsync效果相同，当应用需要使用fsync时，关于IO的操作将同步进行 用户层直接访问外设，存在隔离、保护等安全问题
Motivation 背景 现状：
I/O 请求过程中太多的步骤 页面缓存分配和索引 DMA，一系列数据结构的创建 目前的 ULL SDD实现了低于10微妙的IO延迟，然而操作系统内核产生的延迟没有明显变化
本文专注于：
Linux内核中的read()和write()+fsync() 基于NVMe SSD的Ext4文件系统 For Read Path 研究发现许多剩余的操作不必在设备I/O之前或之后执行
此类操作可以在设备I/O操作进行时执行
因为此类操作大多独立于设备I/O操作，因此考虑让这些操作与IO重叠
For Write Path 缓冲写write()，并不发起IO请求，不能异步处理
由于fsync的回写机制和文件系统崩溃一致性（日志系统），包含部分IO请求
由于文件系统带来的三次IO操作
数据块写 jbd2发起写入日志Block I/O 提交Block I/O 这些IO的创建，涉及众多过程（block分配，请求缓冲页，创建和提交bio，设置DMA地址），因此可以让CPU将这些前置操作在IO请求发起前预执行。
For Lightweight Block Layer 传统Block Layer涉及过多过程，推迟了IO指令提交给设备的时间
因为ULL SSD的高速随机IO性能和低速的顺序IO，请求重排的效果很低]]></description>
</item>
<item>
  <title>[OSDI&#39;19] Flashshare: Punching Through Server Storage Stack from Kernel to Firmware for Ultra-Low Latency SSDs</title>
  <link>https://zhiwayzhang.github.io/posts/flashshare/</link>
  <pubDate>Wed, 17 Aug 2022 10:30:30 &#43;0800</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/flashshare/</guid>
  <description><![CDATA[超低延迟固态硬盘从内核到固件的服务器存储堆栈
个别名词解释 the 99^th percentile 超过统计数据99%的数是多少
blk-mq Linux Multiqueue block layer 内核对ssd随机I/O的优化
message signaled interrupt (MSI)
1.摘要 flash share
在内核中，扩展了系统堆栈的数据结构，传递应用程序的属性（？），包括内核层到SSD固件。
对于给定的属性，FlashShare的块层管理IO调度并处理NVMe中断。
评估结果表明，FLASHSHARE可以将共同运行应用程序的平均周转响应时间分别缩短22%和31%。
1.0 Intro 1.1 现状 网络服务提供商，满足服务级别协议SLA，延迟敏感
某个段时间短可能有大量请求涌入，供应商会超额配置机器以满足SLA
现状：该场景并不常见，因此大部分情况下服务器的资源占用率非常低，能耗比低。
为了解决利用率低，服务器会运行离线的数据分析应用，延迟不敏感，以吞吐量为导向。
因此，在运行了多个进程的服务器上，I/O延迟增高，满足SLA非常困难。
现有的ULL SSD相较于NVMe SSD可以减少10倍的延迟
但是这些ULL SSD在同时运行多个进程下高强度压榨服务器的时候，不能充分利用ULL SSD的优势/表现一般。
the 99th percentile 是0.8ms（apache）
但是当服务器同时运行pagerank的时候，延迟会增加228.5%。
原因：略
从固件到内核优化堆栈的存储。
内核级别的增强：
两个挑战
Linux的blk-mq导致I/O请求队列化，引入延迟 NVMe的队列机制没有对I/O优先级的策略，因此，来自离线应用的IO请求容易阻塞在线应用的紧急请求，造成延迟。 对于latency critical的请求，绕过NVMe的请求队列。同时令NVMe的驱动通过知晓每个应用的延迟临界匹配NVMe的提交和请求队列。
固件层设计：
​	即使内核级的优化保证了延迟敏感的请求可以获得高优先级，但如果基础固件不了解延迟临界值，ULL特性（类似内存的性能）无法完全暴露给用户。本文中重新设计了I/O调度和缓存的固件，以直接向用户暴露ULL特性。将ULL SSD的集成缓存进行分区，并根据工作负载的属性对每个I/O服务独立的分配缓存。固件动态的更新分区大小并以精细粒度调整预取I/O粒度。
ULL SSD的新中断处理服务： ​	当前的NVMe中断机制没有对ULL I/O服务优化。轮询方法（Linux 4.9.30）消耗了大量的CPU资源去检查I/O服务的完成情况。当轮询在线交互服务的IO请求完成状态时，flashShare使用一个仅对离线应用程序使用消息信号中断的选择性中断服务程序Select-ISR。
​	通过将NVMe队列和ISR卸载到硬件加速器中来进一步优化NVMe completion routine。
​	各种仿真实验后效果不错，效率提高了22%和31%。
2.0 Background 2.1 存储内核栈 Linux文件系统IO]]></description>
</item>
<item>
  <title>【操作系统】系统级IO</title>
  <link>https://zhiwayzhang.github.io/posts/sysio/</link>
  <pubDate>Mon, 03 Jan 2022 10:31:33 &#43;0000</pubDate>
  <author>Zhiwei Zhang</author>
  <guid>https://zhiwayzhang.github.io/posts/sysio/</guid>
  <description><![CDATA[<p>高级别的IO程序，如c中的printf和scanf，c++中的<code>&gt;&gt;</code>和<code>&lt;&lt;</code>，都依赖Unix 系统级IO</p>
<p>CSAPP Ch-10 笔记</p>]]></description>
</item>
</channel>
</rss>
