<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Storage on Coding_Panda&#39;s Blog</title>
        <link>https://blog.ipandai.club/categories/storage/</link>
        <description>Recent content in Storage on Coding_Panda&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 29 Oct 2022 16:48:32 +0800</lastBuildDate><atom:link href="https://blog.ipandai.club/categories/storage/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[OSDI&#39;21] Modernizing File System through In-Storage Indexing</title>
        <link>https://blog.ipandai.club/p/osdi21-modernizing-file-system-through-in-storage-indexing/</link>
        <pubDate>Sat, 29 Oct 2022 16:48:32 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi21-modernizing-file-system-through-in-storage-indexing/</guid>
        <description>&lt;h1 id=&#34;todo-论文在事务一致性恢复的部分&#34;&gt;TODO 论文在事务一致性/恢复的部分&lt;/h1&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions&lt;/h1&gt;
&lt;p&gt;核心思想部分需要再思考和梳理下，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么采用 KV SSD 替代传统 block SSD 能够解决上述问题？&lt;/li&gt;
&lt;li&gt;其中，利用了 KV SSD 的什么特性？&lt;/li&gt;
&lt;li&gt;对文件系统哪些地方做了什么以优化？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;论文试图解决的问题：&lt;/p&gt;
&lt;p&gt;随着存储设备的不断发展，存储设备的性能越来越高，但当前操作系统内核的文件系统在一些操作上并不能够充分利用如今存储设备的性能。&lt;/p&gt;
&lt;p&gt;文件系统在执行数据写入时，需要执行大量操作维护元数据、进行硬盘的空间管理、维护文件系统的一致性，工作量大。&lt;/p&gt;
&lt;p&gt;核心思想：&lt;/p&gt;
&lt;p&gt;使用Key-Value存储接口取代传统的快设备接口。&lt;/p&gt;
&lt;p&gt;具体实现：&lt;/p&gt;
&lt;p&gt;提出Kevin，分为Kevin=KevinFS + KevinSSD&lt;/p&gt;
&lt;p&gt;KevinFS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;维护文件和目录到KV对象的映射关系&lt;/li&gt;
&lt;li&gt;将POSIX系统调用转译成KV操作指令&lt;/li&gt;
&lt;li&gt;保证KV-SSD的一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KevinSSD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在存储设备中索引KV对象&lt;/li&gt;
&lt;li&gt;对多个KV对象提供事务操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;0x00-intro&#34;&gt;0x00 Intro&lt;/h1&gt;
&lt;p&gt;Kevin避免了大量元数据的维护带来的I/O放大&lt;/p&gt;
&lt;p&gt;不需要日志即可完成崩溃一致性的维护&lt;/p&gt;
&lt;p&gt;可以抵御文件分段后造成的性能下降&lt;/p&gt;
&lt;p&gt;存储的文件块通过LSM进行分部排序和索引&lt;/p&gt;
&lt;h1 id=&#34;0x01-bg--related-work&#34;&gt;0x01 BG &amp;amp;&amp;amp; Related Work&lt;/h1&gt;
&lt;h2 id=&#34;传统块设备&#34;&gt;传统块设备&lt;/h2&gt;
&lt;p&gt;提供块粒度（512B or 4KB）的访问&lt;/p&gt;
&lt;p&gt;HDD通过维护一个间接表来处理坏块&lt;/p&gt;
&lt;p&gt;基于闪存的SSD通过FTL来维护逻辑块到物理地址的映射索引表，以便在异地更新的NAND上模拟可重写介质并且排除坏块。&lt;/p&gt;
&lt;p&gt;现有研究缓解了I/O调度、文件碎片化、日志相关问题，没能消除元数据的修改带来的I/O流量&lt;/p&gt;
&lt;p&gt;DevFS实现了在存储设备内部的文件系统，直接将接口暴露给应用，调用时不用发生Trap，对于元数据的维护都在存储设备端执行，移除了I/O 栈减少了通信接口的开销。缺点是需要大量的DRAM和多核心的CPU、能提供的功能有限，还限制了快照、重复数据删除等文件系统高级功能的实现。&lt;/p&gt;
&lt;p&gt;KV存储可以高效处理元数据和小文件的写入&lt;/p&gt;
&lt;h2 id=&#34;lsm-tree&#34;&gt;LSM-Tree&lt;/h2&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221102145019013.png&#34; alt=&#34;image-20221102145019013&#34; style=&#34;zoom:70%;&#34; /&gt;
&lt;p&gt;LSM Tree有多级，包括$L_1,L_2,&amp;hellip;,L_{h-1},L_{h}$，h是树的高度，对于各级的大小有如下关系，$Size(L_{i+1})&amp;gt;=T*Size(L_i)$&lt;/p&gt;
&lt;p&gt;每层都有按Key排序的唯一KV对象。各层之间的Key范围可能会出现重叠。&lt;/p&gt;
&lt;p&gt;KV对象首先写入DRAM中的Memtable，当Memtable不为空时，缓存的KV对象将刷新到L1中进行持久化，当L1不为空时，KV对象刷新到L2中，依此类推。在刷新过程中会执行压缩操作来对两层之间的KV对象进行合并和排序，在排序后写回磁盘中。&lt;/p&gt;
&lt;p&gt;为了改善压缩操作的I/O开销，可以将所有的value存储在value log中，在LSM tree中只保存value指针，存储&lt;code&gt;&amp;lt;key, value pointer&amp;gt;&lt;/code&gt;形式。对于失效的value要考虑垃圾回收问题。&lt;/p&gt;
&lt;p&gt;在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。&lt;/p&gt;
&lt;h1 id=&#34;0x02-architecture&#34;&gt;0x02 Architecture&lt;/h1&gt;
&lt;p&gt;KEVIN分为两个部分&lt;/p&gt;
&lt;p&gt;KEVINFS：维护文件、目录到KV对象映射的文件系统&lt;/p&gt;
&lt;p&gt;KEVINSSD：索引KV对象到闪存的LSM-tree&lt;/p&gt;
&lt;h2 id=&#34;kv-command&#34;&gt;KV Command&lt;/h2&gt;
&lt;p&gt;支持多种KV指令，同时支持事务&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221102154423353.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102154423353&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者将Key限制为256B，value大小没有限制&lt;/p&gt;
&lt;h2 id=&#34;文件和目录的映射&#34;&gt;文件和目录的映射&lt;/h2&gt;
&lt;p&gt;KevinFS只使用三个类型的KV对象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$superblock$：保存文件系统的信息，大小128B&lt;/li&gt;
&lt;li&gt;$meta$：保存文件、目录的元数据，大小256B&lt;/li&gt;
&lt;li&gt;$data$：保存文件数据，unlimited，不超过文件大小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于目录的遍历通过ITERATE&lt;/p&gt;
&lt;p&gt;Key的命名遵循如下规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;meta对象的key组成：
&lt;ul&gt;
&lt;li&gt;前缀&lt;code&gt;m:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;父目录的inode number&lt;/li&gt;
&lt;li&gt;分隔符&lt;code&gt;:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;文件/目录名&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data对象的key组成：
&lt;ul&gt;
&lt;li&gt;前缀&lt;code&gt;d:&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;文件的inode number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key命名规范参考了其他论文（Kai Ren and Garth Gibson. TABLEFS: Enhancing Metadata Efficiency in the Local File System. In Proceedings of the USENIX Annual Technical Conference, pages 145–156, 2013.），本篇文章将其扩展到了存储设备层面&lt;/p&gt;
&lt;p&gt;KevinFS没有实现dentry，如果需要访问一个目录内的所有项目，可以通过&lt;code&gt;ITERATE(m:50:, 2)&lt;/code&gt;，来获取两个父目录inode为50（前缀匹配给定的pattern）的子目录/文件的元数据。&lt;/p&gt;
&lt;p&gt;一个优化：为了防止ITERATE消耗太长时间，建议指定cnt&lt;/p&gt;
&lt;p&gt;一个优化：为了高效处理小文件，KevinFS将总体小于4KB的小文件的元数据和数据内容打包，I/O直接操作meta对象GET/SET来进行读写&lt;/p&gt;
&lt;p&gt;@TODO 一个优化：使用全路径索引来取代基于inode的索引，这提高了基于排序算法的KV存储的扫描性能，主要对seek time开销大的HDD优化比较明显&lt;/p&gt;
&lt;h2 id=&#34;kv对象索引&#34;&gt;KV对象索引&lt;/h2&gt;
&lt;p&gt;KevinSSD基本实现了传统SSD 中FTL的所有功能，将KV对象映射到闪存、分配和释放闪存的空间。&lt;/p&gt;
&lt;p&gt;FTL只做坏块管理和磨损均衡等简单工作。&lt;/p&gt;
&lt;p&gt;每个Level维护一个内存表，来记录Flash中的KV对象。表的每个记录都有&lt;code&gt;&amp;lt;start key, end key, pointer&amp;gt;&lt;/code&gt;，其中指针指向保存KV对象的闪存页面的位置，start key和end key是页面中key的范围。key的范围可以在多个Level上重叠。为了快速查找，所有记录都按开始键排序。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221102165350981.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102165350981&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;meta和data对象的Key和Value会被分离存储。通过&lt;code&gt;&amp;lt;key, value pointer&amp;gt;&lt;/code&gt;的方式进行存储，会有专门的flash page来分别存储这些信息（meta和data对象使用不同的flash page），key-index pages&lt;/p&gt;
&lt;h3 id=&#34;meta-object&#34;&gt;Meta object&lt;/h3&gt;
&lt;p&gt;在对KV对象进行检索时，可能需要在多级进行查找，当在Li不匹配时，查找Li+1。为了减少多级查找的读取，可以使用布隆过滤器进行优化，当检索到目标KV对象时，LSM tree将会返回相应的value，KV分离存储的情况下，将会额外去读一次value log。&lt;/p&gt;
&lt;p&gt;对目录项的更新只需要修改其元数据即可完成，而不需要修改4KB的块&lt;/p&gt;
&lt;h3 id=&#34;data-object&#34;&gt;Data object&lt;/h3&gt;
&lt;p&gt;为了避免对大数据的小范围更新带来的I/O高负载，KevinSSD将data object分割为4KB大小的子对象，使用单独的后缀来对他们进行表示，后缀从0开始，拼接在原始的data object Key后，因此data object的key最终构成为
$$
d:{inode\ \ \ number}:{subobject \ \ number,start\ with \ 0}
$$
不需要中间表进行索引，只需要通过偏移量即可定位要修改的对象&lt;/p&gt;
&lt;p&gt;Data object同样使用保存指针的方式将映射存储在key-index pages中，key-index pages会对键进行排序，因此属于同一个文件的subobject一般会在同一个flash page中。&lt;/p&gt;
&lt;p&gt;==优化==：在查找指针时，会将一整个闪存页内的数据全部取出，存储在控制器的DRAM中，减轻后续查找可能带来的负载&lt;/p&gt;
&lt;h2 id=&#34;缓解索引负载&#34;&gt;缓解索引负载&lt;/h2&gt;
&lt;p&gt;使用LSM-tree由于多级查找带来额外的I/O，传统FTL的映射都保存在DRAM，因此没有额外开销。作者介绍了三种主要原因和解决方案。&lt;/p&gt;
&lt;h3 id=&#34;压缩操作的开销&#34;&gt;压缩操作的开销&lt;/h3&gt;
&lt;p&gt;同上文介绍的方案，通过分离Value可以显著缓解I/O的数据传输负载，对data进行分片我个人感觉反而会加大压缩操作的合并过程，即使分片后键值都是有序的。&lt;/p&gt;
&lt;h3 id=&#34;层级查找的开销&#34;&gt;层级查找的开销&lt;/h3&gt;
&lt;p&gt;由于LSM-tree的特性，需要逐层进行查找，为了防止大量的顺序查找，作者使用布隆过滤器进行了优化。&lt;/p&gt;
&lt;p&gt;同时还缓存了热点K2V索引数据（与目标索引处在同一个flash page中的）&lt;/p&gt;
&lt;p&gt;为了利用大容量SSD中提供的超大DRAM，作者采用压缩存储K2V索引，并在其中插入没有压缩的K2V索引来作为二分查找的参照物。（快速在缓存中查找？不如继续用布隆过滤器）&lt;/p&gt;
&lt;h3 id=&#34;分散的对象带来的开销&#34;&gt;分散的对象带来的开销&lt;/h3&gt;
&lt;p&gt;LSM-tree允许各层之间的key范围重叠，因此有同一个前缀的目录或者文件可能被分配到不同的Level中，对于获取一个目录中所有目录、文件的操作，需要对多个闪存页进行访问。这个问题作者没有给出一个明确的方案（在压缩合并时隐式解决），但是提供了一个用户工具来主动触发合并，效率较高。&lt;/p&gt;
&lt;p&gt;作者对比了各种方法在随机读、局部读（？）、顺序读情况下对闪存页的读取次数，使用布隆过滤器的时候稳定会有一次读取&lt;/p&gt;
&lt;p&gt;使用了KevinSSD后对于SSD而言，I/O延迟有轻微的增加，整体而言效率提高了&lt;/p&gt;
&lt;h1 id=&#34;0x03-implement-vfs&#34;&gt;0x03 Implement VFS&lt;/h1&gt;
&lt;h2 id=&#34;write&#34;&gt;write&lt;/h2&gt;
&lt;p&gt;所有的写相关系统调用可以通过SET和DELETE来实现。&lt;/p&gt;
&lt;p&gt;例如unlink，只需要两次DELETE指令，移除meta和data object。&lt;/p&gt;
&lt;p&gt;SET时现在Memtable中保存一个KV对象，然后持久化到flash中，若Key已存在则丢弃旧的对象&lt;/p&gt;
&lt;p&gt;DELETE时在树上写入一个4B大小的墓碑&lt;/p&gt;
&lt;p&gt;失效的对象（被SET覆盖）和删除的对象在压缩期间被永久删除&lt;/p&gt;
&lt;h2 id=&#34;read&#34;&gt;read&lt;/h2&gt;
&lt;p&gt;通过GET和ITERATE实现&lt;/p&gt;
&lt;p&gt;如下图所示&lt;/p&gt;
&lt;p&gt;执行open系统调用时，查询meta object获取文件的inode，通过GET命令实现&lt;/p&gt;
&lt;p&gt;执行lookup系统调用时，给定一个完整的路径&lt;code&gt;/home/alice/&lt;/code&gt;，从根目录开始获取多个meta object，最终获取目标文件的inode&lt;/p&gt;
&lt;p&gt;执行read系统调用时，将GET一个data object&lt;/p&gt;
&lt;p&gt;执行readdir系统调用时，使用ITERATE，批量获取一组meta objects来获取inode&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221102210344432.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221102210344432&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;相较于Ext4，KevinFS可以更好的防止文件碎片化带来的影响。&lt;/p&gt;
&lt;h1 id=&#34;0x04-crash-consistency&#34;&gt;0x04 Crash Consistency&lt;/h1&gt;
&lt;h2 id=&#34;一致性维护&#34;&gt;一致性维护&lt;/h2&gt;
&lt;p&gt;通过事务即可实现原子化的操作&lt;/p&gt;
&lt;p&gt;事务的隔离？对==fsync==指令单独创建一个小型事务来提高效率。&lt;/p&gt;
&lt;p&gt;KevinFS基于KV事务的特性来维护一致性，不使用日志系统。&lt;/p&gt;
&lt;h2 id=&#34;kv事务的实现&#34;&gt;KV事务的实现&lt;/h2&gt;
&lt;p&gt;使用了三个数据结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务表$TxTable$：记录事务基本信息&lt;/li&gt;
&lt;li&gt;事务日志$TxLogs$：维护事务对象的K2V索引，存储在DRAM或者闪存中&lt;/li&gt;
&lt;li&gt;恢复日志$TxRecovery$：用于恢复和终止事务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是收到BeginTx指令时，KevinSSD在TxTable中创建一个对象，包含TID，当前事务的状态，与事务相关K2V索引的位置，初始状态为RUNNING。当后续指令到达后，KevinSSD将KV索引记录在TxLogs，并缓存在Memtable中。&lt;/p&gt;
&lt;p&gt;当收到EndTx指令后，完成事务的提交，标记事务状态为COMMITTED。将提交的KV索引同步到LSM-tree中&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221103102304181.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221103102304181&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Linux Block I/O Stack简介</title>
        <link>https://blog.ipandai.club/p/linux-block-i/o-stack%E7%AE%80%E4%BB%8B/</link>
        <pubDate>Sun, 09 Oct 2022 13:44:18 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/linux-block-i/o-stack%E7%AE%80%E4%BB%8B/</guid>
        <description>&lt;p&gt;本文将从操作系统内核层面去介绍Linux I/O 栈&lt;/p&gt;
&lt;p&gt;==涉及Linux内核的部分主要为Linux 2.6==&lt;/p&gt;
&lt;h1 id=&#34;linux-io-栈概述&#34;&gt;Linux I/O 栈概述&lt;/h1&gt;
&lt;p&gt;Linux I/O 栈的简图如下&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221106012036118.png&#34; alt=&#34;image-20221106012036118&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;下面分别简述各个层次的功能&lt;/p&gt;
&lt;h2 id=&#34;应用程序层&#34;&gt;应用程序层&lt;/h2&gt;
&lt;p&gt;应用程序层通过系统调用向VFS发起I/O请求&lt;/p&gt;
&lt;h2 id=&#34;vfs层&#34;&gt;VFS层&lt;/h2&gt;
&lt;p&gt;VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。&lt;/p&gt;
&lt;p&gt;来自应用程序的请求到达VFS后，VFS会创建包含I/O请求信息的结构体提交给块I/O层。&lt;/p&gt;
&lt;p&gt;Linux在文件系统层提供三种IO访问的形式：Buffered IO、MMap 、Direct I/O。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Buffered I/O：这种访问情况下文件数据需要经过设备、Page Cache、用户态缓存，需要进行两次的拷贝动作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MMap：内存映射技术，将内存中的一部分空间与设备进行映射，使得应用程序能够向访问普通内存一样对文件进行访问。使文件数据仅经过设备、Page Cache即可直接传递到应用程序。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Direct I/O：采用Direct IO的方式可以让用户态直接与块设备进行对接，跳过了Page Cache，从磁盘和用户态中拷贝数据，提高文件第一次读写的效率，若之后需要重复访问同一数据，需要消耗比利用Page Cache更多的时间，一般用于数据库系统。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;块io层&#34;&gt;块I/O层&lt;/h2&gt;
&lt;p&gt;系统能够随机访问固定大小的数据片的设备被称为块设备，最常见的块设备为硬盘，对块设备的访问通常是在其内部安装文件系统。操作系统为了对其访问和保证性能，需要一个子系统来对块设备和对块设备的请求进行管理。&lt;/p&gt;
&lt;h2 id=&#34;scsi底层--设备驱动层&#34;&gt;SCSI底层 &amp;amp;&amp;amp; 设备驱动层&lt;/h2&gt;
&lt;p&gt;块I/O层将请求发往SCSI层，SCSI就开始真实处理这些IO请求，但是SCSI层又对其内部按照功能划分了不同层次：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SCSI高层：高层驱动负责管理disk，接收块I/O层发出的IO请求，打包成SCSI层可识别的命令格式，继续往下发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SCSI中层：中层负责通用功能，如错误处理，超时重试等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SCSI低层：底层负责识别物理设备，将其抽象提供给高层，同时接收高层派发的SCSI命令，交给物理设备处理&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更正：对于传统的块设备而言，服务器通过SCSI协议、SAS接口连接，个人电脑通过AHCI协议/SATA接口连接，目前主流的发展方向为NVMe协议/M.2接口。&lt;/p&gt;
&lt;h2 id=&#34;物理外设层&#34;&gt;物理外设层&lt;/h2&gt;
&lt;p&gt;在I/O中一般为磁盘、固态硬盘等存储设备&lt;/p&gt;
&lt;h1 id=&#34;vfs简介&#34;&gt;VFS简介&lt;/h1&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;VFS主要向用户空间提供文件系统层面的接口，同时向不同的文件系统实现提供抽象，使他们可以共存。&lt;/p&gt;
&lt;p&gt;VFS将文件系统在用户层面进行抽象，便于用户对文件进行操作。&lt;/p&gt;
&lt;p&gt;VFS中有四个最重要的数据结构，分别是dentry、file、inode、superblock，下文对他们分别进行介绍。&lt;/p&gt;
&lt;h2 id=&#34;directory-entry-cache-dcache&#34;&gt;Directory Entry Cache (dcache)&lt;/h2&gt;
&lt;p&gt;VFS提供了open，stat，chmod等系统调用，需要向他们提供文件的路径名，VFS会在dcache中去查询。&lt;/p&gt;
&lt;p&gt;dcache提供了非常快的查找机制将路径名转换到一个具体的dentry目录项，缓存的目录项存储在RAM中并且不会持久化到磁盘。&lt;/p&gt;
&lt;p&gt;在没有命中缓存时，VFS会通过查找和加载inode来创建dentry缓存，最终实现可以通过path name查找到对应的dentry目录项。&lt;/p&gt;
&lt;h2 id=&#34;the-inode-object&#34;&gt;The Inode Object&lt;/h2&gt;
&lt;p&gt;每个独立的dentry都有一个指向一个inode的指针，inode一般保存在disc（块设备文件系统）中或者内存中（虚拟文件系统）。disc中的inode在被请求或修改时会复制到内存中，并在修改后回写到disc。多个dentry可以指向同一个inode（硬链接）&lt;/p&gt;
&lt;p&gt;对于查找inode的请求，VFS在父目录的inode调用lookup函数。&lt;/p&gt;
&lt;h2 id=&#34;the-file-object&#34;&gt;The File Object&lt;/h2&gt;
&lt;p&gt;在打开一个文件时，需要分配一个文件结构体（内核层面对file descriptor的实现），结构体内容的创建通过dentry指针来获取，文件结构体存储在进程的文件描述符表中。&lt;/p&gt;
&lt;p&gt;对文件的读写、关闭通过用户空间的fd来操作正确的文件结构。&lt;/p&gt;
&lt;p&gt;只要文件打开，它就会保持对dentry的使用状态，这同时意味着inode仍在使用。&lt;/p&gt;
&lt;h2 id=&#34;the-dentry-object&#34;&gt;The Dentry Object&lt;/h2&gt;
&lt;p&gt;目录项纪录子文件和子目录的名称&lt;/p&gt;
&lt;p&gt;每个目录项的结构&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;大小&lt;/th&gt;
&lt;th&gt;字段&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;__le32&lt;/td&gt;
&lt;td&gt;4bytes&lt;/td&gt;
&lt;td&gt;Inode&lt;/td&gt;
&lt;td&gt;inode编号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;__le16&lt;/td&gt;
&lt;td&gt;2bytes&lt;/td&gt;
&lt;td&gt;Rec_len&lt;/td&gt;
&lt;td&gt;目录项的长度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;__u8&lt;/td&gt;
&lt;td&gt;2bytes&lt;/td&gt;
&lt;td&gt;Name_len&lt;/td&gt;
&lt;td&gt;文件名长度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;__u8&lt;/td&gt;
&lt;td&gt;2bytes&lt;/td&gt;
&lt;td&gt;File_type&lt;/td&gt;
&lt;td&gt;文件类型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;char[EXT2_NAME_LEN]&lt;/td&gt;
&lt;td&gt;最大255个字符&lt;/td&gt;
&lt;td&gt;Name&lt;/td&gt;
&lt;td&gt;文件名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所有目录都包含的目录项包括当前目录和上级目录:&lt;code&gt;.&lt;/code&gt;,&lt;code&gt;..&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;要对文件名进行4字节对齐，后面补&lt;code&gt;\0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;所有目录项顺序拼接组成目录信息，在进行查询时可以使用rec_len来计算偏移量，便于查找目录中的文件（顺序遍历）&lt;/p&gt;
&lt;h2 id=&#34;the-superblock-object&#34;&gt;The SuperBlock Object&lt;/h2&gt;
&lt;p&gt;superblock对象表示一个挂载的文件系统，存储有关文件系统的相关信息&lt;/p&gt;
&lt;h3 id=&#34;注册和挂载一个文件系统&#34;&gt;注册和挂载一个文件系统&lt;/h3&gt;
&lt;p&gt;使用如下的API&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;linux/fs.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;extern&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;register_filesystem&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;extern&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;unregister_filesystem&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;文件系统的挂载可以通过&lt;code&gt;-&amp;gt;mount()&lt;/code&gt;方法将新的文件系统挂载到挂载点，当pathname解析解析到挂载点时，会跳转到被挂在文件系统的root。&lt;/p&gt;
&lt;p&gt;内核所注册的文件系统在&lt;code&gt;/proc/filesystems&lt;/code&gt;文件中&lt;/p&gt;
&lt;p&gt;&lt;code&gt;file_system_type&lt;/code&gt;结构的定义如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fs_flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dentry&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mount&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                 &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kill_sb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;super_block&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;module&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;next&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;list_head&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fs_supers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lock_class_key&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_lock_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lock_class_key&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s_umount_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;mount()&lt;/code&gt;方法有以下几个参数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file_system_type&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fs_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dev_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;page-cache&#34;&gt;Page Cache&lt;/h1&gt;
&lt;p&gt;​	页高速缓存(Page Cache)是Linux内核所使用的主要磁盘高速缓存。在绝大多数情况下，内核在读写磁盘时都引用页高速缓存。新页被追加到页高速缓存以满足用户态进程的读请求。如果页不在高速缓存中，新页就被加到高速缓存中，然后用从磁盘读出的数据填充它 。 如 果内存有足够的空闲空间 ，就让该页在高速缓存中长期保留 ，使其他进程再使用该页时不再访问磁盘。&lt;/p&gt;
&lt;p&gt;​	同时， 在把一页数据写到块设备之前，内核首先检查对应的页是否已经在高速缓存中， 如果不在，就要先在其中增加 一个新项，并用要写到磁盘中的数据填充该项。I/O数据的传送并不是马 上开始，而是要延迟几秒之后才对磁盘进行更新，从而使进程有机会对要写人磁盘的数据做进 一步的修改 (内核执行延迟的写操作)。&lt;/p&gt;
&lt;p&gt;​	几乎所有的读写都依赖Page Cache，除非指定了O_DIRECT标志位，此时I/O将使用进程用户态地址空间的缓冲区，一般数据库应用会使用此方式。&lt;/p&gt;
&lt;p&gt;​	对Page Cache中页的识别通过索引节点和在相应文件中的偏移量来实现。&lt;/p&gt;
&lt;h2 id=&#34;主要数据结构&#34;&gt;主要数据结构&lt;/h2&gt;
&lt;p&gt;​	Page Cache中的核心数据结构是&lt;code&gt;address_space&lt;/code&gt;，该对象在页面所属的inode中，由于多个页面可能属于同一个所有者，因此多个页面可能被连接到同一个address space，该对象同时还将所有者的页面和对页面的操作建立连接。&lt;/p&gt;
&lt;p&gt;有关页面的操作主要有以下几个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;==writepage== 写操作，将页写入到磁盘映像&lt;/li&gt;
&lt;li&gt;==readpage== 读操作，将数据从磁盘映像加载到页&lt;/li&gt;
&lt;li&gt;sync_page 在所有者的页面已经准备就绪后，启动I/O数据的传输&lt;/li&gt;
&lt;li&gt;wirtepages 把指定数量的所有者脏页写回磁盘&lt;/li&gt;
&lt;li&gt;==prepare_write== 为写操作做准备（磁盘文件系统使用）&lt;/li&gt;
&lt;li&gt;==commit_write== 完成写操作（磁盘文件系统使用）&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;基数树-radix-tree&#34;&gt;基数树 Radix Tree&lt;/h2&gt;
&lt;p&gt;基数树类似于字典树，通过要查找的ID的二进制序列，来一级一级的进行查找，为了减少树的高度和叶子非结点个树，可以使用2 bit或者4 bit作为树的结点。&lt;/p&gt;
&lt;p&gt;在内核中，Radix Tree的叶子结点保存了指向所有者页描述符的指针。&lt;/p&gt;
&lt;p&gt;Page Cache 中的每个文件都是一棵基数树（radix tree，本质上是多叉搜索树），树的每个节点都是一个页。&lt;/p&gt;
&lt;p&gt;内核通过把页索引转换为Radix Tree中的路径，使用Radix Tree来快速搜索页描述符所在的位置。内核通过页描述符可以确定页面是否为等待刷新到磁盘的脏页或其中数据的I/O传输是否正在进行。&lt;/p&gt;
&lt;p&gt;Radix Tree中的每一个叶子节点指向文件内相应偏移所对应的Cache项&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221101143123364.png&#34; alt=&#34;image-20221101143123364&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;page-cache处理函数&#34;&gt;Page Cache处理函数&lt;/h2&gt;
&lt;h3 id=&#34;find_get_page&#34;&gt;find_get_page()&lt;/h3&gt;
&lt;p&gt;传入 address space对象指针和便宜了，获取自旋锁，在Radix Tree中搜索具有指定偏移量的叶子结点，若成功找到，会增加页面使用计数器，释放自旋锁，并返回页描述符的指针，否则将释放自旋锁并返回NULL。&lt;/p&gt;
&lt;h3 id=&#34;add_to_page_cache&#34;&gt;add_to_page_cache()&lt;/h3&gt;
&lt;p&gt;传入页描述符地址，address space对象的address mapping字段，页索引在地址空间的偏移量，为Radix Tree分配新节点所使用的内存分配标志gfp_mask，插入成功则返回0&lt;/p&gt;
&lt;h3 id=&#34;remove_from_page_cache&#34;&gt;remove_from_page_cache()&lt;/h3&gt;
&lt;p&gt;获取自旋锁、关闭中断后在Radix Tree中删除节点，返回删除页的描述符指针，page-&amp;gt;mapping字段设置为NULL，将所缓存页的page-&amp;gt;mapping-&amp;gt;nrpages计数器值减1，最后释放自旋锁，打开中断。&lt;/p&gt;
&lt;h3 id=&#34;read_cache_page&#34;&gt;read_cache_page()&lt;/h3&gt;
&lt;p&gt;确保Page Cache中包括指定页的最新版本，首先检查页面是否存在，若不存在则新申请分配空间，调用add_to_page_cache插入相应的页描述符，调用lru cache add插入到该地址空间的非活跃LRU链表中。&lt;/p&gt;
&lt;p&gt;在保证页面存在于Page Cache后，使用mark page accessed记录页面已经被访问过，若PG_uptodate标志位0，则页面不是最新的，需要从磁盘重新读取该页&lt;/p&gt;
&lt;h2 id=&#34;buffer-cache&#34;&gt;Buffer Cache&lt;/h2&gt;
&lt;p&gt;在Linux内核2.4版本之前，Buffer Cache和Page Cache是独立的，因为操作系统对硬盘等块设备的读写是基于块的，而非页，文件的数据在Page Cache中缓存，磁盘的块数据在Buffer Cache中缓存。但是这种表示方式效率非常低，自2.4版本的内核开始，二者开始统一表示。&lt;/p&gt;
&lt;p&gt;因为VFS和文件系统都是通过块Block来组织磁盘上的数据，因此Buffer Cache必须存在，目前它作为Buffer Page存储在特定的页中。&lt;/p&gt;
&lt;p&gt;对于文件这种通过Page Cache来表示的数据，通过Page Cache来表示Buffer Cache，而对于文件的metadata，直接使用Buffer Cache来表示。&lt;/p&gt;
&lt;h2 id=&#34;脏页的处理&#34;&gt;脏页的处理&lt;/h2&gt;
&lt;p&gt;在进程修改了页面数据后，该页面就成为了脏页，并将脏页刷新到块设备上的操作延迟，这样可以显著提高系统的性能。&lt;/p&gt;
&lt;p&gt;为了防止硬件错误、掉电等情况导致RAM内容的丢失，以及RAM容量过大的需求，在以下情况主动刷新脏页到磁盘中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Page Cache太满，或脏页数量太多&lt;/li&gt;
&lt;li&gt;脏页已经存在了较长的时间&lt;/li&gt;
&lt;li&gt;进程请求对块设备或文件的变化进行刷新（sync、fsync和fdatasync系统调用）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;文件的访问&#34;&gt;文件的访问&lt;/h1&gt;
&lt;p&gt;文件的访问一般有五种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;==规范模式==：由系统调用read和write来读写，read将阻塞调用进程，直到数据被拷贝到用户地址空间，write则在数据拷贝到Page Cache后立即结束&lt;/li&gt;
&lt;li&gt;==同步模式==：&lt;code&gt;O_SYNC&lt;/code&gt;标志为1，影响写操作（读操作总是阻塞的），将阻塞写操作，直到数据被有效的写入磁盘。&lt;/li&gt;
&lt;li&gt;==内存映射模式==：文件打开后，通过系统调用&lt;code&gt;mmap&lt;/code&gt;将文件映射到内存中，此后文件成为完全保存在RAM中的字节数组。&lt;/li&gt;
&lt;li&gt;==直接I/O模式==：O_DIRECT标志为1，任何读写操作都在用户地址空间和磁盘间进行传输，直接跳过Page Cache。&lt;/li&gt;
&lt;li&gt;==异步模式==：该模式下，文件可以通过一组POSIX API或者Linux系统调用来进行访问，异步模式下数据传输请求不阻塞调用的进程，而是在后台执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;读文件&#34;&gt;读文件&lt;/h2&gt;
&lt;p&gt;内核实现了&lt;code&gt;read()&lt;/code&gt;系统调用为进程提供文件的读取功能，入口为&lt;code&gt;sys_read()&lt;/code&gt;需要提供三个参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文件描述符&lt;/li&gt;
&lt;li&gt;保存数据的缓冲区&lt;/li&gt;
&lt;li&gt;读取字符数目的长度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于VFS而言，首先根据文件描述符编号，内核(使用&lt;code&gt;fs/file_table.c&lt;/code&gt;中的&lt;code&gt;fget_light()&lt;/code&gt;函数)从进程的 &lt;code&gt;task_struct&lt;/code&gt;中找到与之相关的文件实例，&lt;code&gt;file_pos_read()&lt;/code&gt;函数确定读取当前文件的位置，并返回一个参数&lt;code&gt;file-&amp;gt;file_pos&lt;/code&gt;，实际的读取操作通过&lt;code&gt;vfs_read()&lt;/code&gt;函数来执行，该函数会判断文件是否有&lt;code&gt;file-&amp;gt;f_op-&amp;gt;read&lt;/code&gt;方法，从而确定后续使用的函数，若不存在则会调用&lt;code&gt;do_sync_read&lt;/code&gt;函数。最后通过&lt;code&gt;file_pos_write&lt;/code&gt;函数来记录文件新的读写位置，通过修改&lt;code&gt;file_ops&lt;/code&gt;的值来实现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221022151840698.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221022151840698&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;为了提高在文件读取时的性能，Linux kernel提供了缓冲区和缓冲系统。&lt;/p&gt;
&lt;p&gt;Linux对文件的读是基于页的，内核总是一次传送几个完整的数据页。若发起read调用时数据不在RAM中，内核则分配一个新的page frame，并调取文件的相应部分填入其中，将其加入到Page Cache中，最后把请求的数据拷贝到进程的地址空间。&lt;/p&gt;
&lt;p&gt;大多数磁盘文件系统，使用通用函数generic_file_read来实现read系统调用。&lt;/p&gt;
&lt;h3 id=&#34;generic_file_read&#34;&gt;generic_file_read()&lt;/h3&gt;
&lt;p&gt;该函数有如下几个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filp 文件对象的地址&lt;/li&gt;
&lt;li&gt;buf 用户态内存区域的线性地址，用于存放读取出的文件数据&lt;/li&gt;
&lt;li&gt;count 读取字符的个数&lt;/li&gt;
&lt;li&gt;ppos 存放读操作开始处的文件偏移量，一般为&lt;code&gt;filp-&amp;gt;f_pos&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;首先，函数初始化两个描述符：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;局部变量iovec类型的local_iov，包含buf和count字段&lt;/li&gt;
&lt;li&gt;局部变量kiocb类型的kiocb，跟踪正在进行的同步和异步I/O操作的完成状态&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后调用&lt;code&gt;__generic_file_aio_read()&lt;/code&gt;获取上面两个描述符的地址，返回值时文件有效读入的字节数，在该函数返回后，&lt;code&gt;generic_file_read()&lt;/code&gt;也将终止。&lt;/p&gt;
&lt;h3 id=&#34;__generic_file_aio_read&#34;&gt;__generic_file_aio_read()&lt;/h3&gt;
&lt;p&gt;接收四个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kiocb描述符地址：iocb&lt;/li&gt;
&lt;li&gt;iovec描述符数组的地址：iov（描述等待接收数据的用户态缓冲区）&lt;/li&gt;
&lt;li&gt;iovec描述符数组的长度&lt;/li&gt;
&lt;li&gt;存放文件当前指针的变量地址：ppos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该函数在对Page Cach触发read时的执行过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调用access_ok()来检查iovec描述的用户态缓冲区是否合法&lt;/li&gt;
&lt;li&gt;建立读操作描述符，存放与用户缓冲区有关的、正在进行的文件操作的状态&lt;/li&gt;
&lt;li&gt;调用&lt;code&gt;do_generic_file_read()&lt;/code&gt;，给定参数文件对象指针filp，文件偏移量指针ppos，读操作描述符地址、函数file_read_actor()的地址&lt;/li&gt;
&lt;li&gt;返回拷贝到用户态缓冲区的字节数，对应读操作描述符的written字段&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;do_generic_file_read()&lt;/p&gt;
&lt;h2 id=&#34;写流程&#34;&gt;写流程&lt;/h2&gt;
&lt;p&gt;​	很多磁盘文件系统通过通用函数generic_file_write实现write方法。write系统调用负责将用户地址空间中的数据移动到内核数据结构中，然后将数据拷贝到Page Cache中，将这些页标记为脏，然后持久化到磁盘中。&lt;/p&gt;
&lt;p&gt;​	write系统调用的结构与read同样简单。除了用f_op-&amp;gt;write和do_sync_write替换了read中对应的例程之外，二者的代码流程图几乎完全相同。&lt;/p&gt;
&lt;p&gt;​	从形式上看来，sys_write与sys_read的参数相同:一个文件描述符、一个指针变量、一个长度指示(表示为整数)。显然，其语义稍有不同。指针并非指向存储读取数据的缓冲区，而是指向需要 写入文件的数据。长度参数指定了数据的字节长度。&lt;/p&gt;
&lt;h2 id=&#34;预取机制&#34;&gt;预取机制&lt;/h2&gt;
&lt;p&gt;​	很多磁盘的访问都是顺序的。普通文件以相邻扇区成组存放在磁盘上，因此很少移动磁头就可以快速检索到文件。当程序读或拷贝一个文件时，它通常从第一个字节到最后一个字节顺序地访问文件。因此，在处理进程对同一文件的一系列读请求时，可以从磁盘上很多相邻的扇区读取。&lt;/p&gt;
&lt;p&gt;​	预读(Read-ahead)是一种技术，这种技术在于在实际请求前读普通文件或块设备文件的几个相邻的数据页。在大多数情况下，预读能极大地提高磁盘的性能，因为预读使磁盘控制器处理较少的命令，其中的每条命令都涉及一大组相邻的扇区。此外，预读还能提高系统的晌应能力。顺序读取文件的进程通常不需要等待请求的数据，因为请求的数据已经在RAM中了。&lt;/p&gt;
&lt;h2 id=&#34;缓存回写机制&#34;&gt;缓存回写机制&lt;/h2&gt;
&lt;p&gt;缓存回写机制也就是Page Cache对脏页进行回写，将对文件的I/O修改永久持久化到磁盘上的过程，此处不再赘述。后续计划深入了解Linux内核对回写的控制。&lt;/p&gt;
&lt;h1 id=&#34;文件系统&#34;&gt;文件系统&lt;/h1&gt;
&lt;p&gt;文件系统作为操作系统中一个不可或缺的部分，负责在存储设备上管理、存储、获取数据、维护信息。&lt;/p&gt;
&lt;p&gt;在Linux中，内核通过VFS虚拟文件系统向用户提供通用的接口，向下对接各种不同的文件系统，可以有效避免不同文件系统的实现差异对上层带来影响。&lt;/p&gt;
&lt;h2 id=&#34;ext4&#34;&gt;ext4&lt;/h2&gt;
&lt;p&gt;本部分大量内容来自kernel.org的文档&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kernel.org/doc/html/latest/filesystems/ext4/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ext4 Data Structures and Algorithms — The Linux Kernel documentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;ext-文件系统发展历史&#34;&gt;ext 文件系统发展历史&lt;/h3&gt;
&lt;h4 id=&#34;ext1&#34;&gt;ext1&lt;/h4&gt;
&lt;p&gt;Extended file system，扩展文件系统。&lt;/p&gt;
&lt;p&gt;Linux最早使用的文件系统为Minix的文件系统，但是该系统存在文件大小限制的问题，同时性能不佳。在1992年4月，Rémy Card开发了扩展文件系统，首个版本作为Linux中的文件系统一起发行，最大支持2GB空间，同时它还是Linux中第一个使用VFS实现出的文件系统。&lt;/p&gt;
&lt;h4 id=&#34;ext2&#34;&gt;ext2&lt;/h4&gt;
&lt;p&gt;在首个ext文件系统中，文件访问、存在inode修改以及文件内容修改没有使用独立时间戳的问题，同时最大仅支持255个字符的文件名以及2GB的空间，ext2系统除了修复了这个问题外，还在磁盘存储数据结构中预留了很多空间供未来开发使用，具有良好的可拓展性。&lt;/p&gt;
&lt;p&gt;由于块驱动的限制，ext2文件系统最大支持2TB的单个文件。&lt;/p&gt;
&lt;h4 id=&#34;ext3&#34;&gt;ext3&lt;/h4&gt;
&lt;p&gt;ext3在当时性能不是很出众，但是支持从当时最流行的ext2文件系统升级，无需备份和数据恢复，同时有较少的资源占用（CPU开销）。&lt;/p&gt;
&lt;p&gt;ext3相较于ext2增加了&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;日志支持&lt;/li&gt;
&lt;li&gt;文件系统在线增长&lt;/li&gt;
&lt;li&gt;对大目录提供HTree哈希树索引&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ext3提供了三个日志级别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;日志（最低风险）：metadata和文件内容一起写入日志中，由于还要写入文件系统，因此带来额外的性能开销。&lt;/li&gt;
&lt;li&gt;有序（中等风险）：只将metadata写入日志，但是保证metadata提交前，文件内容会被写入。大部份发行版默认的方式。写入过程中崩溃时，文件系统会清除还没有被提交的文件修改/创建，但是对于文件的覆盖写入，可能会导致文件处于新旧文件的中间态。&lt;/li&gt;
&lt;li&gt;回写（最高风险）：只记录metadata到日志，文件内容在日志提交前或者提交后写入。如果在日志提交前写入失败，会导致磁盘出现垃圾。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ext3存在的一些缺陷：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;没有在线的碎片整理工具&lt;/li&gt;
&lt;li&gt;官方缺少压缩工具&lt;/li&gt;
&lt;li&gt;不支持恢复已经删除的文件&lt;/li&gt;
&lt;li&gt;缺少快照支持&lt;/li&gt;
&lt;li&gt;日志中不支持校验和&lt;/li&gt;
&lt;li&gt;使用四个字节存储Unix时间，因此在2038年1月18日之后将无法继续处理文件&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ext4-1&#34;&gt;ext4&lt;/h4&gt;
&lt;p&gt;ext4在Linux 2.6.28中作为功能完整稳定的文件系统发布，ext4文件系统首先兼容了原有的ext3系统，提供安全可靠的快速迁移。同时还有如下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;支持更大的文件系统和文件大小&lt;/p&gt;
&lt;p&gt;以4KB块，目前支持最大&lt;code&gt;1EiB(1024*1024*1024GB)&lt;/code&gt;的文件系统大小和16TiB的文件大小，使用48bit来编址，未来会支持64bit&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;子目录可扩展性&lt;/p&gt;
&lt;p&gt;ext4文件系统支持最多64000个子目录，是ext3文件系统的两倍&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;扩展数据块&lt;/p&gt;
&lt;p&gt;鼓励对大文件划分多个连续数据块，在磁盘上进行连续布局，从而减少维护大量的间接映射，有助于提高性能，减少磁盘碎片&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多块同时分配，&lt;/p&gt;
&lt;p&gt;在ext3文件系统中，文件系统在分配空闲块时，只能一次处理一个块申请，ext4支持多块分配机制，可以一次申请多个块&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;块的延时分配&lt;/p&gt;
&lt;p&gt;借鉴了XFS、ZFS、btrfs等现代文件系统，在执行写入过程时，在以往的文件系统中，都是对写入到cache的数据立即分配相应的block，消耗了大量的时间，延时分配后，只有当数据需要被刷新到磁盘时，才会执行块的分配，可以和上面两个feat进行配合，提高了文件系统的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;快速fsck&lt;/p&gt;
&lt;p&gt;fsck是极其耗时的过程，主要是需要检查文件系统所有的inode。ext4中，在每个group&amp;rsquo;s inode table中都保存了一个空闲inode链表（同时保存了checksum），因此fsck就不必再检查这些inode&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;日志校验和&lt;/p&gt;
&lt;p&gt;日志在磁盘中是很容易出错的部分，添加校验和可以避免文件系统恢复错误的日志带来更大的损失。使用校验和还使得在ext3中日志的两步提交简化为一步&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;无日志模式&lt;/p&gt;
&lt;p&gt;ext4中可以关闭日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在线碎片整理&lt;/p&gt;
&lt;p&gt;使用e4defrag工具手动进行碎片整理&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inode相关feat&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;更大的inode&lt;/p&gt;
&lt;p&gt;从128bytes提高到256bytes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;inode预留&lt;/p&gt;
&lt;p&gt;创建目录时预留一些inode提高性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;纳秒级时间戳&lt;/p&gt;
&lt;p&gt;提高系统的time resolution&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;硬盘空间预分配&lt;/p&gt;
&lt;p&gt;应用可以让文件系统预先分配一部分硬盘上的空间，文件系统会提前创建好相关的数据结构并分配相应的块，后续可以直接写入数据，类似P2P下载时预分配，1.可以防止类似功能的低效率应用级实现；2.减少磁盘碎片；3.对于实时应用来说（延迟敏感，航空工业等），这个特点改善了延迟。此feat通过&lt;code&gt;libc posix_fallocate()&lt;/code&gt;实现&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;默认启用写屏障&lt;/p&gt;
&lt;p&gt;通过消耗一些性能来改善文件系统的完整性，它确保文件系统元数据在磁盘上被正确地写入和排序，即使在掉电时也是如此，对于一些创建大量小文件或者操作元数据的程序，会带来很大的性能影响。如果硬盘是电池供电，通过&lt;code&gt;barrier=0&lt;/code&gt;可以关闭写屏障（会保证安全性）。写屏障保护保证在数据写入&lt;code&gt;缓存（磁盘缓存）&lt;/code&gt;后，先写入日志中的元数据刷新到磁盘，防止出现因为调度策略导致的数据先于日志写入磁盘&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;discard/TRIM&lt;/p&gt;
&lt;p&gt;为SSD提供的TRIM支持。&lt;/p&gt;
&lt;p&gt;对于SSD而言，由于文件系统在删除时只对块标记为空闲，而SSD却并不知道哪些数据块可用，再次写入时会先清除闪存中的数据，再进行写入，要擦除无效页，要先移走有效页，然后再对一整行进行擦除，最后才能执行写入过程，这个现象也被称为&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Write_amplification&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;写入放大(Write Amplification)&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;TRIM主要使得文件系统告知SSD哪些页不再包含有效的数据，有助于提高SSD的寿命和磨损均衡&lt;/p&gt;
&lt;p&gt;随着使用的block数量接近SSD的容量上限，会导致SSD的性能下降，文件系统通过discard指令告知SSD哪些范围内的block已经不再使用，SSD可以将其回收或者用来实现磨损均衡&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据组织&#34;&gt;数据组织&lt;/h3&gt;
&lt;p&gt;ext4文件系统将部分block组织为一个group，默认block大小采用4KB，因此一个group中的block数量为&lt;code&gt;8*block_size_in_bytes=32768 blocks&lt;/code&gt;，空间为128MB。&lt;/p&gt;
&lt;p&gt;ext4部分在磁盘中按小端写入，jdb2日志部分在磁盘中按大端写入。&lt;/p&gt;
&lt;p&gt;ext4在分配磁盘空间时只能分配若干个block，一个block包含磁盘上的若干个扇区，扇区的数量必须是2的幂次。&lt;/p&gt;
&lt;p&gt;不使用扩展布局的文件（通过block映射维护）必须存放在文件系统前$2^{32}$个block中，通过扩展布局来保存的文件，必须存放在前$2^{48}$个block中。&lt;/p&gt;
&lt;p&gt;对于32位和64位的文件系统，各个数据结构的大小限制如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221115165058013.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221115165058013&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221115165128885.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221115165128885&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在一个Block Group中，数据的组织如表格所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Group 0 Padding&lt;/th&gt;
&lt;th&gt;ext4 Super Block&lt;/th&gt;
&lt;th&gt;Group Descriptors&lt;/th&gt;
&lt;th&gt;Reserved GDT Blocks&lt;/th&gt;
&lt;th&gt;Data Block Bitmap&lt;/th&gt;
&lt;th&gt;inode Bitmap&lt;/th&gt;
&lt;th&gt;inode Table&lt;/th&gt;
&lt;th&gt;Data Blocks&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1024 bytes&lt;/td&gt;
&lt;td&gt;1 block&lt;/td&gt;
&lt;td&gt;many blocks&lt;/td&gt;
&lt;td&gt;many blocks&lt;/td&gt;
&lt;td&gt;1 block&lt;/td&gt;
&lt;td&gt;1 block&lt;/td&gt;
&lt;td&gt;many blocks&lt;/td&gt;
&lt;td&gt;many more blocks&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;磁盘中会预留1024 bytes，用于操作系统的引导模块安装，该padding分区只有group 0含有，为了避免supper block损坏导致整个文件系统不可用，在其他block group中还会有备份，如果一个block group不含有冗余备份，那么它将以data block bitmap开头。&lt;/p&gt;
&lt;p&gt;在执行mkfs时，还会在group descriptor描述符分区和data block bitmap分区之间分配一个保留的GDT blocks分区，用于文件系统日后的扩展。&lt;/p&gt;
&lt;p&gt;ext4文件系统中还引入了Flexible Block Groups的概念(flex_bg)，主要思想是将若干个block group组合成一个大的Group，将所有block group中的元数据（inode，bitmap）都集中到第一个block group中，提高对元数据加载和查询的效率，并且使得文件数据在磁盘上连续，更紧凑。一般通过&lt;code&gt;2^sb.s_log_groups_per_flex&lt;/code&gt;个block group来组成一个大的Group。&lt;/p&gt;
&lt;p&gt;自ext3起，ext文件系统就开始使用Meta block groups(META_BG)，是一组只用一个group descriptor来描述的block groups，首个block group不再存储每个block group的描述符，转由meta block group来存储，并且会在其中存储几个冗余副本。&lt;/p&gt;
&lt;p&gt;对于ext4文件系统的读写流程计划在此后单独写一篇文章进行总结。&lt;/p&gt;
&lt;h2 id=&#34;f2fs&#34;&gt;F2FS&lt;/h2&gt;
&lt;p&gt;F2FS文件系统提出于&lt;code&gt;USENIX FAST&#39;21&lt;/code&gt;，&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast15/technical-sessions/presentation/lee&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;F2FS: A New File System for Flash Storage | USENIX&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;全称为Flash-Friendly File System，基于日志结构文件系统(Log-structured File System, LFS)，针对LFS中wandering tree和gc开销大的问题进行了优化。&lt;/p&gt;
&lt;h3 id=&#34;主要特点&#34;&gt;主要特点&lt;/h3&gt;
&lt;p&gt;针对闪存进行了优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;扩大随机写入区域来提高性能，但带来了空间局部性&lt;/li&gt;
&lt;li&gt;尽可能使得文件系统的处理单元与FTL中保持一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;针对wandering tree问题，该问题是因为LFS的脏数据通过追加更新，如果一个数据块变成脏数据，那么其索引块，以及间接索引块都会变成脏块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用node来代替inode和指针块&lt;/li&gt;
&lt;li&gt;通过包含所有node块地址的Node Address Table(NAT)来切断更新递归的传播&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;垃圾回收问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持后台清理&lt;/li&gt;
&lt;li&gt;支持贪心和cost-benefit算法&lt;/li&gt;
&lt;li&gt;为动态/静态冷热数据分离提供multi-head logs&lt;/li&gt;
&lt;li&gt;引入自适应日志来实现高效的块分配&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;btrfs&#34;&gt;Btrfs&lt;/h2&gt;
&lt;p&gt;Btrfs是一个写时复制的文件系统，基于B-tree实现，专注于容错，修复和易于管理，发布于2014年，发展目标是为了取代ext3文件系统，解决ext3的限制。在2021年，Fedora 33宣布将使用Btrfs作为安装时默认的文件系统。而Fedora受到redhat的直接赞助，说明Btrfs在不断发展的过程中，得到了社区中部分用户以及企业的认可，目前还处在测试和不断完善的阶段。&lt;/p&gt;
&lt;p&gt;主要特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于B-Tree维护元数据，插入查询操作高效&lt;/li&gt;
&lt;li&gt;基于COW，提高磁盘寿命&lt;/li&gt;
&lt;li&gt;支持只读/可读快照&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;块io层-与-io调度器&#34;&gt;块I/O层 与 I/O调度器&lt;/h1&gt;
&lt;h2 id=&#34;块设备&#34;&gt;块设备&lt;/h2&gt;
&lt;p&gt;对于Unix系统来说，有着一切皆文件的设计哲学，因此外部设备在操作系统看来是一个设备文件。设备文件共分为两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;块设备：数据可以被随机访问&lt;/li&gt;
&lt;li&gt;字符设备：数据不可以被随机访问，或者包含受限制的随机访问（设备内部构成）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;块设备的一个主要特点就是CPU和总线读写数据的时间开销与磁盘硬件速度不匹配。&lt;/p&gt;
&lt;p&gt;内核再对块设备发出I/O请求时，内核利用&lt;code&gt;通用块层&lt;/code&gt;发起I/O，每次I/O请求都是通过一个&lt;code&gt;bio&lt;/code&gt;结构体来描述。通用块层下的I/O调度程序会根据策略将待处理的I/O请求进行归类，将相邻的请求聚集在一起，减少磁盘磁头的移动。&lt;/p&gt;
&lt;p&gt;块是操作系统和硬件设备在传输数据时的基本单位，&lt;/p&gt;
&lt;h2 id=&#34;mq-deadline&#34;&gt;mq-deadline&lt;/h2&gt;
&lt;h2 id=&#34;bfq&#34;&gt;bfq&lt;/h2&gt;
&lt;h2 id=&#34;kyber&#34;&gt;kyber&lt;/h2&gt;
&lt;h1 id=&#34;nvme接口协议&#34;&gt;NVMe接口协议&lt;/h1&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;NVMe协议的产生是为了取代固态硬盘原有的AHCI协议+SATA接口，随着固态硬盘技术的发展，使得性能瓶颈从存储设备上的转移到了协议和接口中，于是固态硬盘的几大生产商一起制定了该协议。&lt;/p&gt;
&lt;p&gt;NVMe实际上是非易失性存储器标准，不限于闪存SSD，使用PCIe接口。&lt;/p&gt;
&lt;p&gt;相比于AHCI协议，NVMe的主要特点为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;低时延：存储介质方面，存储介质得到了巨大提升；控制器方面，PCIe主控直接与CPU相连，SATA接口需要南桥控制器中转再连接CPU；软件接口方面，简化了指令路径，提高了并发能力&lt;/li&gt;
&lt;li&gt;高性能：相较于AHCI做出了大量优化&lt;/li&gt;
&lt;li&gt;低功耗：自动功耗状态切换，动态能耗管理&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;nvme工作原理&#34;&gt;NVMe工作原理&lt;/h2&gt;
&lt;p&gt;NVMe作为高层次的协议，原则上可以用于任何接口，一般使用PCIe&lt;/p&gt;
&lt;p&gt;NVMe制定了主机和SSD之间的通信命令，NVMe共有两种命令：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Admin命令：用于主机管理、SSD控制&lt;/li&gt;
&lt;li&gt;I/O 命令：用于主机和SSD之间的数据传输&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Admin命令集有：&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221105160942779.png&#34; alt=&#34;image-20221105160942779&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;NVM I/O指令集有：&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221105161030669.png&#34; alt=&#34;image-20221105161030669&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;从主机io请求到ssd&#34;&gt;从主机I/O请求到SSD&lt;/h3&gt;
&lt;p&gt;NVMe有三个重要的机制，Submission Queue、Completion Queue、Doorbell Register。&lt;/p&gt;
&lt;p&gt;SQ和CQ位于主机的内存中，DB位于SSD控制器内部，下图直观的展示了他们之间的位置关系。&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20221105161458148.png&#34; alt=&#34;image-20221105161458148&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;在主机发起I/O指令时，现将指令放在SQ中，主机会通过修改DB寄存器来通知SSD从SQ中取出需要执行的指令。CQ记录了指令执行的状态（成功/失败），SSD负责向CQ中写入命令的状态。&lt;/p&gt;
&lt;p&gt;NVMe对指令的处理流程可以用下图来概括：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221105162016851.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221105162016851&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主机写请求到SQ&lt;/li&gt;
&lt;li&gt;主机写SQ尾DB寄存器，通知SSD需要执行I/O请求&lt;/li&gt;
&lt;li&gt;SSD控制器取出SQ中的请求&lt;/li&gt;
&lt;li&gt;SSD控制器执行命令&lt;/li&gt;
&lt;li&gt;SSD控制器将请求的状态写入CQ&lt;/li&gt;
&lt;li&gt;SSD控制器发出MSI-X中断，通知主机指令完成&lt;/li&gt;
&lt;li&gt;主机收到中断后，处理CQ，看查请求的完成状态&lt;/li&gt;
&lt;li&gt;主机写入CQ头DB寄存器，告知SSD该指令的结果已经收到并处理&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;submission-queue和completion-queue简介&#34;&gt;Submission Queue和Completion Queue简介&lt;/h2&gt;
&lt;p&gt;由于NVMe的SQ、CQ机制，这两个数据结构必然是成对存在的，也存在多对一的关系。&lt;/p&gt;
&lt;p&gt;对于I/O指令和Admin指令，分别由专有的SQ和CQ进行管理，即Admin SQ/CQ和I/O SQ/CQ。&lt;/p&gt;
&lt;p&gt;I/O SQ和SQ是通过Admin的相关指令来进行创建的。&lt;/p&gt;
&lt;p&gt;主机方面每个CPU核心可以有一个或者多个SQ，但是只能有一个CQ。一个核心创建多个SQ主要是为了提高多线程的并发能力，同时可以对不同SQ设置优先级来提高服务质量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221105165937311.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221105165937311&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于Admin SQ/CQ队列，深度在$2-4096(4K)$&lt;/p&gt;
&lt;p&gt;对于I/O SQ/CQ队列，深度在$2-65536(64K)$，一个SQ命令条目大小为$64Byte$，一个CQ条目大小为$16Byte$，队列深度可以自行配置。&lt;/p&gt;
&lt;p&gt;一个PCIe接口也支持多个lane。&lt;/p&gt;
&lt;p&gt;整个NVMe的工作流程像是如下图所示的两个生产者消费者模型&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20221105234231636.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20221105234231636&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;db-doorbell-register&#34;&gt;DB: Doorbell Register&lt;/h2&gt;
&lt;p&gt;在SQ和CQ队列中都有Head和Tail，并且分别有对应的Doorbell，即Head DB和Tail DB，DB在SSD一端，记录SQ和CQ队列头、尾的位置。&lt;/p&gt;
&lt;p&gt;对于SSD而言，要频繁从SQ头取出数据，可以轻松的获取SQ队头的位置，所以SQ Head DB由SSD来维护，而对于SQ的队尾对于主机而言更容易维护，因为要频繁的向队尾插入数据，因此SQ Tail DB由主机维护，SSD根据SQ的头尾可以获取当前队列中&lt;code&gt;有多少请求在等待执行&lt;/code&gt;。同理，CQ Head DB由主机来维护，CQ Tail DB由SSD来维护，SSD根据CQ队列的头尾来判断CQ队列是否还能接受新的&lt;code&gt;请求完成信息&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;从Doorbell的名字来看，还有通知的作用，当Doorbell Register的值改变时，SSD就知道有新的请求需要处理；主机就知道有新的请求已经完成了。&lt;/p&gt;
&lt;p&gt;注意：对于主机来说，DB是可以写不可读的。&lt;/p&gt;
&lt;h2 id=&#34;其他有关nvme的内容&#34;&gt;其他有关NVMe的内容&lt;/h2&gt;
&lt;p&gt;寻址问题（PRP、SGL）&lt;/p&gt;
&lt;p&gt;Namespace&lt;/p&gt;
&lt;p&gt;数据保护问题&lt;/p&gt;
&lt;p&gt;后续有时间再对这些内容进行学习&lt;/p&gt;
&lt;h1 id=&#34;基于闪存的固态盘&#34;&gt;基于闪存的固态盘&lt;/h1&gt;
&lt;p&gt;此前一篇博客整理了有关闪存和固态盘的内容，下文为转载过来的内容&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.ipandai.club/p/%e5%ad%98%e5%82%a8%e6%8a%80%e6%9c%af%e5%9f%ba%e7%a1%80%e5%9b%ba%e6%80%81%e7%a1%ac%e7%9b%98/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Flash Memory &amp;amp;&amp;amp; 固态硬盘&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要由Flash Memory 和FTL组成&lt;/p&gt;
&lt;p&gt;Non-Volatile Memory 提供低延迟持久性的内存/存储，也可以用来做内存&lt;/p&gt;
&lt;p&gt;根据延迟数量级，一般用PCM做内存，Flash Memory做外存&lt;/p&gt;
&lt;h2 id=&#34;flash-memory&#34;&gt;Flash Memory&lt;/h2&gt;
&lt;h3 id=&#34;闪存原理&#34;&gt;闪存原理&lt;/h3&gt;
&lt;h4 id=&#34;类型&#34;&gt;类型&lt;/h4&gt;
&lt;p&gt;NOR闪存&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储密度低&lt;/li&gt;
&lt;li&gt;可字节改写&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NAND闪存（主流）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储密度高&lt;/li&gt;
&lt;li&gt;不可覆盖写&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用于外存需要较高的存储量级，一般用NAND&lt;/p&gt;
&lt;h4 id=&#34;闪存单元&#34;&gt;闪存单元&lt;/h4&gt;
&lt;p&gt;读：电压代表不同数值&lt;/p&gt;
&lt;p&gt;写：电子注入&lt;/p&gt;
&lt;p&gt;相比晶体管添加了浮栅门，保存电子&lt;/p&gt;
&lt;p&gt;原理其实比较简单，非电子系就不做太详细的研究了&lt;/p&gt;
&lt;p&gt;闪存页(4KB,8KB,16KB，读写单元)，阵列中的每一行&lt;/p&gt;
&lt;p&gt;闪存块(擦除单元)，由多个页组成的单元&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922124359358.png&#34; alt=&#34;image-20220922124359358&#34; style=&#34;zoom:33%;&#34; /&gt;
&lt;p&gt;选中行和列，然后将数据加载到Sense Amplifiers&lt;/p&gt;
&lt;p&gt;存储单元有两个阈值的电压，可以根据两个电压的中点作为读电压，2.5V读电压时左边通电，数据为1，右边则不通电，数据为0&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922124755447.png&#34; alt=&#34;image-20220922124755447&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;Pass Through&lt;/p&gt;
&lt;p&gt;选取一个较大的电压，使得所有的单元都接通，数据为1，不影响其他行的状态&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922131649578.png&#34; alt=&#34;image-20220922131649578&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;如图所示的存储结构，在第二行施加2.5V电压，其他行施加5V，最终读取数据为0011&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922134857889.png&#34; alt=&#34;image-20220922134857889&#34; style=&#34;zoom:33%;&#34;/&gt;
&lt;p&gt;上述为SLC，Single Level Cell，单存储单元&lt;/p&gt;
&lt;h4 id=&#34;多比特闪存&#34;&gt;多比特闪存&lt;/h4&gt;
&lt;p&gt;多比特闪存单元MLC，包含2Bits 4个Level的数据&lt;/p&gt;
&lt;p&gt;TLC 3 Bits 8个Level&lt;/p&gt;
&lt;p&gt;QLC 4 Bits 16个Level&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多比特使用格雷码来编码&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;使用格雷码使得相邻单元只有一位差异，方便纠错&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;多比特提高了存储密度，但是提高了错误率，因为施加的电压差距很小。可靠性会降低。&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922135516463.png&#34; alt=&#34;image-20220922135516463&#34; style=&#34;zoom:45%;&#34; /&gt;
&lt;p&gt;对于多比特的写，MLC分为高比特和低比特，对于低比特的状态加偏移电压确定高比特，在低比特时需要加的电压较大，操作难度低，运行速度快，在高比特时需要加的电压小，波形的间距小，操作难度高，运行的速度较慢。&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922140421405.png&#34; alt=&#34;image-20220922140421405&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;p&gt;对于多比特的读，先看lower bit，加一次电压，即可筛选出低位的0，1，再加两次电压确定upper bit。因为upper bit为0的在中间部分，为1的在两侧，因此需要在两个分界线分别加一次电压来确定upper bit为多少。&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922141455826.png&#34; alt=&#34;image-20220922141455826&#34; style=&#34;zoom:40%;&#34; /&gt;
&lt;h4 id=&#34;闪存&#34;&gt;闪存&lt;/h4&gt;
&lt;p&gt;Block的大小的一种配置：&lt;/p&gt;
&lt;p&gt;一行有两个Page，Upper Page和Lower Page，每个单元中，低位构成Lower Page，高位构成Upper Page，有128个单元，128K/8=16KB&lt;/p&gt;
&lt;p&gt;有64列bitlines，一个block的大小即为&lt;code&gt;16KB*64*2=2MB&lt;/code&gt;，一般按照此比例配置Block&lt;/p&gt;
&lt;p&gt;写入时按照固定顺序，写入高低页面相互独立，不能同时写，在写入加压时容易使相邻单元发生数据偏移，要降低错误率&lt;/p&gt;
&lt;p&gt;写入是需要先擦除再写入，擦除整个块&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922143608667.png&#34; alt=&#34;image-20220922143608667&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h4 id=&#34;闪存特性&#34;&gt;闪存特性&lt;/h4&gt;
&lt;h5 id=&#34;读写粒度&#34;&gt;读写粒度&lt;/h5&gt;
&lt;p&gt;闪存页读写粒度：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4KB，8KB，16KB必须全部读取或者写入&lt;/li&gt;
&lt;li&gt;us延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;闪存块擦除力度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2MB擦除&lt;/li&gt;
&lt;li&gt;ms延迟，可以通过FTL来优化&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;不可覆盖写&#34;&gt;不可覆盖写&lt;/h5&gt;
&lt;p&gt;写前需要擦除，读写粒度与擦除粒度不同&lt;/p&gt;
&lt;p&gt;存在64bytes的OOB（out of bound area），保存ECC，用于纠错，容忍写入时部分比特出错&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922162139217.png&#34; alt=&#34;image-20220922162139217&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;有限次擦除&#34;&gt;有限次擦除&lt;/h5&gt;
&lt;p&gt;随着擦除次数的增加，存储单元不能可靠的保持状态（存储数据）。&lt;/p&gt;
&lt;p&gt;氧化层老化变薄，束缚电子能力变弱&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;耐久性 变薄地次数&lt;/li&gt;
&lt;li&gt;保持力 不通电可以放置的时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SLC：10w次&lt;/p&gt;
&lt;p&gt;MLC：1w次&lt;/p&gt;
&lt;p&gt;TLC：1k次&lt;/p&gt;
&lt;p&gt;根据特性来设计FTL固件&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922164523965.png&#34; alt=&#34;image-20220922164523965&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h2 id=&#34;ftl&#34;&gt;FTL&lt;/h2&gt;
&lt;p&gt;固态硬盘整体构成&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922164728094.png&#34; alt=&#34;image-20220922164728094&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;SSD中的通道可以并行，通道中也可以并行读取，每个Plane中有寄存器，暂时存储准备好的数据。不同单元并行，因此内部带宽大&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922165354311.png&#34; alt=&#34;image-20220922165354311&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;垃圾回收&#34;&gt;垃圾回收&lt;/h3&gt;
&lt;p&gt;page对于OS而言，是写入时的block&lt;/p&gt;
&lt;p&gt;Page三种状态&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;空闲 free page&lt;/li&gt;
&lt;li&gt;有效页 live/valid page&lt;/li&gt;
&lt;li&gt;无效页 dead/invalid page&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922184349456.png&#34; alt=&#34;image-20220922184349456&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;需要擦除无效页，先移走有效页，然后再对一整行进行擦除，转为空闲&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220922184406162.png&#34; alt=&#34;image-20220922184406162&#34; style=&#34;zoom:25%;&#34; /&gt;
&lt;p&gt;时间开销：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复制有效数据到$(R_{Latency}+W_{Latency})*N$，N是移动page的数量&lt;/li&gt;
&lt;li&gt;擦除产生的开销 ms级延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;gc策略&#34;&gt;GC策略&lt;/h4&gt;
&lt;p&gt;要解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;何时启动GC&lt;/li&gt;
&lt;li&gt;选中那些/多少Block进行GC&lt;/li&gt;
&lt;li&gt;有效的页如何被转写&lt;/li&gt;
&lt;li&gt;新数据写到哪里&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GC的时间开销：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;块擦除的时间 ms&lt;/li&gt;
&lt;li&gt;有效页的复制时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;贪心策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找到脏页最多的block来进行擦除&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Hot/Cold 数据隔离，分组问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;磨损均衡&#34;&gt;磨损均衡&lt;/h3&gt;
&lt;p&gt;优化寿命，有静态和动态策略&lt;/p&gt;
&lt;p&gt;静态：周期性的调整冷热数据存储的位置&lt;/p&gt;
&lt;p&gt;冷热数据的分区：将冷数据放在一起，热数据放在一起&lt;/p&gt;
&lt;h3 id=&#34;ftl简介&#34;&gt;FTL简介&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;维护映射，虚拟地址到物理地址&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用SRAM存储映射&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向上层隐藏擦除操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;避免原地更新-&amp;gt;异地更新&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新一个新页面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高性能的垃圾回收和擦除&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OOB有物理地址到虚拟地址的映射，用于掉电恢复，这里引用一段&lt;a class=&#34;link&#34; href=&#34;https://pages.cs.wisc.edu/~remzi/OSTEP/file-ssd.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;wisc的OSTEP中的一段解释（44 Flash- based SSD）&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;OOB保存的在每个页中映射信息，当掉电或者重启时用它在内存中重建映射&lt;/p&gt;
&lt;p&gt;为了防止在重建时扫码整个SSD，可以使用日志或者检查点的方式来加速这个过程&lt;/p&gt;
&lt;p&gt;大致看了一下OSTEP，是有关操作系统的一本非常好的书，希望以后有时间读一下&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220924141132556.png&#34; alt=&#34;image-20220924141132556&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;page-level-ftl&#34;&gt;Page-Level FTL&lt;/h3&gt;
&lt;p&gt;原理类似OS中的页表，由Logical Page Number查询页表得到Physical Page Number&lt;/p&gt;
&lt;p&gt;缺点是页表占用很大的空间&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220924152322358.png&#34; alt=&#34;image-20220924152322358&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;block-level-ftl&#34;&gt;Block-Level FTL&lt;/h3&gt;
&lt;p&gt;保持Block 到Block的映射&lt;/p&gt;
&lt;p&gt;先查找到对应的Block，在根据offset得到page，块内的页码偏移offset是固定的&lt;/p&gt;
&lt;img src=&#34;https://cdn.ipandai.club/image-20220924152259043.png&#34; alt=&#34;image-20220924152259043&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;优点是占用空间很小，缺点是GC负载增加&lt;/p&gt;
&lt;p&gt;原因：offset在不同Block中保持不变，在异地更新时，要选择其他block中相同的offset进行写入，如果选中的block已经存在数据，需要把数据迁移。【TODO 这里讲得不是非常的清楚】&lt;/p&gt;
&lt;h3 id=&#34;hybrid-ftl&#34;&gt;Hybrid FTL&lt;/h3&gt;
&lt;p&gt;对写入分为新/旧数据，新写入的数据用Page-Level Mapping效率高，写入Log Blocks作为缓冲，之后再更新到Data Blocks&lt;/p&gt;
&lt;p&gt;旧数据因为写入后更新相对不频繁，使用Block-Level Mapping，写入Data Blocks&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;NNSS实验室暑期实习文档&lt;/li&gt;
&lt;li&gt;《Linux内核设计与实现》&lt;/li&gt;
&lt;li&gt;《深入理解Linux内核》&lt;/li&gt;
&lt;li&gt;《Linux设备驱动程序》&lt;/li&gt;
&lt;li&gt;《深入浅出SSD：固态存储核心技术、原理与实战》&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.enterprisestorageforum.com/hardware/nand-dram-sas-scsi-and-sata-ahci-not-dead-yet/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;NAND, DRAM, SAS/SCSI and SATA/AHCI: Not Dead, Yet | Enterprise Storage Forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.quora.com/Linux-Kernel/What-is-the-major-difference-between-the-buffer-cache-and-the-page-cache-Why-were-they-separate-entities-in-older-kernels-Why-were-they-merged-later-on/answer/Robert-Love-1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【Quora】Robert Love对缓冲区缓存和页面缓存的主要区别是什么的回答？为什么它们在较旧的内核中是独立的实体？为什么后来他们被合并了？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://pages.cs.wisc.edu/~remzi/OSTEP/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【OSTEP】Operating Systems: Three Easy Pieces (wisc.edu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1xE411T7Dy/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【bilibili 清华大学】存储技术基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ext4.wiki.kernel.org/index.php/Ext4_Howto&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org Ext4文件系统介绍】Ext4 Howto - Ext4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://btrfs.wiki.kernel.org/index.php/Status&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org Btrfs文件系统现状】Status - btrfs Wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.kernel.org/doc/html/latest/filesystems/ext4/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【kernel.org Ext4文件系统设计】ext4 Data Structures and Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/fast15/technical-sessions/presentation/lee&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【USENIX FAST&#39;21】F2FS: A New File System for Flash Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://wiki.archlinux.org/title/Ext4_%28%e7%ae%80%e4%bd%93%e4%b8%ad%e6%96%87%29#%e5%85%b3%e9%97%ad%e5%b1%8f%e9%9a%9c&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【archlinux.org ArchWiki】Ext4 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-ssd&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【Red Hat Customer Portal | SSD discard】Chapter 21. Solid-State Disk Deployment Guidelines Red Hat Enterprise Linux 7&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>[OSDI&#39;19] Flashshare: Punching Through Server Storage Stack from Kernel to Firmware for Ultra-Low Latency SSDs</title>
        <link>https://blog.ipandai.club/p/osdi19-flashshare-punching-through-server-storage-stack-from-kernel-to-firmware-for-ultra-low-latency-ssds/</link>
        <pubDate>Wed, 17 Aug 2022 10:30:30 +0800</pubDate>
        
        <guid>https://blog.ipandai.club/p/osdi19-flashshare-punching-through-server-storage-stack-from-kernel-to-firmware-for-ultra-low-latency-ssds/</guid>
        <description>&lt;p&gt;超低延迟固态硬盘从内核到固件的服务器存储堆栈&lt;/p&gt;
&lt;h1 id=&#34;个别名词解释&#34;&gt;个别名词解释&lt;/h1&gt;
&lt;p&gt;the 99^th percentile 超过统计数据99%的数是多少&lt;/p&gt;
&lt;p&gt;blk-mq Linux Multiqueue block layer 内核对ssd随机I/O的优化&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;message signaled interrupt (MSI)&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;1摘要&#34;&gt;1.摘要&lt;/h1&gt;
&lt;p&gt;flash share&lt;/p&gt;
&lt;p&gt;在内核中，扩展了系统堆栈的数据结构，传递应用程序的属性（？），包括内核层到SSD固件。&lt;/p&gt;
&lt;p&gt;对于给定的属性，FlashShare的块层管理IO调度并处理NVMe中断。&lt;/p&gt;
&lt;p&gt;评估结果表明，FLASHSHARE可以将共同运行应用程序的平均周转响应时间分别缩短22%和31%。&lt;/p&gt;
&lt;h1 id=&#34;10-intro&#34;&gt;1.0 Intro&lt;/h1&gt;
&lt;h2 id=&#34;11-现状&#34;&gt;1.1 现状&lt;/h2&gt;
&lt;p&gt;网络服务提供商，满足服务级别协议SLA，延迟敏感&lt;/p&gt;
&lt;p&gt;某个段时间短可能有大量请求涌入，供应商会超额配置机器以满足SLA&lt;/p&gt;
&lt;p&gt;现状：该场景并不常见，因此大部分情况下服务器的资源占用率非常低，能耗比低。&lt;/p&gt;
&lt;p&gt;为了解决利用率低，服务器会运行离线的数据分析应用，延迟不敏感，以吞吐量为导向。&lt;/p&gt;
&lt;p&gt;因此，在运行了多个进程的服务器上，I/O延迟增高，满足SLA非常困难。&lt;/p&gt;
&lt;p&gt;现有的ULL SSD相较于NVMe SSD可以减少10倍的延迟&lt;/p&gt;
&lt;p&gt;但是这些ULL SSD在同时运行多个进程下高强度压榨服务器的时候，不能充分利用ULL SSD的优势/表现一般。&lt;/p&gt;
&lt;p&gt;the 99th percentile 是0.8ms（apache）&lt;/p&gt;
&lt;p&gt;但是当服务器同时运行pagerank的时候，延迟会增加228.5%。&lt;/p&gt;
&lt;p&gt;原因：略&lt;/p&gt;
&lt;p&gt;从固件到内核优化堆栈的存储。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;内核级别的增强：&lt;/p&gt;
&lt;p&gt;两个挑战&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux的blk-mq导致I/O请求队列化，引入延迟&lt;/li&gt;
&lt;li&gt;NVMe的队列机制没有对I/O优先级的策略，因此，来自离线应用的IO请求容易阻塞在线应用的紧急请求，造成延迟。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于latency critical的请求，绕过NVMe的请求队列。同时令NVMe的驱动通过知晓每个应用的延迟临界匹配NVMe的提交和请求队列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固件层设计：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​		即使内核级的优化保证了延迟敏感的请求可以获得高优先级，但如果基础固件不了解延迟临界值，ULL特性（类似内存的性能）无法完全暴露给用户。本文中重新设计了I/O调度和缓存的固件，以直接向用户暴露ULL特性。将ULL SSD的集成缓存进行分区，并根据工作负载的属性对每个I/O服务独立的分配缓存。固件动态的更新分区大小并以精细粒度调整预取I/O粒度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ULL SSD的新中断处理服务：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​	当前的NVMe中断机制没有对ULL I/O服务优化。轮询方法（Linux 4.9.30）消耗了大量的CPU资源去检查I/O服务的完成情况。当轮询在线交互服务的IO请求完成状态时，flashShare使用一个仅对离线应用程序使用消息信号中断的选择性中断服务程序Select-ISR。&lt;/p&gt;
&lt;p&gt;​	通过将NVMe队列和ISR卸载到硬件加速器中来进一步优化NVMe completion routine。&lt;/p&gt;
&lt;p&gt;​	各种仿真实验后效果不错，效率提高了22%和31%。&lt;/p&gt;
&lt;h1 id=&#34;20-background&#34;&gt;2.0 Background&lt;/h1&gt;
&lt;h2 id=&#34;21-存储内核栈&#34;&gt;2.1 存储内核栈&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220810145032258.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220810145032258&#34;
	
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Linux文件系统IO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bio&lt;/li&gt;
&lt;li&gt;request&lt;/li&gt;
&lt;li&gt;nvme_rw_command&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;存储堆栈中，NVMe驱动发起的请求通过nvme_rw_command的形式传递到PCI/PCIe设备驱动中。&lt;/p&gt;
&lt;p&gt;当I/O请求完成后，发送信号中断，中断直接被写入到中断处理器的中断向量中。被中断的核心选择ISR处理该中断请求，随后NVMe驱动再SQ/CQ中清空相应的记录并将结果返回至上一层（比如blk-mq和文件系统）。&lt;/p&gt;
&lt;h2 id=&#34;22-设备固件栈&#34;&gt;2.2 设备固件栈&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220810232230437.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220810232230437&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;收到request&lt;/li&gt;
&lt;li&gt;SQ tail++入队&lt;/li&gt;
&lt;li&gt;写入SQ门铃寄存器&lt;/li&gt;
&lt;li&gt;通过DMA读取数据的物理位置&lt;/li&gt;
&lt;li&gt;SQ head++出队&lt;/li&gt;
&lt;li&gt;将请求转发至嵌入式缓存层或者FTL&lt;/li&gt;
&lt;li&gt;当出现缺页或者页面替换时，FTL将目标LBA转换成Z-NAND中相应的物理地址，必要时自行GC&lt;/li&gt;
&lt;li&gt;在完成I/O请求之后，NVMe控制器增加这个CQ的tail，入队&lt;/li&gt;
&lt;li&gt;通过DMA传输数据，并修改phase tag&lt;/li&gt;
&lt;li&gt;主机ISR通过搜索队列中检查phase tag，对于有效的phase tag，ISR清除tag位，并且处理剩余的I/O完成请求程序。&lt;/li&gt;
&lt;li&gt;CQ head++出队，在SQ中移除相应的记录，并且写入CQ的head doorbell&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;30-跨层设计&#34;&gt;3.0 跨层设计&lt;/h1&gt;
&lt;h2 id=&#34;31-快速存储的挑战&#34;&gt;3.1 快速存储的挑战&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220811110758918.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220811110758918&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;原因是存储栈无法区分来自Apache的I/O请求，及时两个应用需要不同级别I/O的响应。&lt;/p&gt;
&lt;h1 id=&#34;32-预知灵敏响应&#34;&gt;3.2 预知灵敏响应&lt;/h1&gt;
&lt;p&gt;为了让内核可以区分I/O 请求的优先级和紧迫程度，修改Linux的进程控制快&lt;code&gt;task_struct&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;为了保证有效性，在&lt;code&gt;address_space&lt;/code&gt;,&lt;code&gt;bio&lt;/code&gt;,&lt;code&gt;request&lt;/code&gt;,&lt;code&gt;nvme_rw_command&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;中都保存工作负载属性，在存储堆栈上打孔。&lt;/p&gt;
&lt;p&gt;FlashShare同时提供了一个可以在服务器上配置这些属性的工具。叫做&lt;code&gt;chworkload_attr&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;可以方便的修改每个应用的属性并绑定到&lt;code&gt;task_struct&lt;/code&gt;中&lt;/p&gt;
&lt;p&gt;修改了syscall表&lt;code&gt;arch/x86/entry/syscalls/syscall 64.tbl&lt;/code&gt;添加了两个系统调用，可以从&lt;code&gt;task_struct&lt;/code&gt;中set/get工作属性。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;/linux/syscall.h&lt;/code&gt;中进行注册，并带有&lt;code&gt;asmlinkage&lt;/code&gt;标签。&lt;/p&gt;
&lt;p&gt;用户通过shell给定特定进程，实现于&lt;code&gt;/sched/cores.c&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;33-内核优化&#34;&gt;3.3 内核优化&lt;/h2&gt;
&lt;p&gt;优化文件系统中的blk-mq和NVMe驱动&lt;/p&gt;
&lt;p&gt;blk-mq合并重排请求提高了带宽使用，但是引入了延迟&lt;/p&gt;
&lt;p&gt;跳过所有在线应用的I/O 请求&lt;/p&gt;
&lt;p&gt;如果离线应用程序的 I/O 请求被 blk-mq 调度到后续在线应用程序发出的同一 LBA，则可能发生危险。&lt;/p&gt;
&lt;p&gt;如果两个请求的操作类型不同，blk-mq会将两个请求串联。否则blk-mq会将两个请求合并为一个&lt;code&gt;request&lt;/code&gt;并交给NVMe驱动。&lt;/p&gt;
&lt;p&gt;为了防止延迟敏感的I/O 被NVMe控制器杀死：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为每个核心创建两个SQ队列和一个CQ队列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220811173922087.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220811173922087&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;其中一个SQ保存来自在线应用的I/O请求。&lt;strong&gt;NVMe驱动程序通过管理员队列发送消息，通知NVMe控制器选择一种新的队列调度方法，该方法始终优先安排该SQ中的请求。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;未来避免因优先级带来的饥饿，当该队列中的请求数量大于阈值时，或者没有在规定时间内被满足，NVMe驱动会满足所有离线应用I/O 。&lt;/p&gt;
&lt;p&gt;实验表明，队列大小为8或者200us的阈值最好。&lt;/p&gt;
&lt;h1 id=&#34;40-io-completion和缓存&#34;&gt;4.0 I/O Completion和缓存&lt;/h1&gt;
&lt;p&gt;采用轮询机制时查询I/O Completion时，内核态占用97%。&lt;/p&gt;
&lt;p&gt;带来两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;没有为处理I/O 响应单独分配核心，对于多进程下低效&lt;/li&gt;
&lt;li&gt;我们要减轻处理I/O轮询的核心开销，进一步降低延迟&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;41-中断处理程序&#34;&gt;4.1 中断处理程序&lt;/h2&gt;
&lt;p&gt;flash share仅对来自在线应用的I/O 请求使用轮询&lt;/p&gt;
&lt;p&gt;使用信号处理离线应用&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220811231355037.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220811231355037&#34;
	
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;修改blk-mq中的&lt;code&gt;submit_bio()&lt;/code&gt;，将由文件系统或缓存的bio插入到mq&lt;/li&gt;
&lt;li&gt;如果bio是来自离线应用的，则插入队列，as normal&lt;/li&gt;
&lt;li&gt;如果bio是来自在线应用的，blk-mq则调用&lt;code&gt;queue_rq()&lt;/code&gt;将请求发送至NVMe驱动。&lt;/li&gt;
&lt;li&gt;NVMe驱动转换I/O 请求为NVMe指令并非插入到响应SQ队列中&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用Select-ISR，当请求为离线应用时，CPU核心可以通过上下文切换从NVMe驱动中释放。否则，blk-mq调用轮询机制&lt;code&gt;blk-poll()&lt;/code&gt;。&lt;code&gt;blk-poll()&lt;/code&gt;持续调用&lt;code&gt;nvme_poll()&lt;/code&gt;，检查有效的完成记录是否存在于目标NVMe CQ中。如果存在，blk-mq禁用此CQ的IRQ，以至于MSI信号无法再次捕获blk-mq程序。&lt;code&gt;nvme_poll()&lt;/code&gt;通过检查CQ中的phase tags查找CQ中的新记录。&lt;/p&gt;
&lt;p&gt;具体来说，&lt;code&gt;nvme poll()&lt;/code&gt;搜索一个CQ记录，其请求信息与&lt;code&gt;blk poll()&lt;/code&gt;等待完成的标签匹配。一旦检测到这样的新记录，blk-mq就会退出在&lt;code&gt;blk poll()&lt;/code&gt;中实现的无限迭代，并将上下文切换到其用户进程。&lt;/p&gt;
&lt;p&gt;提出&lt;code&gt;I/O-stack accelerator&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;主要目的是将blk-mq的任务迁移到附属于PCIe的加速器中&lt;/p&gt;
&lt;p&gt;可以使得通过上层文件系统生成的bio直接转换成&lt;code&gt;nvm_rw_command&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;通过特殊的tag索引搜索队列中的元素，并且代表CPU合并bio&lt;/p&gt;
&lt;p&gt;该方法可以减少36%的I/O completion时间。&lt;/p&gt;
&lt;h2 id=&#34;42-固件层&#34;&gt;4.2 固件层&lt;/h2&gt;
&lt;p&gt;创建两个内存分区，一个服务于在线应用，一个服务于离线应用。&lt;/p&gt;
&lt;p&gt;三种模式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;固定拆分缓存&lt;/li&gt;
&lt;li&gt;根据I/O动态划分&lt;/li&gt;
&lt;li&gt;数据可保留&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220812003132031.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220812003132031&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;43-io-stack-acceleration&#34;&gt;4.3 I/O-Stack Acceleration&lt;/h2&gt;
&lt;p&gt;添加了一个barrier logic，简单的MUX，作为硬件仲裁&lt;/p&gt;
&lt;p&gt;引入status bitmap来过滤SQ队列中的记录&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并逻辑插入一个行的nvme 指令，status bitmap设置为1&lt;/li&gt;
&lt;li&gt;如果监测到ULL SSD从I/O SQ中读取NVMe指令，status bitmap设置为0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果状态位图表明CAM中的请求条目（与目标SQ相关联）无效，CAM将跳过对这些条目的搜索。&lt;/p&gt;
&lt;h1 id=&#34;50-实验&#34;&gt;5.0 实验&lt;/h1&gt;
&lt;h2 id=&#34;51-实验步骤&#34;&gt;5.1 实验步骤&lt;/h2&gt;
&lt;p&gt;使用gem5系统结构模拟&lt;/p&gt;
&lt;p&gt;64位arm指令集&lt;/p&gt;
&lt;p&gt;Linux 4.9.30&lt;/p&gt;
&lt;p&gt;8核心2GHz&lt;/p&gt;
&lt;p&gt;L1 Cache 64KB&lt;/p&gt;
&lt;p&gt;2GB Memory&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.ipandai.club/image-20220812191343849.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220812191343849&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;将SSD固件转移到主机上，消除冗余的地址转换&lt;/p&gt;
&lt;p&gt;根据应用程序特征对缓存进行分区处理，然而不能发挥ULL SSD的作用&lt;/p&gt;
&lt;p&gt;从文件系统和block IO设备方面优化移动端操作系统，使其提高SQLite的性能，有局限性，应用程序、ULL SSD&lt;/p&gt;
&lt;p&gt;在内核的多个层对写请求进行调度，容易阻塞读请求和ULL操作&lt;/p&gt;
&lt;p&gt;根据前台任务和后台任务中的依赖关系，分配优先级，允许后台任务高优先级，IO通常情况下没有依赖关系，效果差，服务器大部分都是多进程&lt;/p&gt;
&lt;p&gt;考虑对在线应用设置高优先级，但是没有考虑对IO stack中其他部分的影响&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
